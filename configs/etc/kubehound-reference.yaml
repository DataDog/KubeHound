#
# Reference KubeHound configuration
#

#
# K8s collector configuration
#
collector:
  # Type of collector to use
  type: live-k8s-api-collector

  # Ask for confirmation before collecting the targeted cluster
  # non_interactive: false

  # Live collector configuration
  live:
    # Rate limit of requests/second to the Kubernetes API
    # NOTE: most (>90%) of the current runtime of KubeHound is spent in the transfer of data from the remote K8s API server,
    # and the bulk of that is spent waiting on rate limit. As such increasing this will improve performance roughly linearly.
    rate_limit_per_second: 60

    # Number of entries retrieved by each call on the API (same for all Kubernetes entry types)
    # page_size: 500

    # Number of pages to buffer
    # page_buffer_size: 10

  # Uncomment to use the file collector
  # type: file-collector

  # File collector configuration
  # file:
  #   # Directory holding the K8s json data files
  #   directory: /path/to/directory
  #
  #   # Target cluster name
  #   cluster: <cluster name>

#
# General storage configuration
#
storage:
  # Whether or not to wipe all data on startup
  wipe: true
  
  # Number of connection retries before declaring an error
  retry: 5

  # Delay between connection retries
  retry_delay: 10s

# Store database configuration
mongodb:
  # Connection URL to the mongo DB instance
  url: "mongodb://localhost:27017"

  # Timeout on requests to the mongo DB instance
  connection_timeout: 30s

# Graph database configuration
janusgraph:
  # Connection URL to the JanusGraph DB instance
  url: "ws://localhost:8182/gremlin"

  # Timeout on requests to the JanusGraph DB instance
  connection_timeout: 30s

#
# Datadog telemetry configuration
#
telemetry:
  # Whether to enable Datadog telemetry (default false)
  enabled: true

  # Default tags to add to all telemetry (free form key-value map)
  # tags:
  #   team: ase 
  
  # Statsd configuration for metics support
  statsd:
    # URL to send statsd data to the Datadog agent
    url: "127.0.0.1:8225"

  # Tracer configuration for APM support
  tracer:
    # URL to send tracer data to the Datadog agent
    url: "127.0.0.1:8226"

#
# Graph builder configuration
#
# NOTE: increasing batch sizes can have some performance improvements by reducing network latency in transferring data
# between KubeGraph and the application. However, increasing it past a certain level can overload the backend leading 
# to instability and eventually exceed the size limits of the websocket buffer used to transfer the data. Changing this
# is not recommended.
#
builder:
  # Vertex builder configuration
  # vertex:
  #   # Batch size for vertex inserts
  #   batch_size: 500
  # 
  #   # Small batch size for vertex inserts
  #   batch_size_small: 100

  # Edge builder configuration
  edge:
    # Enable for large clusters to prevent number of edges growing exponentially
    large_cluster_optimizations: true

    # # Size of the worker pool handling parallel edge inserts
    # # NOTE: this should only be changed if granting additional resources to the KubeGraph container
    # worker_pool_size: 5

    # # Capacity of the worker pool handling parallel edge inserts
    # # NOTE: this should only be changed in conjunction with the worker_pool_size
    # worker_pool_capacity: 100

    # # Batch size for edge inserts
    # batch_size: 500

    #  # Small batch size for edge inserts
    # batch_size_small: 75

    #  # Cluster impact batch size for edge inserts
    # batch_size_cluster_impact: 1
  
# Ingestor configuration (for KHaaS)
# ingestor:
#   blob:
#    bucket: "" # (i.e.: s3://your-bucket)
#    region: "" # (i.e.: us-east-1)
#   temp_dir: "/tmp/kubehound"
#   archive_name: "archive.tar.gz"
#  max_archive_size: 2147483648 # 2GB
#   api: # GRPC endpoint for the ingestor
#     endpoint: "127.0.0.1:9000"
#     insecure: true